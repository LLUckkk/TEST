{
  "input_header": "def save_feature(self, utt, content_feature):",
  "input_docstring": "Save a single utternace to path {cfg.preprocess.processed_dir}\n\nArgs:\n    utt (dict): one item in metadata, containing information for one utterance\n    content_feature (tensor): content feature of one utterance",
  "output_code": "    def save_feature(self, utt, content_feature):\n        \n        uid = utt[\"Uid\"]\n        assert self.extractor_type != None\n        out_dir = os.path.join(\n            self.cfg.preprocess.processed_dir, utt[\"Dataset\"], self.extractor_type\n        )\n        os.makedirs(out_dir, exist_ok=True)\n        save_path = os.path.join(out_dir, uid + \".npy\")\n\n        content_feature = self.get_valid_features(utt, content_feature)\n        np.save(save_path, content_feature.cpu().detach().numpy())",
  "input_contexts": [
    {
      "id": "open-mmlab_Amphion_3876_1",
      "input_code": "def extract_utt_content_features_dataloader(cfg, metadata, num_workers):\n    dataset_name = metadata[0][\"Dataset\"]\n    with torch.no_grad():\n        if cfg.preprocess.extract_whisper_feature:\n            feat_dir = os.path.join(\n                cfg.preprocess.processed_dir, dataset_name, \"whisper\"\n            )\n            os.makedirs(feat_dir, exist_ok=True)\n            feat_files_num = len(os.listdir(feat_dir))\n\n            if feat_files_num != len(metadata):\n                whisper_waveforms = FFmpegDataset(\n                    cfg,\n                    dataset_name,\n                    cfg.preprocess.whisper_sample_rate,\n                    metadata=metadata,\n                )\n                data_loader = DataLoader(\n                    whisper_waveforms,\n                    num_workers=num_workers,\n                    shuffle=False,\n                    pin_memory=cfg.preprocess.pin_memory,\n                    batch_size=cfg.preprocess.content_feature_batch_size,\n                    collate_fn=collate_batch,\n                    drop_last=False,\n                )\n                extractor = WhisperExtractor(cfg)\n                extractor.load_model()\n                for batch_idx, items in enumerate(tqdm(data_loader)):\n                    _metadata, wavs, lens = items\n\n                    batch_content_features = extractor.extract_content_features(wavs)\n                    for index, utt in enumerate(_metadata):\n                        extractor.save_feature(utt, batch_content_features[index])\n\n        if cfg.preprocess.extract_contentvec_feature:\n            feat_dir = os.path.join(\n                cfg.preprocess.processed_dir, dataset_name, \"contentvec\"\n            )\n            os.makedirs(feat_dir, exist_ok=True)\n            feat_files_num = len(os.listdir(feat_dir))\n\n            if feat_files_num != len(metadata):\n                contentvec_waveforms = LibrosaDataset(\n                    cfg,\n                    dataset_name,\n                    cfg.preprocess.contentvec_sample_rate,\n                    metadata=metadata,\n                )\n                data_loader = DataLoader(\n                    contentvec_waveforms,\n                    num_workers=num_workers,\n                    shuffle=False,\n                    pin_memory=cfg.preprocess.pin_memory,\n                    batch_size=cfg.preprocess.content_feature_batch_size,\n                    collate_fn=collate_batch,\n                    drop_last=False,\n                )\n                extractor = ContentvecExtractor(cfg)\n                extractor.load_model()\n                for batch_idx, items in enumerate(tqdm(data_loader)):\n                    _metadata, wavs, lens = items\n\n                    batch_content_features = extractor.extract_content_features(wavs)\n                    for index, utt in enumerate(_metadata):\n                        extractor.save_feature(utt, batch_content_features[index])\n\n        if cfg.preprocess.extract_wenet_feature:\n            feat_dir = os.path.join(cfg.preprocess.processed_dir, dataset_name, \"wenet\")\n            os.makedirs(feat_dir, exist_ok=True)\n            feat_files_num = len(os.listdir(feat_dir))\n\n            if feat_files_num != len(metadata):\n                wenet_waveforms = TorchaudioDataset(\n                    cfg,\n                    dataset_name,\n                    cfg.preprocess.wenet_sample_rate,\n                    metadata=metadata,\n                )\n                data_loader = DataLoader(\n                    wenet_waveforms,\n                    num_workers=num_workers,\n                    shuffle=False,\n                    pin_memory=cfg.preprocess.pin_memory,\n                    batch_size=cfg.preprocess.content_feature_batch_size,\n                    collate_fn=collate_batch,\n                    drop_last=False,\n                )\n                extractor = WenetExtractor(cfg)\n                extractor.load_model()\n                for batch_idx, items in enumerate(tqdm(data_loader)):\n                    _metadata, wavs, lens = items\n\n                    batch_content_features = extractor.extract_content_features(\n                        wavs,\n                        lens,\n                    )\n                    for index, utt in enumerate(_metadata):\n                        extractor.save_feature(utt, batch_content_features[index])\n\n        if cfg.preprocess.extract_mert_feature:\n            feat_dir = os.path.join(cfg.preprocess.processed_dir, dataset_name, \"mert\")\n            os.makedirs(feat_dir, exist_ok=True)\n            feat_files_num = len(os.listdir(feat_dir))\n\n            if feat_files_num != len(metadata):\n                mert_waveforms = TorchaudioDataset(\n                    cfg,\n                    dataset_name,\n                    cfg.preprocess.mert_sample_rate,\n                    metadata=metadata,\n                )\n                data_loader = DataLoader(\n                    mert_waveforms,\n                    num_workers=num_workers,\n                    shuffle=False,\n                    pin_memory=cfg.preprocess.pin_memory,\n                    batch_size=cfg.preprocess.content_feature_batch_size,\n                    collate_fn=collate_batch,\n                    drop_last=False,\n                )\n                extractor = MertExtractor(cfg)\n                extractor.load_model()\n                for batch_idx, items in enumerate(tqdm(data_loader)):\n                    _metadata, wavs, lens = items\n\n                    batch_content_features = extractor.extract_content_features(wavs)\n                    for index, utt in enumerate(_metadata):\n                        extractor.save_feature(utt, batch_content_features[index])\n"
    }
  ],
  "generated_inputs": [
    "```python\n<needimport>\nimport torch\nimport numpy as np\n\n<testcase_1>\nutt = {\n    \"Uid\": \"utterance_001\",\n    \"Dataset\": \"dataset_A\"\n}\ncontent_feature = torch.rand(256, 512)\n\n<testcase_2>\nutt = {\n    \"Uid\": \"utterance_002\",\n    \"Dataset\": \"dataset_B\"\n}\ncontent_feature = torch.rand(128, 256)\n\n<testcase_3>\nutt = {\n    \"Uid\": \"utterance_003\",\n    \"Dataset\": \"dataset_C\"\n}\ncontent_feature = torch.rand(512, 1024)\n\n<testcase_4>\nutt = {\n    \"Uid\": \"utterance_004\",\n    \"Dataset\": \"dataset_D\"\n}\ncontent_feature = torch.rand(64, 128)\n\n<testcase_5>\nutt = {\n    \"Uid\": \"utterance_005\",\n    \"Dataset\": \"dataset_E\"\n}\ncontent_feature = torch.rand(1024, 2048)\n```"
  ],
  "gpt_output": "```python\n<needimport>\nimport torch\nimport numpy as np\n\n<testcase_1>\nutt = {\n    \"Uid\": \"utterance_001\",\n    \"Dataset\": \"dataset_A\"\n}\ncontent_feature = torch.rand(256, 512)\n\n<testcase_2>\nutt = {\n    \"Uid\": \"utterance_002\",\n    \"Dataset\": \"dataset_B\"\n}\ncontent_feature = torch.rand(128, 256)\n\n<testcase_3>\nutt = {\n    \"Uid\": \"utterance_003\",\n    \"Dataset\": \"dataset_C\"\n}\ncontent_feature = torch.rand(512, 1024)\n\n<testcase_4>\nutt = {\n    \"Uid\": \"utterance_004\",\n    \"Dataset\": \"dataset_D\"\n}\ncontent_feature = torch.rand(64, 128)\n\n<testcase_5>\nutt = {\n    \"Uid\": \"utterance_005\",\n    \"Dataset\": \"dataset_E\"\n}\ncontent_feature = torch.rand(1024, 2048)\n```",
  "needimport": "import torch\nimport numpy as np\n\n",
  "test_results": {
    "ans1": "./processed\\dataset_A\\hubert\\utterance_001.npy",
    "ans2": "./processed\\dataset_B\\hubert\\utterance_002.npy",
    "ans3": "./processed\\dataset_C\\hubert\\utterance_003.npy",
    "ans4": "./processed\\dataset_D\\hubert\\utterance_004.npy",
    "ans5": "./processed\\dataset_E\\hubert\\utterance_005.npy"
  }
}