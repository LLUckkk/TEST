{
  "input_header": "def prune_linear_layer(layer, index, dim=0):",
  "input_docstring": "Prune a linear layer (a model parameters) to keep only entries in index.\nReturn the pruned layer as a new layer with requires_grad=True.\nUsed to remove heads.",
  "output_code": "def prune_linear_layer(layer, index, dim=0):\n    \n    index = index.to(layer.weight.device)\n    W = layer.weight.index_select(dim, index).clone().detach()\n    if layer.bias is not None:\n        if dim == 1:\n            b = layer.bias.clone().detach()\n        else:\n            b = layer.bias[index].clone().detach()\n    new_size = list(layer.weight.size())\n    new_size[dim] = len(index)\n    new_layer = nn.Linear(new_size[1], new_size[0], bias=layer.bias is not None).to(layer.weight.device)\n    new_layer.weight.requires_grad = False\n    new_layer.weight.copy_(W.contiguous())\n    new_layer.weight.requires_grad = True\n    if layer.bias is not None:\n        new_layer.bias.requires_grad = False\n        new_layer.bias.copy_(b.contiguous())\n        new_layer.bias.requires_grad = True\n    return new_layer",
  "input_contexts": [
    {
      "id": "jessevig_bertviz_236_3",
      "input_code": "    def prune_heads(self, heads):\n        attention_head_size = self.dim // self.n_heads\n        if len(heads) == 0:\n            return\n        mask = torch.ones(self.n_heads, attention_head_size)\n        for head in heads:\n            mask[head] = 0\n        mask = mask.view(-1).contiguous().eq(1)\n        index = torch.arange(len(mask))[mask].long()\n        self.q_lin = prune_linear_layer(self.q_lin, index)\n        self.k_lin = prune_linear_layer(self.k_lin, index)\n        self.v_lin = prune_linear_layer(self.v_lin, index)\n        self.out_lin = prune_linear_layer(self.out_lin, index, dim=1)\n        self.n_heads = self.n_heads - len(heads)\n        self.dim = attention_head_size * self.n_heads\n"
    },
    {
      "id": "jessevig_bertviz_236_1",
      "input_code": "    def prune_heads(self, heads):\n        if len(heads) == 0:\n            return\n        mask = torch.ones(self.self.num_attention_heads, self.self.attention_head_size)\n        for head in heads:\n            mask[head] = 0\n        mask = mask.view(-1).contiguous().eq(1)\n        index = torch.arange(len(mask))[mask].long()\n        self.self.query = prune_linear_layer(self.self.query, index)\n        self.self.key = prune_linear_layer(self.self.key, index)\n        self.self.value = prune_linear_layer(self.self.value, index)\n        self.output.dense = prune_linear_layer(self.output.dense, index, dim=1)\n        self.self.num_attention_heads = self.self.num_attention_heads - len(heads)\n        self.self.all_head_size = self.self.attention_head_size * self.self.num_attention_heads\n"
    },
    {
      "id": "jessevig_bertviz_236_2",
      "input_code": "def prune_layer(layer, index, dim=None):\n    \n    if isinstance(layer, nn.Linear):\n        return prune_linear_layer(layer, index, dim=0 if dim is None else dim)\n    elif isinstance(layer, Conv1D):\n        return prune_conv1d_layer(layer, index, dim=1 if dim is None else dim)\n    else:\n        raise ValueError(\"Can't prune layer of class {}\".format(layer.__class__))\n"
    }
  ],
  "generated_inputs": [
    "<needimport>\nimport torch\nimport torch.nn as nn\n\n<testcase_1>\nlayer = nn.Linear(10, 20)\nindex = torch.tensor([0, 2, 4, 6, 8])\ndim = 0\n\n<testcase_2>\nlayer = nn.Linear(15, 30)\nindex = torch.tensor([1, 3, 5, 7, 9, 11, 13])\ndim = 1\n\n<testcase_3>\nlayer = nn.Linear(25, 50)\nindex = torch.tensor([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])\ndim = 0\n\n<testcase_4>\nlayer = nn.Linear(8, 16)\nindex = torch.tensor([2, 4, 6])\ndim = 1\n\n<testcase_5>\nlayer = nn.Linear(12, 24)\nindex = torch.tensor([0, 3, 6, 9])\ndim = 0"
  ],
  "gpt_output": "<needimport>\nimport torch\nimport torch.nn as nn\n\n<testcase_1>\nlayer = nn.Linear(10, 20)\nindex = torch.tensor([0, 2, 4, 6, 8])\ndim = 0\n\n<testcase_2>\nlayer = nn.Linear(15, 30)\nindex = torch.tensor([1, 3, 5, 7, 9, 11, 13])\ndim = 1\n\n<testcase_3>\nlayer = nn.Linear(25, 50)\nindex = torch.tensor([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])\ndim = 0\n\n<testcase_4>\nlayer = nn.Linear(8, 16)\nindex = torch.tensor([2, 4, 6])\ndim = 1\n\n<testcase_5>\nlayer = nn.Linear(12, 24)\nindex = torch.tensor([0, 3, 6, 9])\ndim = 0",
  "needimport": "import torch\nimport torch.nn as nn\n\n",
  "test_results": {
    "ans1": {
      "training": true,
      "_parameters": {
        "weight": {},
        "bias": {}
      },
      "_buffers": {},
      "_non_persistent_buffers_set": [],
      "_backward_pre_hooks": {},
      "_backward_hooks": {},
      "_is_full_backward_hook": null,
      "_forward_hooks": {},
      "_forward_hooks_with_kwargs": {},
      "_forward_hooks_always_called": {},
      "_forward_pre_hooks": {},
      "_forward_pre_hooks_with_kwargs": {},
      "_state_dict_hooks": {},
      "_state_dict_pre_hooks": {},
      "_load_state_dict_pre_hooks": {},
      "_load_state_dict_post_hooks": {},
      "_modules": {},
      "in_features": 10,
      "out_features": 5
    },
    "ans2": {
      "training": true,
      "_parameters": {
        "weight": {},
        "bias": {}
      },
      "_buffers": {},
      "_non_persistent_buffers_set": [],
      "_backward_pre_hooks": {},
      "_backward_hooks": {},
      "_is_full_backward_hook": null,
      "_forward_hooks": {},
      "_forward_hooks_with_kwargs": {},
      "_forward_hooks_always_called": {},
      "_forward_pre_hooks": {},
      "_forward_pre_hooks_with_kwargs": {},
      "_state_dict_hooks": {},
      "_state_dict_pre_hooks": {},
      "_load_state_dict_pre_hooks": {},
      "_load_state_dict_post_hooks": {},
      "_modules": {},
      "in_features": 7,
      "out_features": 30
    },
    "ans3": {
      "training": true,
      "_parameters": {
        "weight": {},
        "bias": {}
      },
      "_buffers": {},
      "_non_persistent_buffers_set": [],
      "_backward_pre_hooks": {},
      "_backward_hooks": {},
      "_is_full_backward_hook": null,
      "_forward_hooks": {},
      "_forward_hooks_with_kwargs": {},
      "_forward_hooks_always_called": {},
      "_forward_pre_hooks": {},
      "_forward_pre_hooks_with_kwargs": {},
      "_state_dict_hooks": {},
      "_state_dict_pre_hooks": {},
      "_load_state_dict_pre_hooks": {},
      "_load_state_dict_post_hooks": {},
      "_modules": {},
      "in_features": 25,
      "out_features": 10
    },
    "ans4": {
      "training": true,
      "_parameters": {
        "weight": {},
        "bias": {}
      },
      "_buffers": {},
      "_non_persistent_buffers_set": [],
      "_backward_pre_hooks": {},
      "_backward_hooks": {},
      "_is_full_backward_hook": null,
      "_forward_hooks": {},
      "_forward_hooks_with_kwargs": {},
      "_forward_hooks_always_called": {},
      "_forward_pre_hooks": {},
      "_forward_pre_hooks_with_kwargs": {},
      "_state_dict_hooks": {},
      "_state_dict_pre_hooks": {},
      "_load_state_dict_pre_hooks": {},
      "_load_state_dict_post_hooks": {},
      "_modules": {},
      "in_features": 3,
      "out_features": 16
    },
    "ans5": {
      "training": true,
      "_parameters": {
        "weight": {},
        "bias": {}
      },
      "_buffers": {},
      "_non_persistent_buffers_set": [],
      "_backward_pre_hooks": {},
      "_backward_hooks": {},
      "_is_full_backward_hook": null,
      "_forward_hooks": {},
      "_forward_hooks_with_kwargs": {},
      "_forward_hooks_always_called": {},
      "_forward_pre_hooks": {},
      "_forward_pre_hooks_with_kwargs": {},
      "_state_dict_hooks": {},
      "_state_dict_pre_hooks": {},
      "_load_state_dict_pre_hooks": {},
      "_load_state_dict_post_hooks": {},
      "_modules": {},
      "in_features": 12,
      "out_features": 4
    }
  }
}