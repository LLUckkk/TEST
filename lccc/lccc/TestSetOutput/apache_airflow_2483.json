{
  "input_header": "def _is_primitive(cls, var: Any) -> bool:",
  "input_docstring": "Primitive types.",
  "output_code": "    def _is_primitive(cls, var: Any) -> bool:\n        \n        return var is None or isinstance(var, cls._primitive_types)",
  "input_contexts": [
    {
      "id": "apache_airflow_2483_2",
      "input_code": "    def deserialize(cls, encoded_var: Any) -> Any:\n        \n        if cls._is_primitive(encoded_var):\n            return encoded_var\n        elif isinstance(encoded_var, list):\n            return [cls.deserialize(v) for v in encoded_var]\n\n        if not isinstance(encoded_var, dict):\n            raise ValueError(f\"The encoded_var should be dict and is {type(encoded_var)}\")\n        var = encoded_var[Encoding.VAR]\n        type_ = encoded_var[Encoding.TYPE]\n        if type_ == DAT.TASK_CONTEXT:\n            d = {}\n            for k, v in var.items():\n                if k == \"task\":\n                    continue\n                d[k] = cls.deserialize(v)\n            d[\"task\"] = d[\"task_instance\"].task\n            d[\"macros\"] = macros\n            d[\"var\"] = {\n                \"json\": VariableAccessor(deserialize_json=True),\n                \"value\": VariableAccessor(deserialize_json=False),\n            }\n            d[\"conn\"] = ConnectionAccessor()\n            return Context(**d)\n        elif type_ == DAT.DICT:\n            return {k: cls.deserialize(v) for k, v in var.items()}\n        elif type_ == DAT.ASSET_EVENT_ACCESSORS:\n            return decode_outlet_event_accessors(var)\n        elif type_ == DAT.ASSET_UNIQUE_KEY:\n            return AssetUniqueKey(name=var[\"name\"], uri=var[\"uri\"])\n        elif type_ == DAT.ASSET_ALIAS_UNIQUE_KEY:\n            return AssetAliasUniqueKey(name=var[\"name\"])\n        elif type_ == DAT.DAG:\n            return SerializedDAG.deserialize_dag(var)\n        elif type_ == DAT.OP:\n            return SerializedBaseOperator.deserialize_operator(var)\n        elif type_ == DAT.DATETIME:\n            return from_timestamp(var)\n        elif type_ == DAT.POD:\n            if not _has_kubernetes():\n                raise RuntimeError(\"Cannot deserialize POD objects without kubernetes libraries installed!\")\n            pod = PodGenerator.deserialize_model_dict(var)\n            return pod\n        elif type_ == DAT.TIMEDELTA:\n            return datetime.timedelta(seconds=var)\n        elif type_ == DAT.TIMEZONE:\n            return decode_timezone(var)\n        elif type_ == DAT.RELATIVEDELTA:\n            return decode_relativedelta(var)\n        elif type_ == DAT.AIRFLOW_EXC_SER or type_ == DAT.BASE_EXC_SER:\n            deser = cls.deserialize(var)\n            exc_cls_name = deser[\"exc_cls_name\"]\n            args = deser[\"args\"]\n            kwargs = deser[\"kwargs\"]\n            del deser\n            if type_ == DAT.AIRFLOW_EXC_SER:\n                exc_cls = import_string(exc_cls_name)\n            else:\n                exc_cls = import_string(f\"builtins.{exc_cls_name}\")\n            return exc_cls(*args, **kwargs)\n        elif type_ == DAT.BASE_TRIGGER:\n            tr_cls_name, kwargs = cls.deserialize(var)\n            tr_cls = import_string(tr_cls_name)\n            return tr_cls(**kwargs)\n        elif type_ == DAT.SET:\n            return {cls.deserialize(v) for v in var}\n        elif type_ == DAT.TUPLE:\n            return tuple(cls.deserialize(v) for v in var)\n        elif type_ == DAT.PARAM:\n            return cls._deserialize_param(var)\n        elif type_ == DAT.XCOM_REF:\n            return _XComRef(var)\n        elif type_ == DAT.ASSET:\n            return Asset(**var)\n        elif type_ == DAT.ASSET_ALIAS:\n            return AssetAlias(**var)\n        elif type_ == DAT.ASSET_ANY:\n            return AssetAny(*(decode_asset_condition(x) for x in var[\"objects\"]))\n        elif type_ == DAT.ASSET_ALL:\n            return AssetAll(*(decode_asset_condition(x) for x in var[\"objects\"]))\n        elif type_ == DAT.ASSET_REF:\n            return Asset.ref(**var)\n        elif type_ == DAT.SIMPLE_TASK_INSTANCE:\n            return SimpleTaskInstance(**cls.deserialize(var))\n        elif type_ == DAT.CONNECTION:\n            return Connection(**var)\n        elif type_ == DAT.TASK_CALLBACK_REQUEST:\n            return TaskCallbackRequest.from_json(var)\n        elif type_ == DAT.DAG_CALLBACK_REQUEST:\n            return DagCallbackRequest.from_json(var)\n        elif type_ == DAT.TASK_INSTANCE_KEY:\n            return TaskInstanceKey(**var)\n        elif type_ == DAT.ARG_NOT_SET:\n            return NOTSET\n        else:\n            raise TypeError(f\"Invalid type {type_!s} in deserialization.\")\n"
    },
    {
      "id": "apache_airflow_2483_1",
      "input_code": "    def serialize(\n        cls, var: Any, *, strict: bool = False\n    ) -> Any:\n        \n        if cls._is_primitive(var):\n            if isinstance(var, enum.Enum):\n                return var.value\n            return var\n        elif isinstance(var, dict):\n            return cls._encode(\n                {str(k): cls.serialize(v, strict=strict) for k, v in var.items()},\n                type_=DAT.DICT,\n            )\n        elif isinstance(var, list):\n            return [cls.serialize(v, strict=strict) for v in var]\n        elif var.__class__.__name__ == \"V1Pod\" and _has_kubernetes() and isinstance(var, k8s.V1Pod):\n            json_pod = PodGenerator.serialize_pod(var)\n            return cls._encode(json_pod, type_=DAT.POD)\n        elif isinstance(var, OutletEventAccessors):\n            return cls._encode(\n                encode_outlet_event_accessors(var),\n                type_=DAT.ASSET_EVENT_ACCESSORS,\n            )\n        elif isinstance(var, AssetUniqueKey):\n            return cls._encode(\n                attrs.asdict(var),\n                type_=DAT.ASSET_UNIQUE_KEY,\n            )\n        elif isinstance(var, AssetAliasUniqueKey):\n            return cls._encode(\n                attrs.asdict(var),\n                type_=DAT.ASSET_ALIAS_UNIQUE_KEY,\n            )\n        elif isinstance(var, DAG):\n            return cls._encode(SerializedDAG.serialize_dag(var), type_=DAT.DAG)\n        elif isinstance(var, Resources):\n            return var.to_dict()\n        elif isinstance(var, MappedOperator):\n            return cls._encode(SerializedBaseOperator.serialize_mapped_operator(var), type_=DAT.OP)\n        elif isinstance(var, BaseOperator):\n            var._needs_expansion = var.get_needs_expansion()\n            return cls._encode(SerializedBaseOperator.serialize_operator(var), type_=DAT.OP)\n        elif isinstance(var, cls._datetime_types):\n            return cls._encode(var.timestamp(), type_=DAT.DATETIME)\n        elif isinstance(var, datetime.timedelta):\n            return cls._encode(var.total_seconds(), type_=DAT.TIMEDELTA)\n        elif isinstance(var, (Timezone, FixedTimezone)):\n            return cls._encode(encode_timezone(var), type_=DAT.TIMEZONE)\n        elif isinstance(var, relativedelta.relativedelta):\n            return cls._encode(encode_relativedelta(var), type_=DAT.RELATIVEDELTA)\n        elif isinstance(var, TaskInstanceKey):\n            return cls._encode(\n                var._asdict(),\n                type_=DAT.TASK_INSTANCE_KEY,\n            )\n        elif isinstance(var, (AirflowException, TaskDeferred)) and hasattr(var, \"serialize\"):\n            exc_cls_name, args, kwargs = var.serialize()\n            return cls._encode(\n                cls.serialize(\n                    {\"exc_cls_name\": exc_cls_name, \"args\": args, \"kwargs\": kwargs},\n                    strict=strict,\n                ),\n                type_=DAT.AIRFLOW_EXC_SER,\n            )\n        elif isinstance(var, (KeyError, AttributeError)):\n            return cls._encode(\n                cls.serialize(\n                    {\n                        \"exc_cls_name\": var.__class__.__name__,\n                        \"args\": [var.args],\n                        \"kwargs\": {},\n                    },\n                    strict=strict,\n                ),\n                type_=DAT.BASE_EXC_SER,\n            )\n        elif isinstance(var, BaseTrigger):\n            return cls._encode(\n                cls.serialize(\n                    var.serialize(),\n                    strict=strict,\n                ),\n                type_=DAT.BASE_TRIGGER,\n            )\n        elif callable(var):\n            return str(get_python_source(var))\n        elif isinstance(var, set):\n            try:\n                return cls._encode(\n                    sorted(cls.serialize(v, strict=strict) for v in var),\n                    type_=DAT.SET,\n                )\n            except TypeError:\n                return cls._encode(\n                    [cls.serialize(v, strict=strict) for v in var],\n                    type_=DAT.SET,\n                )\n        elif isinstance(var, tuple):\n            return cls._encode(\n                [cls.serialize(v, strict=strict) for v in var],\n                type_=DAT.TUPLE,\n            )\n        elif isinstance(var, TaskGroup):\n            return TaskGroupSerialization.serialize_task_group(var)\n        elif isinstance(var, Param):\n            return cls._encode(cls._serialize_param(var), type_=DAT.PARAM)\n        elif isinstance(var, XComArg):\n            return cls._encode(serialize_xcom_arg(var), type_=DAT.XCOM_REF)\n        elif isinstance(var, LazySelectSequence):\n            return cls.serialize(list(var))\n        elif isinstance(var, BaseAsset):\n            serialized_asset = encode_asset_condition(var)\n            return cls._encode(serialized_asset, type_=serialized_asset.pop(\"__type\"))\n        elif isinstance(var, AssetRef):\n            return cls._encode(attrs.asdict(var), type_=DAT.ASSET_REF)\n        elif isinstance(var, SimpleTaskInstance):\n            return cls._encode(\n                cls.serialize(var.__dict__, strict=strict),\n                type_=DAT.SIMPLE_TASK_INSTANCE,\n            )\n        elif isinstance(var, Connection):\n            return cls._encode(var.to_dict(validate=True), type_=DAT.CONNECTION)\n        elif isinstance(var, TaskCallbackRequest):\n            return cls._encode(var.to_json(), type_=DAT.TASK_CALLBACK_REQUEST)\n        elif isinstance(var, DagCallbackRequest):\n            return cls._encode(var.to_json(), type_=DAT.DAG_CALLBACK_REQUEST)\n        elif var.__class__ == Context:\n            d = {}\n            for k, v in var.items():\n                obj = cls.serialize(v, strict=strict)\n                d[str(k)] = obj\n            return cls._encode(d, type_=DAT.TASK_CONTEXT)\n        elif isinstance(var, ArgNotSet):\n            return cls._encode(None, type_=DAT.ARG_NOT_SET)\n        else:\n            return cls.default_serialization(strict, var)\n"
    }
  ],
  "generated_inputs": [
    "```python\n<needimport>\nimport datetime\n\n<testcase_1>\ncls = type('TestClass', (), {'_primitive_types': (int, float, str, bool)})\nvar = 42\n\n<testcase_2>\ncls = type('TestClass', (), {'_primitive_types': (int, float, str, bool)})\nvar = \"Hello, World!\"\n\n<testcase_3>\ncls = type('TestClass', (), {'_primitive_types': (int, float, str, bool)})\nvar = None\n\n<testcase_4>\ncls = type('TestClass', (), {'_primitive_types': (int, float, str, bool)})\nvar = 3.14\n\n<testcase_5>\ncls = type('TestClass', (), {'_primitive_types': (int, float, str, bool)})\nvar = True\n```"
  ],
  "gpt_output": "```python\n<needimport>\nimport datetime\n\n<testcase_1>\ncls = type('TestClass', (), {'_primitive_types': (int, float, str, bool)})\nvar = 42\n\n<testcase_2>\ncls = type('TestClass', (), {'_primitive_types': (int, float, str, bool)})\nvar = \"Hello, World!\"\n\n<testcase_3>\ncls = type('TestClass', (), {'_primitive_types': (int, float, str, bool)})\nvar = None\n\n<testcase_4>\ncls = type('TestClass', (), {'_primitive_types': (int, float, str, bool)})\nvar = 3.14\n\n<testcase_5>\ncls = type('TestClass', (), {'_primitive_types': (int, float, str, bool)})\nvar = True\n```",
  "needimport": "import datetime\n\n",
  "test_results": {
    "ans1": true,
    "ans2": false,
    "ans3": true,
    "ans4": true,
    "ans5": true
  }
}