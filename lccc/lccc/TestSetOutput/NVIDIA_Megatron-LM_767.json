{
  "input_header": "def condition_init_method(config, init_method):",
  "input_docstring": "Condition TE init_method on config.perform_initialization.",
  "output_code": "def condition_init_method(config, init_method):\n    \n    return init_method if config.perform_initialization else (lambda w: None)",
  "input_contexts": [
    {
      "id": "NVIDIA_Megatron-LM_767_3",
      "input_code": "    def __init__(\n        self,\n        input_size: int,\n        output_size: int,\n        *,\n        config: ModelParallelConfig,\n        init_method: Callable,\n        gather_output: bool,\n        bias: bool,\n        skip_bias_add: bool,\n        is_expert: bool,\n        skip_weight_param_allocation: bool = False,\n        tp_comm_buffer_name: str = None,\n    ):\n        if gather_output:\n            raise ValueError('Transformer Engine linear layers do not support gather_output = True')\n\n        super().__init__(\n            input_size=input_size,\n            output_size=output_size,\n            parallel_mode=\"column\",\n            config=config,\n            init_method=(\n                condition_init_method(config, init_method)\n                if not config.use_cpu_initialization\n                else lambda w: None\n            ),\n            bias=bias,\n            skip_bias_add=skip_bias_add,\n            is_expert=is_expert,\n            skip_weight_param_allocation=skip_weight_param_allocation,\n            tp_comm_buffer_name=tp_comm_buffer_name,\n        )\n\n        if config.use_cpu_initialization:\n            if is_expert:\n                world_size = get_expert_tensor_parallel_world_size()\n                rank = get_expert_tensor_parallel_rank()\n            else:\n                world_size = get_tensor_model_parallel_world_size()\n                rank = get_tensor_model_parallel_rank()\n            output_size_per_partition = divide(output_size, world_size)\n            _ = _initialize_affine_weight_cpu(\n                self.weight,\n                output_size,\n                input_size,\n                output_size_per_partition,\n                0,\n                init_method=condition_init_method(config, init_method),\n                stride=1,\n                return_master_weight=False,\n                rank=rank,\n                world_size=world_size,\n                skip_set_tensor_parallel_attributes=True,\n            )\n            if bias:\n                self.bias = Parameter(\n                    torch.empty(output_size_per_partition, dtype=config.params_dtype)\n                )\n                set_tensor_model_parallel_attributes(self.bias, True, 0, 1)\n                with torch.no_grad():\n                    self.bias.zero_()\n                setattr(self.bias, 'allreduce', True)\n"
    },
    {
      "id": "NVIDIA_Megatron-LM_767_1",
      "input_code": "    def __init__(\n        self,\n        input_size: int,\n        output_size: int,\n        *,\n        parallel_mode: str,\n        config: ModelParallelConfig,\n        init_method: Callable,\n        bias: bool,\n        skip_bias_add: bool,\n        skip_weight_param_allocation: bool,\n        tp_comm_buffer_name: str = None,\n        is_expert: bool = False,\n    ):\n        self.config = config\n\n        self.te_return_bias = skip_bias_add and bias\n        self.is_first_microbatch = True\n        self.disable_parameter_transpose_cache = self.config.disable_parameter_transpose_cache\n        if skip_weight_param_allocation:\n            raise ValueError(\n                'Transformer Engine linear layers do not support skip_weight_param_allocation'\n            )\n\n        extra_kwargs = _get_extra_te_kwargs(config)\n\n        if is_te_min_version(\"0.8.0\"):\n            if self.config.tp_comm_overlap:\n                if is_te_min_version(\"1.5.0\"):\n                    extra_kwargs[\"ub_overlap_ag\"] = (\n                        self.config.tp_comm_overlap_ag\n                        if hasattr(self.config, \"tp_comm_overlap_ag\")\n                        else self.config.tp_comm_split_ag or self.config.tp_comm_atomic_ag\n                    )\n                    extra_kwargs[\"ub_overlap_rs\"] = (\n                        self.config.tp_comm_overlap_rs\n                        if hasattr(self.config, \"tp_comm_overlap_rs\")\n                        else self.config.tp_comm_split_rs or self.config.tp_comm_atomic_rs\n                    )\n                    if is_expert:\n                        extra_kwargs[\"ub_overlap_ag\"] = False\n                        extra_kwargs[\"ub_overlap_rs\"] = False\n                else:\n                    extra_kwargs[\"ub_split_ag\"] = self.config.tp_comm_split_ag\n                    extra_kwargs[\"ub_atomic_gemm_ag\"] = self.config.tp_comm_atomic_ag\n                    extra_kwargs[\"ub_split_rs\"] = self.config.tp_comm_split_rs\n                    extra_kwargs[\"ub_atomic_gemm_rs\"] = self.config.tp_comm_atomic_rs\n                    if is_expert:\n                        extra_kwargs[\"ub_split_ag\"] = False\n                        extra_kwargs[\"ub_atomic_gemm_ag\"] = False\n                        extra_kwargs[\"ub_split_rs\"] = False\n                        extra_kwargs[\"ub_atomic_gemm_rs\"] = False\n                if is_te_min_version(\"1.0.0\", check_equality=False):\n                    assert (\n                        tp_comm_buffer_name is not None\n                    ), \"Buffer name should be set to configure communication overlap settings\"\n                    extra_kwargs[\"ub_name\"] = tp_comm_buffer_name\n\n        self.expert_parallel = self.config.expert_model_parallel_size > 1\n        if is_expert:\n            rng_tracker_name = get_expert_parallel_rng_tracker_name()\n        else:\n            rng_tracker_name = None\n        if is_te_min_version(\"1.7.0\"):\n            extra_kwargs[\"rng_tracker_name\"] = rng_tracker_name\n\n        if is_expert:\n            tp_group = get_expert_tensor_parallel_group(check_initialized=False)\n            tp_size = get_expert_tensor_parallel_world_size()\n        else:\n            tp_group = get_tensor_model_parallel_group(check_initialized=False)\n            tp_size = get_tensor_model_parallel_world_size()\n        explicit_expert_comm = is_expert and (tp_size > 1 or self.expert_parallel)\n\n        if explicit_expert_comm:\n            if parallel_mode == \"column\":\n                output_size = divide(output_size, tp_size)\n            elif parallel_mode == \"row\":\n                input_size = divide(input_size, tp_size)\n            parallel_mode = None\n            tp_size = 1\n            tp_group = None\n\n        super().__init__(\n            in_features=input_size,\n            out_features=output_size,\n            sequence_parallel=self.config.sequence_parallel,\n            fuse_wgrad_accumulation=self.config.gradient_accumulation_fusion,\n            tp_group=tp_group,\n            tp_size=tp_size,\n            get_rng_state_tracker=(\n                get_cuda_rng_tracker if get_cuda_rng_tracker().is_initialized() else None\n            ),\n            init_method=condition_init_method(config, init_method),\n            bias=bias,\n            return_bias=self.te_return_bias,\n            parallel_mode=parallel_mode,\n            **extra_kwargs,\n        )\n\n        for param in self.parameters():\n            setattr(param, 'allreduce', not (is_expert and self.expert_parallel))\n"
    },
    {
      "id": "NVIDIA_Megatron-LM_767_4",
      "input_code": "    def __init__(\n        self,\n        input_size: int,\n        output_size: int,\n        *,\n        config: ModelParallelConfig,\n        init_method: Callable,\n        bias: bool,\n        input_is_parallel: bool,\n        skip_bias_add: bool,\n        is_expert: bool,\n        tp_comm_buffer_name: str = None,\n    ):\n        if not input_is_parallel:\n            raise ValueError(\n                \"Transformer Engine linear layers do not support input_is_parallel = False\"\n            )\n\n        super().__init__(\n            input_size=input_size,\n            output_size=output_size,\n            parallel_mode=\"row\",\n            config=config,\n            init_method=(\n                condition_init_method(config, init_method)\n                if not config.use_cpu_initialization\n                else lambda w: None\n            ),\n            bias=bias,\n            skip_bias_add=skip_bias_add,\n            skip_weight_param_allocation=False,            is_expert=is_expert,\n            tp_comm_buffer_name=tp_comm_buffer_name,\n        )\n        if config.use_cpu_initialization:\n            if is_expert:\n                world_size = get_expert_tensor_parallel_world_size()\n                rank = get_expert_tensor_parallel_rank()\n            else:\n                world_size = get_tensor_model_parallel_world_size()\n                rank = get_tensor_model_parallel_rank()\n            input_size_per_partition = divide(input_size, world_size)\n            self.master_weight = _initialize_affine_weight_cpu(\n                self.weight,\n                output_size,\n                input_size,\n                input_size_per_partition,\n                1,\n                init_method=condition_init_method(config, init_method),\n                stride=1,\n                return_master_weight=False,\n                params_dtype=config.params_dtype,\n                rank=rank,\n                world_size=world_size,\n                skip_set_tensor_parallel_attributes=True,\n            )\n            if bias:\n                self.bias = Parameter(torch.empty(output_size, dtype=config.params_dtype))\n                with torch.no_grad():\n                    self.bias.zero_()\n                setattr(self.bias, 'allreduce', True)\n                setattr(self.bias, 'sequence_parallel', config.sequence_parallel)\n"
    },
    {
      "id": "NVIDIA_Megatron-LM_767_7",
      "input_code": "        def __init__(\n            self,\n            num_gemms: int,\n            input_size: int,\n            output_size: int,\n            *,\n            config: ModelParallelConfig,\n            init_method: Callable,\n            bias: bool,\n            skip_bias_add: bool,\n            is_expert: bool,\n            tp_comm_buffer_name: str = None,\n        ):\n\n            super().__init__(\n                num_gemms=num_gemms,\n                input_size=input_size,\n                output_size=output_size,\n                parallel_mode=\"row\",\n                config=config,\n                init_method=condition_init_method(config, init_method),\n                bias=bias,\n                skip_bias_add=skip_bias_add,\n                is_expert=is_expert,\n                tp_comm_buffer_name=tp_comm_buffer_name,\n            )\n"
    },
    {
      "id": "NVIDIA_Megatron-LM_767_5",
      "input_code": "        def __init__(\n            self,\n            num_gemms: int,\n            input_size: int,\n            output_size: int,\n            *,\n            parallel_mode: str,\n            config: ModelParallelConfig,\n            init_method: Callable,\n            bias: bool,\n            skip_bias_add: bool,\n            is_expert: bool = False,\n            tp_comm_buffer_name: str = None,\n        ):\n            self.config = config\n\n            self.te_return_bias = skip_bias_add and bias\n            self.is_first_microbatch = True\n            self.disable_parameter_transpose_cache = self.config.disable_parameter_transpose_cache\n\n            extra_kwargs = _get_extra_te_kwargs(config)\n            extra_kwargs[\"ub_name\"] = tp_comm_buffer_name\n\n            self.expert_parallel = self.config.expert_model_parallel_size > 1\n            if is_expert:\n                extra_kwargs[\"rng_tracker_name\"] = get_expert_parallel_rng_tracker_name()\n\n            if is_expert:\n                tp_group = get_expert_tensor_parallel_group(check_initialized=False)\n                tp_size = get_expert_tensor_parallel_world_size()\n            else:\n                tp_group = get_tensor_model_parallel_group(check_initialized=False)\n                tp_size = get_tensor_model_parallel_world_size()\n            self.explicit_expert_comm = is_expert and (tp_size > 1 or self.expert_parallel)\n\n            if self.explicit_expert_comm:\n                if parallel_mode == \"column\":\n                    output_size = divide(output_size, tp_size)\n                elif parallel_mode == \"row\":\n                    input_size = divide(input_size, tp_size)\n                parallel_mode = None\n                tp_size = 1\n                tp_group = None\n\n            super().__init__(\n                num_gemms=num_gemms,\n                in_features=input_size,\n                out_features=output_size,\n                sequence_parallel=self.config.sequence_parallel,\n                fuse_wgrad_accumulation=self.config.gradient_accumulation_fusion,\n                tp_group=tp_group,\n                tp_size=tp_size,\n                get_rng_state_tracker=(\n                    get_cuda_rng_tracker if get_cuda_rng_tracker().is_initialized() else None\n                ),\n                init_method=condition_init_method(config, init_method),\n                bias=bias,\n                return_bias=self.te_return_bias,\n                parallel_mode=parallel_mode,\n                **extra_kwargs,\n            )\n\n            for param in self.parameters():\n                setattr(param, 'allreduce', not (is_expert and self.expert_parallel))\n\n            def merge_extra_states(\n                self,\n                state_dict,\n                prefix,\n                local_metadata,\n                strict,\n                missing_keys,\n                unexpected_keys,\n                error_msgs,\n            ):\n                \n                self.init_fp8_metadata(num_gemms=self.num_gemms)\n                fp8_checkpoint = self.fp8_meta[\"fp8_checkpoint\"] or self.fp8 or self.fp8_calibration\n\n                try:\n                    state_list = [\n                        state_dict.pop(f\"{prefix}_extra_state{i}\") for i in range(1, self.num_gemms)\n                    ]\n                except KeyError:\n                    return\n\n                if not fp8_checkpoint:\n                    return\n                state_list = [state_dict.pop(f\"{prefix}_extra_state\")] + state_list\n                state_list = [self._decode_extra_state(state) for state in state_list]\n                extra_fp8_variables = state_list[0]['extra_fp8_variables']\n                extra_fp8_variables['num_gemms'] = self.num_gemms\n                extra_state = {\n                    \"scale_fwd\": torch.cat(\n                        [state['scale_fwd'].view(-1, 1) for state in state_list], dim=1\n                    ).view(-1),\n                    \"scale_inv_fwd\": torch.cat(\n                        [state['scale_inv_fwd'].view(-1, 1) for state in state_list], dim=1\n                    ).view(-1),\n                    \"amax_history_fwd\": torch.cat(\n                        [state['amax_history_fwd'].view(-1, 1) for state in state_list], dim=1\n                    ).view(self.fp8_meta[\"recipe\"].amax_history_len, -1),\n                    \"scale_bwd\": torch.cat(\n                        [state['scale_bwd'].view(-1, 1) for state in state_list], dim=1\n                    ).view(-1),\n                    \"scale_inv_bwd\": torch.cat(\n                        [state['scale_inv_bwd'].view(-1, 1) for state in state_list], dim=1\n                    ).view(-1),\n                    \"amax_history_bwd\": torch.cat(\n                        [state['amax_history_bwd'].view(-1, 1) for state in state_list], dim=1\n                    ).view(self.fp8_meta[\"recipe\"].amax_history_len, -1),\n                    \"extra_fp8_variables\": extra_fp8_variables,\n                }\n                state_dict[f\"{prefix}_extra_state\"] = self._encode_extra_state(extra_state)\n\n            self._register_load_state_dict_pre_hook(merge_extra_states, with_module=True)\n"
    },
    {
      "id": "NVIDIA_Megatron-LM_767_6",
      "input_code": "        def __init__(\n            self,\n            num_gemms: int,\n            input_size: int,\n            output_size: int,\n            *,\n            config: ModelParallelConfig,\n            init_method: Callable,\n            bias: bool,\n            skip_bias_add: bool,\n            is_expert: bool,\n            tp_comm_buffer_name: str = None,\n        ):\n\n            super().__init__(\n                num_gemms=num_gemms,\n                input_size=input_size,\n                output_size=output_size,\n                parallel_mode=\"column\",\n                config=config,\n                init_method=condition_init_method(config, init_method),\n                bias=bias,\n                skip_bias_add=skip_bias_add,\n                is_expert=is_expert,\n                tp_comm_buffer_name=tp_comm_buffer_name,\n            )\n"
    },
    {
      "id": "NVIDIA_Megatron-LM_767_2",
      "input_code": "    def __init__(\n        self,\n        input_size: int,\n        output_size: int,\n        *,\n        config: TransformerConfig,\n        init_method: Callable,\n        gather_output: bool,\n        bias: bool,\n        skip_bias_add: bool,\n        is_expert: bool,\n        skip_weight_param_allocation: bool = False,\n        tp_comm_buffer_name: str = None,\n    ):\n        self.config = config\n\n        if gather_output:\n            raise ValueError('Transformer Engine linear layers do not support gather_output = True')\n\n        if is_expert:\n            raise ValueError('Transformer Engine linear layers do not yet support MoE')\n\n        if skip_weight_param_allocation:\n            raise ValueError(\n                'Transformer Engine linear layers do not support skip_weight_param_allocation'\n            )\n\n        self.te_return_bias = skip_bias_add and bias\n        self.is_first_microbatch = True\n        self.disable_parameter_transpose_cache = self.config.disable_parameter_transpose_cache\n        extra_kwargs = _get_extra_te_kwargs(config)\n\n        if is_te_min_version(\"0.11.0\"):\n            extra_kwargs[\"normalization\"] = self.config.normalization\n        elif self.config.normalization != \"LayerNorm\":\n            te_version = get_te_version()\n            raise ValueError(\n                f\"Transformer Engine v{te_version} does not support {self.config.normalization}.\"\n            )\n\n        if is_te_min_version(\"0.8.0\"):\n            if self.config.tp_comm_overlap:\n                extra_kwargs[\"ub_bulk_wgrad\"] = self.config.tp_comm_bulk_wgrad\n                extra_kwargs[\"ub_bulk_dgrad\"] = self.config.tp_comm_bulk_dgrad\n                if is_te_min_version(\"1.5.0\", check_equality=False):\n                    extra_kwargs[\"ub_overlap_ag\"] = (\n                        self.config.tp_comm_overlap_ag\n                        if hasattr(self.config, \"tp_comm_overlap_ag\")\n                        else self.config.tp_comm_split_ag or self.config.tp_comm_atomic_ag\n                    )\n                    if is_te_min_version(\"1.6.0.dev0\", check_equality=False):\n                        extra_kwargs[\"ub_overlap_rs_dgrad\"] = (\n                            self.config.tp_comm_overlap_rs_dgrad\n                            if hasattr(self.config, \"tp_comm_overlap_rs_dgrad\")\n                            else False\n                        )\n                    if tp_comm_buffer_name == 'qkv' and self.config.tp_comm_overlap_disable_qkv:\n                        extra_kwargs[\"ub_overlap_ag\"] = False\n                        extra_kwargs[\"ub_overlap_rs_dgrad\"] = False\n\n                    if tp_comm_buffer_name == 'fc1' and self.config.tp_comm_overlap_disable_fc1:\n                        extra_kwargs[\"ub_overlap_ag\"] = False\n                        extra_kwargs[\"ub_overlap_rs_dgrad\"] = False\n                else:\n                    extra_kwargs[\"ub_atomic_gemm_ag\"] = self.config.tp_comm_atomic_ag\n                    extra_kwargs[\"ub_split_ag\"] = self.config.tp_comm_split_ag\n                if is_te_min_version(\"1.0.0\", check_equality=False):\n                    assert (\n                        tp_comm_buffer_name is not None\n                    ), \"Buffer name should be set to configure communication overlap settings\"\n                    extra_kwargs[\"ub_name\"] = tp_comm_buffer_name\n\n        super().__init__(\n            in_features=input_size,\n            out_features=output_size,\n            eps=self.config.layernorm_epsilon,\n            sequence_parallel=self.config.sequence_parallel,\n            fuse_wgrad_accumulation=self.config.gradient_accumulation_fusion,\n            tp_group=get_tensor_model_parallel_group(check_initialized=False),\n            tp_size=self.config.tensor_model_parallel_size,\n            get_rng_state_tracker=(\n                get_cuda_rng_tracker if get_cuda_rng_tracker().is_initialized() else None\n            ),\n            init_method=(\n                condition_init_method(config, init_method)\n                if not config.use_cpu_initialization\n                else lambda w: None\n            ),\n            bias=bias,\n            return_bias=self.te_return_bias,\n            parallel_mode=\"column\",\n            return_layernorm_output=False,\n            zero_centered_gamma=self.config.layernorm_zero_centered_gamma,\n            **extra_kwargs,\n        )\n\n        world_size = get_tensor_model_parallel_world_size()\n        rank = get_tensor_model_parallel_rank()\n\n        if config.use_cpu_initialization:\n            output_size_per_partition = divide(output_size, world_size)\n            _ = _initialize_affine_weight_cpu(\n                self.weight,\n                output_size,\n                input_size,\n                output_size_per_partition,\n                0,\n                init_method=condition_init_method(config, init_method),\n                stride=1,\n                return_master_weight=False,\n                rank=rank,\n                world_size=world_size,\n                skip_set_tensor_parallel_attributes=True,\n            )\n            if bias:\n                self.bias = Parameter(\n                    torch.empty(output_size_per_partition, dtype=config.params_dtype)\n                )\n                set_tensor_model_parallel_attributes(self.bias, True, 0, 1)\n                with torch.no_grad():\n                    self.bias.zero_()\n                setattr(self.bias, 'allreduce', True)\n"
    }
  ],
  "generated_inputs": [
    "<needimport>\nimport torch\nfrom typing import Callable\n\n<testcase_1>\nconfig = ModelParallelConfig(\n    perform_initialization=True,\n    use_cpu_initialization=False,\n    params_dtype=torch.float32,\n    sequence_parallel=True,\n    gradient_accumulation_fusion=True,\n    tp_comm_overlap=True,\n    tp_comm_split_ag=True,\n    tp_comm_atomic_ag=True,\n    tp_comm_split_rs=True,\n    tp_comm_atomic_rs=True,\n    expert_model_parallel_size=2,\n    disable_parameter_transpose_cache=False,\n    normalization=\"LayerNorm\",\n    layernorm_epsilon=1e-5,\n    layernorm_zero_centered_gamma=False,\n    tensor_model_parallel_size=2\n)\ninit_method = lambda w: torch.nn.init.xavier_uniform_(w)\n\n<testcase_2>\nconfig = ModelParallelConfig(\n    perform_initialization=False,\n    use_cpu_initialization=True,\n    params_dtype=torch.float16,\n    sequence_parallel=False,\n    gradient_accumulation_fusion=False,\n    tp_comm_overlap=False,\n    tp_comm_split_ag=False,\n    tp_comm_atomic_ag=False,\n    tp_comm_split_rs=False,\n    tp_comm_atomic_rs=False,\n    expert_model_parallel_size=1,\n    disable_parameter_transpose_cache=True,\n    normalization=\"BatchNorm\",\n    layernorm_epsilon=1e-6,\n    layernorm_zero_centered_gamma=True,\n    tensor_model_parallel_size=4\n)\ninit_method = lambda w: torch.nn.init.kaiming_normal_(w)\n\n<testcase_3>\nconfig = ModelParallelConfig(\n    perform_initialization=True,\n    use_cpu_initialization=False,\n    params_dtype=torch.float64,\n    sequence_parallel=True,\n    gradient_accumulation_fusion=True,\n    tp_comm_overlap=True,\n    tp_comm_split_ag=True,\n    tp_comm_atomic_ag=True,\n    tp_comm_split_rs=True,\n    tp_comm_atomic_rs=True,\n    expert_model_parallel_size=3,\n    disable_parameter_transpose_cache=False,\n    normalization=\"LayerNorm\",\n    layernorm_epsilon=1e-7,\n    layernorm_zero_centered_gamma=False,\n    tensor_model_parallel_size=8\n)\ninit_method = lambda w: torch.nn.init.constant_(w, 0.1)\n\n<testcase_4>\nconfig = ModelParallelConfig(\n    perform_initialization=False,\n    use_cpu_initialization=True,\n    params_dtype=torch.float32,\n    sequence_parallel=False,\n    gradient_accumulation_fusion=False,\n    tp_comm_overlap=False,\n    tp_comm_split_ag=False,\n    tp_comm_atomic_ag=False,\n    tp_comm_split_rs=False,\n    tp_comm_atomic_rs=False,\n    expert_model_parallel_size=1,\n    disable_parameter_transpose_cache=True,\n    normalization=\"LayerNorm\",\n    layernorm_epsilon=1e-8,\n    layernorm_zero_centered_gamma=True,\n    tensor_model_parallel_size=1\n)\ninit_method = lambda w: torch.nn.init.uniform_(w, -0.1, 0.1)\n\n<testcase_5>\nconfig = ModelParallelConfig(\n    perform_initialization=True,\n    use_cpu_initialization=False,\n    params_dtype=torch.float16,\n    sequence_parallel=True,\n    gradient_accumulation_fusion=True,\n    tp_comm_overlap=True,\n    tp_comm_split_ag=True,\n    tp_comm_atomic_ag=True,\n    tp_comm_split_rs=True,\n    tp_comm_atomic_rs=True,\n    expert_model_parallel_size=4,\n    disable_parameter_transpose_cache=False,\n    normalization=\"LayerNorm\",\n    layernorm_epsilon=1e-9,\n    layernorm_zero_centered_gamma=False,\n    tensor_model_parallel_size=16\n)\ninit_method = lambda w: torch.nn.init.normal_(w, mean=0.0, std=0.02)"
  ],
  "gpt_output": "<needimport>\nimport torch\nfrom typing import Callable\n\n<testcase_1>\nconfig = ModelParallelConfig(\n    perform_initialization=True,\n    use_cpu_initialization=False,\n    params_dtype=torch.float32,\n    sequence_parallel=True,\n    gradient_accumulation_fusion=True,\n    tp_comm_overlap=True,\n    tp_comm_split_ag=True,\n    tp_comm_atomic_ag=True,\n    tp_comm_split_rs=True,\n    tp_comm_atomic_rs=True,\n    expert_model_parallel_size=2,\n    disable_parameter_transpose_cache=False,\n    normalization=\"LayerNorm\",\n    layernorm_epsilon=1e-5,\n    layernorm_zero_centered_gamma=False,\n    tensor_model_parallel_size=2\n)\ninit_method = lambda w: torch.nn.init.xavier_uniform_(w)\n\n<testcase_2>\nconfig = ModelParallelConfig(\n    perform_initialization=False,\n    use_cpu_initialization=True,\n    params_dtype=torch.float16,\n    sequence_parallel=False,\n    gradient_accumulation_fusion=False,\n    tp_comm_overlap=False,\n    tp_comm_split_ag=False,\n    tp_comm_atomic_ag=False,\n    tp_comm_split_rs=False,\n    tp_comm_atomic_rs=False,\n    expert_model_parallel_size=1,\n    disable_parameter_transpose_cache=True,\n    normalization=\"BatchNorm\",\n    layernorm_epsilon=1e-6,\n    layernorm_zero_centered_gamma=True,\n    tensor_model_parallel_size=4\n)\ninit_method = lambda w: torch.nn.init.kaiming_normal_(w)\n\n<testcase_3>\nconfig = ModelParallelConfig(\n    perform_initialization=True,\n    use_cpu_initialization=False,\n    params_dtype=torch.float64,\n    sequence_parallel=True,\n    gradient_accumulation_fusion=True,\n    tp_comm_overlap=True,\n    tp_comm_split_ag=True,\n    tp_comm_atomic_ag=True,\n    tp_comm_split_rs=True,\n    tp_comm_atomic_rs=True,\n    expert_model_parallel_size=3,\n    disable_parameter_transpose_cache=False,\n    normalization=\"LayerNorm\",\n    layernorm_epsilon=1e-7,\n    layernorm_zero_centered_gamma=False,\n    tensor_model_parallel_size=8\n)\ninit_method = lambda w: torch.nn.init.constant_(w, 0.1)\n\n<testcase_4>\nconfig = ModelParallelConfig(\n    perform_initialization=False,\n    use_cpu_initialization=True,\n    params_dtype=torch.float32,\n    sequence_parallel=False,\n    gradient_accumulation_fusion=False,\n    tp_comm_overlap=False,\n    tp_comm_split_ag=False,\n    tp_comm_atomic_ag=False,\n    tp_comm_split_rs=False,\n    tp_comm_atomic_rs=False,\n    expert_model_parallel_size=1,\n    disable_parameter_transpose_cache=True,\n    normalization=\"LayerNorm\",\n    layernorm_epsilon=1e-8,\n    layernorm_zero_centered_gamma=True,\n    tensor_model_parallel_size=1\n)\ninit_method = lambda w: torch.nn.init.uniform_(w, -0.1, 0.1)\n\n<testcase_5>\nconfig = ModelParallelConfig(\n    perform_initialization=True,\n    use_cpu_initialization=False,\n    params_dtype=torch.float16,\n    sequence_parallel=True,\n    gradient_accumulation_fusion=True,\n    tp_comm_overlap=True,\n    tp_comm_split_ag=True,\n    tp_comm_atomic_ag=True,\n    tp_comm_split_rs=True,\n    tp_comm_atomic_rs=True,\n    expert_model_parallel_size=4,\n    disable_parameter_transpose_cache=False,\n    normalization=\"LayerNorm\",\n    layernorm_epsilon=1e-9,\n    layernorm_zero_centered_gamma=False,\n    tensor_model_parallel_size=16\n)\ninit_method = lambda w: torch.nn.init.normal_(w, mean=0.0, std=0.02)",
  "needimport": "import torch\nfrom typing import Callable\n\n",
  "test_results": {
    "ans1": "<lambda>",
    "ans2": "<lambda>",
    "ans3": "<lambda>",
    "ans4": "<lambda>",
    "ans5": "<lambda>"
  }
}