{
  "input_header": "def get_num_params(self, non_embedding=True):",
  "input_docstring": "Return the number of parameters in the model.\nFor non-embedding count (default), the position embeddings get subtracted.\nThe token embeddings would too, except due to the parameter sharing these\nparams are actually used as weights in the final layer, so we include them.",
  "output_code": "    def get_num_params(self, non_embedding=True):\n        \n        n_params = sum(p.numel() for p in self.parameters())\n        if non_embedding:\n            n_params -= self.transformer.wpe.weight.numel()\n        return n_params",
  "input_contexts": [
    {
      "id": "karpathy_nanoGPT_13_1",
      "input_code": "    def __init__(self, config):\n        super().__init__()\n        assert config.vocab_size is not None\n        assert config.block_size is not None\n        self.config = config\n\n        self.transformer = nn.ModuleDict(dict(\n            wte = nn.Embedding(config.vocab_size, config.n_embd),\n            wpe = nn.Embedding(config.block_size, config.n_embd),\n            drop = nn.Dropout(config.dropout),\n            h = nn.ModuleList([Block(config) for _ in range(config.n_layer)]),\n            ln_f = LayerNorm(config.n_embd, bias=config.bias),\n        ))\n        self.lm_head = nn.Linear(config.n_embd, config.vocab_size, bias=False)\n        self.transformer.wte.weight = self.lm_head.weight\n\n        self.apply(self._init_weights)\n        for pn, p in self.named_parameters():\n            if pn.endswith('c_proj.weight'):\n                torch.nn.init.normal_(p, mean=0.0, std=0.02/math.sqrt(2 * config.n_layer))\n\n        print(\"number of parameters: %.2fM\" % (self.get_num_params()/1e6,))\n"
    },
    {
      "id": "karpathy_nanoGPT_13_2",
      "input_code": "    def estimate_mfu(self, fwdbwd_per_iter, dt):\n        \n        N = self.get_num_params()\n        cfg = self.config\n        L, H, Q, T = cfg.n_layer, cfg.n_head, cfg.n_embd//cfg.n_head, cfg.block_size\n        flops_per_token = 6*N + 12*L*H*Q*T\n        flops_per_fwdbwd = flops_per_token * T\n        flops_per_iter = flops_per_fwdbwd * fwdbwd_per_iter\n        flops_achieved = flops_per_iter * (1.0/dt)\n        flops_promised = 312e12\n        mfu = flops_achieved / flops_promised\n        return mfu\n"
    }
  ],
  "generated_inputs": [
    "<needimport>\nimport torch\nimport torch.nn as nn\nimport math\n\n<testcase_1>\nself = type('test', (object,), {\n    'parameters': lambda self: [torch.randn(10, 10), torch.randn(20, 20)],\n    'transformer': type('transformer', (object,), {\n        'wpe': type('wpe', (object,), {'weight': torch.randn(30, 30)})\n    })\n})()\nnon_embedding = True\n\n<testcase_2>\nself = type('test', (object,), {\n    'parameters': lambda self: [torch.randn(50, 50), torch.randn(100, 100)],\n    'transformer': type('transformer', (object,), {\n        'wpe': type('wpe', (object,), {'weight': torch.randn(40, 40)})\n    })\n})()\nnon_embedding = False\n\n<testcase_3>\nself = type('test', (object,), {\n    'parameters': lambda self: [torch.randn(5, 5), torch.randn(15, 15)],\n    'transformer': type('transformer', (object,), {\n        'wpe': type('wpe', (object,), {'weight': torch.randn(10, 10)})\n    })\n})()\nnon_embedding = True\n\n<testcase_4>\nself = type('test', (object,), {\n    'parameters': lambda self: [torch.randn(25, 25), torch.randn(35, 35)],\n    'transformer': type('transformer', (object,), {\n        'wpe': type('wpe', (object,), {'weight': torch.randn(20, 20)})\n    })\n})()\nnon_embedding = False\n\n<testcase_5>\nself = type('test', (object,), {\n    'parameters': lambda self: [torch.randn(60, 60), torch.randn(80, 80)],\n    'transformer': type('transformer', (object,), {\n        'wpe': type('wpe', (object,), {'weight': torch.randn(50, 50)})\n    })\n})()\nnon_embedding = True"
  ],
  "gpt_output": "<needimport>\nimport torch\nimport torch.nn as nn\nimport math\n\n<testcase_1>\nself = type('test', (object,), {\n    'parameters': lambda self: [torch.randn(10, 10), torch.randn(20, 20)],\n    'transformer': type('transformer', (object,), {\n        'wpe': type('wpe', (object,), {'weight': torch.randn(30, 30)})\n    })\n})()\nnon_embedding = True\n\n<testcase_2>\nself = type('test', (object,), {\n    'parameters': lambda self: [torch.randn(50, 50), torch.randn(100, 100)],\n    'transformer': type('transformer', (object,), {\n        'wpe': type('wpe', (object,), {'weight': torch.randn(40, 40)})\n    })\n})()\nnon_embedding = False\n\n<testcase_3>\nself = type('test', (object,), {\n    'parameters': lambda self: [torch.randn(5, 5), torch.randn(15, 15)],\n    'transformer': type('transformer', (object,), {\n        'wpe': type('wpe', (object,), {'weight': torch.randn(10, 10)})\n    })\n})()\nnon_embedding = True\n\n<testcase_4>\nself = type('test', (object,), {\n    'parameters': lambda self: [torch.randn(25, 25), torch.randn(35, 35)],\n    'transformer': type('transformer', (object,), {\n        'wpe': type('wpe', (object,), {'weight': torch.randn(20, 20)})\n    })\n})()\nnon_embedding = False\n\n<testcase_5>\nself = type('test', (object,), {\n    'parameters': lambda self: [torch.randn(60, 60), torch.randn(80, 80)],\n    'transformer': type('transformer', (object,), {\n        'wpe': type('wpe', (object,), {'weight': torch.randn(50, 50)})\n    })\n})()\nnon_embedding = True",
  "needimport": "import torch\nimport torch.nn as nn\nimport math\n\n",
  "test_results": {
    "ans1": -400,
    "ans2": 12500,
    "ans3": 150,
    "ans4": 1850,
    "ans5": 7500
  }
}