{
  "input_header": "def get_type(cls, model_name: str):",
  "input_docstring": null,
  "output_code": "    def get_type(cls, model_name: str):\n        model_type = MODEL_METADATA[model_name][\"model_type\"]\n        if model_type is not None:\n            for member in cls:\n                if member.name == model_type:\n                    return member\n\n        model_type = None\n        model_name_lower = model_name.lower()\n        if \"gpt\" in model_name_lower:\n            try:\n                assert MODEL_METADATA[model_name][\"multimodal\"] == True\n                model_type = ModelType.OpenAIVision\n            except:\n                if \"instruct\" in model_name_lower:\n                    model_type = ModelType.OpenAIInstruct\n                elif \"vision\" in model_name_lower:\n                    model_type = ModelType.OpenAIVision\n                else:\n                    model_type = ModelType.OpenAI\n        elif \"chatglm\" in model_name_lower:\n            model_type = ModelType.ChatGLM\n        elif \"groq\" in model_name_lower:\n            model_type = ModelType.Groq\n        elif \"ollama\" in model_name_lower:\n            model_type = ModelType.Ollama\n        elif \"llama\" in model_name_lower or \"alpaca\" in model_name_lower:\n            model_type = ModelType.LLaMA\n        elif \"xmchat\" in model_name_lower:\n            model_type = ModelType.XMChat\n        elif \"stablelm\" in model_name_lower:\n            model_type = ModelType.StableLM\n        elif \"moss\" in model_name_lower:\n            model_type = ModelType.MOSS\n        elif \"yuanai\" in model_name_lower:\n            model_type = ModelType.YuanAI\n        elif \"minimax\" in model_name_lower:\n            model_type = ModelType.Minimax\n        elif \"川虎助理\" in model_name_lower:\n            model_type = ModelType.ChuanhuAgent\n        elif \"palm\" in model_name_lower:\n            model_type = ModelType.GooglePaLM\n        elif \"gemini\" in model_name_lower:\n            model_type = ModelType.GoogleGemini\n        elif \"midjourney\" in model_name_lower:\n            model_type = ModelType.Midjourney\n        elif \"azure\" in model_name_lower or \"api\" in model_name_lower:\n            model_type = ModelType.LangchainChat\n        elif \"讯飞星火\" in model_name_lower:\n            model_type = ModelType.Spark\n        elif \"claude\" in model_name_lower:\n            model_type = ModelType.Claude\n        elif \"qwen\" in model_name_lower:\n            model_type = ModelType.Qwen\n        elif \"ernie\" in model_name_lower:\n            model_type = ModelType.ERNIE\n        elif \"dall\" in model_name_lower:\n            model_type = ModelType.DALLE3\n        elif \"gemma\" in model_name_lower:\n            model_type = ModelType.GoogleGemma\n        else:\n            model_type = ModelType.LLaMA\n        return model_type",
  "input_contexts": [
    {
      "id": "GaiZhenbiao_ChuanhuChatGPT_265_1",
      "input_code": "    def __init__(\n        self,\n        model_name,\n        user=\"\",\n        config=None,\n    ) -> None:\n\n        if config is not None:\n            temp = MODEL_METADATA[model_name].copy()\n            keys_with_diff_values = {key: temp[key] for key in temp if key in DEFAULT_METADATA and temp[key] != DEFAULT_METADATA[key]}\n            config.update(keys_with_diff_values)\n            temp.update(config)\n            config = temp\n        else:\n            config = MODEL_METADATA[model_name]\n\n        self.model_name = config[\"model_name\"]\n        self.multimodal = config[\"multimodal\"]\n        self.description = config[\"description\"]\n        self.placeholder = config[\"placeholder\"]\n        self.token_upper_limit = config[\"token_limit\"]\n        self.system_prompt = config[\"system\"]\n        self.api_key = config[\"api_key\"]\n        self.api_host = config[\"api_host\"]\n        self.stream = config[\"stream\"]\n\n        self.interrupted = False\n        self.need_api_key = self.api_key is not None\n        self.history = []\n        self.all_token_counts = []\n        self.model_type = ModelType.get_type(model_name)\n        self.history_file_path = get_first_history_name(user)\n        self.user_name = user\n        self.chatbot = []\n\n        self.default_single_turn = config[\"single_turn\"]\n        self.default_temperature = config[\"temperature\"]\n        self.default_top_p = config[\"top_p\"]\n        self.default_n_choices = config[\"n_choices\"]\n        self.default_stop_sequence = config[\"stop\"]\n        self.default_max_generation_token = config[\"max_generation\"]\n        self.default_presence_penalty = config[\"presence_penalty\"]\n        self.default_frequency_penalty = config[\"frequency_penalty\"]\n        self.default_logit_bias = config[\"logit_bias\"]\n        self.default_user_identifier = user\n        self.default_stream = config[\"stream\"]\n\n        self.single_turn = self.default_single_turn\n        self.temperature = self.default_temperature\n        self.top_p = self.default_top_p\n        self.n_choices = self.default_n_choices\n        self.stop_sequence = self.default_stop_sequence\n        self.max_generation_token = self.default_max_generation_token\n        self.presence_penalty = self.default_presence_penalty\n        self.frequency_penalty = self.default_frequency_penalty\n        self.logit_bias = self.default_logit_bias\n        self.user_identifier = user\n\n        self.metadata = config[\"metadata\"]\n"
    },
    {
      "id": "GaiZhenbiao_ChuanhuChatGPT_265_2",
      "input_code": "def get_model(\n    model_name,\n    lora_model_path=None,\n    access_key=None,\n    temperature=None,\n    top_p=None,\n    system_prompt=None,\n    user_name=\"\",\n    original_model = None\n) -> BaseLLMModel:\n    msg = i18n(\"模型设置为了：\") + f\" {model_name}\"\n    model_type = ModelType.get_type(model_name)\n    lora_selector_visibility = False\n    lora_choices = [\"No LoRA\"]\n    dont_change_lora_selector = False\n    if model_type != ModelType.OpenAI:\n        config.local_embedding = True\n    model = original_model\n    try:\n        if model_type == ModelType.OpenAIVision or model_type == ModelType.OpenAI:\n            logging.info(f\"正在加载 OpenAI 模型: {model_name}\")\n            from .OpenAIVision import OpenAIVisionClient\n            access_key = os.environ.get(\"OPENAI_API_KEY\", access_key)\n            model = OpenAIVisionClient(\n                model_name, api_key=access_key, user_name=user_name)\n        elif model_type == ModelType.OpenAIInstruct:\n            logging.info(f\"正在加载OpenAI Instruct模型: {model_name}\")\n            from .OpenAIInstruct import OpenAI_Instruct_Client\n            access_key = os.environ.get(\"OPENAI_API_KEY\", access_key)\n            model = OpenAI_Instruct_Client(\n                model_name, api_key=access_key, user_name=user_name)\n        elif model_type == ModelType.ChatGLM:\n            logging.info(f\"正在加载ChatGLM模型: {model_name}\")\n            from .ChatGLM import ChatGLM_Client\n            model = ChatGLM_Client(model_name, user_name=user_name)\n        elif model_type == ModelType.Groq:\n            logging.info(f\"正在加载Groq模型: {model_name}\")\n            from .Groq import Groq_Client\n            model = Groq_Client(model_name, access_key, user_name=user_name)\n        elif model_type == ModelType.LLaMA and lora_model_path == \"\":\n            msg = f\"现在请为 {model_name} 选择LoRA模型\"\n            logging.info(msg)\n            lora_selector_visibility = True\n            if os.path.isdir(\"lora\"):\n                lora_choices = [\"No LoRA\"] + get_file_names_by_pinyin(\"lora\", filetypes=[\"\"])\n        elif model_type == ModelType.LLaMA and lora_model_path != \"\":\n            logging.info(f\"正在加载LLaMA模型: {model_name} + {lora_model_path}\")\n            from .LLaMA import LLaMA_Client\n            dont_change_lora_selector = True\n            if lora_model_path == \"No LoRA\":\n                lora_model_path = None\n                msg += \" + No LoRA\"\n            else:\n                msg += f\" + {lora_model_path}\"\n            model = LLaMA_Client(\n                model_name, lora_model_path, user_name=user_name)\n        elif model_type == ModelType.XMChat:\n            from .XMChat import XMChat\n            if os.environ.get(\"XMCHAT_API_KEY\") != \"\":\n                access_key = os.environ.get(\"XMCHAT_API_KEY\")\n            model = XMChat(api_key=access_key, user_name=user_name)\n        elif model_type == ModelType.StableLM:\n            from .StableLM import StableLM_Client\n            model = StableLM_Client(model_name, user_name=user_name)\n        elif model_type == ModelType.MOSS:\n            from .MOSS import MOSS_Client\n            model = MOSS_Client(model_name, user_name=user_name)\n        elif model_type == ModelType.YuanAI:\n            from .inspurai import Yuan_Client\n            model = Yuan_Client(model_name, api_key=access_key,\n                                user_name=user_name, system_prompt=system_prompt)\n        elif model_type == ModelType.Minimax:\n            from .minimax import MiniMax_Client\n            if os.environ.get(\"MINIMAX_API_KEY\") != \"\":\n                access_key = os.environ.get(\"MINIMAX_API_KEY\")\n            model = MiniMax_Client(\n                model_name, api_key=access_key, user_name=user_name, system_prompt=system_prompt)\n        elif model_type == ModelType.ChuanhuAgent:\n            from .ChuanhuAgent import ChuanhuAgent_Client\n            model = ChuanhuAgent_Client(model_name, access_key, user_name=user_name)\n            msg = i18n(\"启用的工具：\") + \", \".join([i.name for i in model.tools])\n        elif model_type == ModelType.GooglePaLM:\n            from .GooglePaLM import Google_PaLM_Client\n            access_key = os.environ.get(\"GOOGLE_GENAI_API_KEY\", access_key)\n            model = Google_PaLM_Client(\n                model_name, access_key, user_name=user_name)\n        elif model_type == ModelType.GoogleGemini:\n            from .GoogleGemini import GoogleGeminiClient\n            access_key = os.environ.get(\"GOOGLE_GENAI_API_KEY\", access_key)\n            model = GoogleGeminiClient(\n                model_name, access_key, user_name=user_name)\n        elif model_type == ModelType.LangchainChat:\n            from .Azure import Azure_OpenAI_Client\n            model = Azure_OpenAI_Client(model_name, user_name=user_name)\n        elif model_type == ModelType.Midjourney:\n            from .midjourney import Midjourney_Client\n            mj_proxy_api_secret = os.getenv(\"MIDJOURNEY_PROXY_API_SECRET\")\n            model = Midjourney_Client(\n                model_name, mj_proxy_api_secret, user_name=user_name)\n        elif model_type == ModelType.Spark:\n            from .spark import Spark_Client\n            model = Spark_Client(model_name, os.getenv(\"SPARK_APPID\"), os.getenv(\n                \"SPARK_API_KEY\"), os.getenv(\"SPARK_API_SECRET\"), user_name=user_name)\n        elif model_type == ModelType.Claude:\n            from .Claude import Claude_Client\n            model = Claude_Client(model_name=model_name, api_secret=os.getenv(\"CLAUDE_API_SECRET\"))\n        elif model_type == ModelType.Qwen:\n            from .Qwen import Qwen_Client\n            model = Qwen_Client(model_name, user_name=user_name)\n        elif model_type == ModelType.ERNIE:\n            from .ERNIE import ERNIE_Client\n            model = ERNIE_Client(model_name, api_key=os.getenv(\"ERNIE_APIKEY\"),secret_key=os.getenv(\"ERNIE_SECRETKEY\"))\n        elif model_type == ModelType.DALLE3:\n            from .DALLE3 import OpenAI_DALLE3_Client\n            access_key = os.environ.get(\"OPENAI_API_KEY\", access_key)\n            model = OpenAI_DALLE3_Client(model_name, api_key=access_key, user_name=user_name)\n        elif model_type == ModelType.Ollama:\n            from .Ollama import OllamaClient\n            ollama_host = os.environ.get(\"OLLAMA_HOST\", access_key)\n            model = OllamaClient(model_name, user_name=user_name, backend_model=lora_model_path)\n            model_list = model.get_model_list()\n            lora_selector_visibility = True\n            lora_choices = [i[\"name\"] for i in model_list[\"models\"]]\n        elif model_type == ModelType.GoogleGemma:\n            from .GoogleGemma import GoogleGemmaClient\n            model = GoogleGemmaClient(\n                model_name, access_key, user_name=user_name)\n        elif model_type == ModelType.Unknown:\n            raise ValueError(f\"Unknown model: {model_name}\")\n        else:\n            raise ValueError(f\"Unimplemented model type: {model_type}\")\n        logging.info(msg)\n    except Exception as e:\n        import traceback\n        traceback.print_exc()\n        msg = f\"{STANDARD_ERROR_MSG}: {e}\"\n    modelDescription = i18n(model.description)\n    presudo_key = hide_middle_chars(access_key)\n    if original_model is not None and model is not None:\n        model.history = original_model.history\n        model.history_file_path = original_model.history_file_path\n        model.system_prompt = original_model.system_prompt\n    if dont_change_lora_selector:\n        return model, msg, gr.update(label=model_name, placeholder=setPlaceholder(model=model)), gr.update(), access_key, presudo_key, modelDescription, model.stream\n    else:\n        return model, msg, gr.update(label=model_name, placeholder=setPlaceholder(model=model)), gr.Dropdown(choices=lora_choices, visible=lora_selector_visibility), access_key, presudo_key, modelDescription, model.stream\n"
    }
  ],
  "generated_inputs": [
    "<needimport>\nimport enum\n\nclass ModelType(enum.Enum):\n    OpenAIVision = 1\n    OpenAI = 2\n    OpenAIInstruct = 3\n    ChatGLM = 4\n    Groq = 5\n    LLaMA = 6\n    XMChat = 7\n    StableLM = 8\n    MOSS = 9\n    YuanAI = 10\n    Minimax = 11\n    ChuanhuAgent = 12\n    GooglePaLM = 13\n    GoogleGemini = 14\n    LangchainChat = 15\n    Midjourney = 16\n    Spark = 17\n    Claude = 18\n    Qwen = 19\n    ERNIE = 20\n    DALLE3 = 21\n    Ollama = 22\n    GoogleGemma = 23\n    Unknown = 24\n\nMODEL_METADATA = {\n    \"GPT-3\": {\"model_type\": \"OpenAI\", \"multimodal\": False},\n    \"ChatGLM-6B\": {\"model_type\": \"ChatGLM\", \"multimodal\": False},\n    \"Claude-v1\": {\"model_type\": \"Claude\", \"multimodal\": False},\n    \"StableLM-alpha\": {\"model_type\": \"StableLM\", \"multimodal\": False},\n    \"Midjourney-v4\": {\"model_type\": \"Midjourney\", \"multimodal\": True},\n}\n\n<testcase_1>\ncls = ModelType\nmodel_name = \"GPT-3\"\n\n<testcase_2>\ncls = ModelType\nmodel_name = \"ChatGLM-6B\"\n\n<testcase_3>\ncls = ModelType\nmodel_name = \"Claude-v1\"\n\n<testcase_4>\ncls = ModelType\nmodel_name = \"StableLM-alpha\"\n\n<testcase_5>\ncls = ModelType\nmodel_name = \"Midjourney-v4\""
  ],
  "gpt_output": "<needimport>\nimport enum\n\nclass ModelType(enum.Enum):\n    OpenAIVision = 1\n    OpenAI = 2\n    OpenAIInstruct = 3\n    ChatGLM = 4\n    Groq = 5\n    LLaMA = 6\n    XMChat = 7\n    StableLM = 8\n    MOSS = 9\n    YuanAI = 10\n    Minimax = 11\n    ChuanhuAgent = 12\n    GooglePaLM = 13\n    GoogleGemini = 14\n    LangchainChat = 15\n    Midjourney = 16\n    Spark = 17\n    Claude = 18\n    Qwen = 19\n    ERNIE = 20\n    DALLE3 = 21\n    Ollama = 22\n    GoogleGemma = 23\n    Unknown = 24\n\nMODEL_METADATA = {\n    \"GPT-3\": {\"model_type\": \"OpenAI\", \"multimodal\": False},\n    \"ChatGLM-6B\": {\"model_type\": \"ChatGLM\", \"multimodal\": False},\n    \"Claude-v1\": {\"model_type\": \"Claude\", \"multimodal\": False},\n    \"StableLM-alpha\": {\"model_type\": \"StableLM\", \"multimodal\": False},\n    \"Midjourney-v4\": {\"model_type\": \"Midjourney\", \"multimodal\": True},\n}\n\n<testcase_1>\ncls = ModelType\nmodel_name = \"GPT-3\"\n\n<testcase_2>\ncls = ModelType\nmodel_name = \"ChatGLM-6B\"\n\n<testcase_3>\ncls = ModelType\nmodel_name = \"Claude-v1\"\n\n<testcase_4>\ncls = ModelType\nmodel_name = \"StableLM-alpha\"\n\n<testcase_5>\ncls = ModelType\nmodel_name = \"Midjourney-v4\"",
  "needimport": "import enum\n\nclass ModelType(enum.Enum):\n    OpenAIVision = 1\n    OpenAI = 2\n    OpenAIInstruct = 3\n    ChatGLM = 4\n    Groq = 5\n    LLaMA = 6\n    XMChat = 7\n    StableLM = 8\n    MOSS = 9\n    YuanAI = 10\n    Minimax = 11\n    ChuanhuAgent = 12\n    GooglePaLM = 13\n    GoogleGemini = 14\n    LangchainChat = 15\n    Midjourney = 16\n    Spark = 17\n    Claude = 18\n    Qwen = 19\n    ERNIE = 20\n    DALLE3 = 21\n    Ollama = 22\n    GoogleGemma = 23\n    Unknown = 24\n\nMODEL_METADATA = {\n    \"GPT-3\": {\"model_type\": \"OpenAI\", \"multimodal\": False},\n    \"ChatGLM-6B\": {\"model_type\": \"ChatGLM\", \"multimodal\": False},\n    \"Claude-v1\": {\"model_type\": \"Claude\", \"multimodal\": False},\n    \"StableLM-alpha\": {\"model_type\": \"StableLM\", \"multimodal\": False},\n    \"Midjourney-v4\": {\"model_type\": \"Midjourney\", \"multimodal\": True},\n}\n\n",
  "test_results": {
    "ans1": {
      "Type": "OpenAI",
      "Value": 2
    },
    "ans2": {
      "Type": "ChatGLM",
      "Value": 4
    },
    "ans3": {
      "Type": "Claude",
      "Value": 18
    },
    "ans4": {
      "Type": "StableLM",
      "Value": 8
    },
    "ans5": {
      "Type": "Midjourney",
      "Value": 16
    }
  }
}