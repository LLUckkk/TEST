{
  "input_header": "def initialize_weights(net):",
  "input_docstring": null,
  "output_code": "def initialize_weights(net):\n    for m in net.modules():\n        if isinstance(m, nn.Conv2d):\n            m.weight.data.normal_(0, 0.02)\n            m.bias.data.zero_()\n        elif isinstance(m, nn.ConvTranspose2d):\n            m.weight.data.normal_(0, 0.02)\n            m.bias.data.zero_()\n        elif isinstance(m, nn.Linear):\n            m.weight.data.normal_(0, 0.02)\n            m.bias.data.zero_()",
  "input_contexts": [
    {
      "id": "eriklindernoren_PyTorch-GAN_61_2",
      "input_code": "    def __init__(self, latent_dim, n_c, verbose=False):\n        super(Encoder_CNN, self).__init__()\n\n        self.name = 'encoder'\n        self.channels = 1\n        self.latent_dim = latent_dim\n        self.n_c = n_c\n        self.cshape = (128, 5, 5)\n        self.iels = int(np.prod(self.cshape))\n        self.lshape = (self.iels,)\n        self.verbose = verbose\n        \n        self.model = nn.Sequential(\n            nn.Conv2d(self.channels, 64, 4, stride=2, bias=True),\n            nn.LeakyReLU(0.2, inplace=True),\n            nn.Conv2d(64, 128, 4, stride=2, bias=True),\n            nn.LeakyReLU(0.2, inplace=True),\n            \n            Reshape(self.lshape),\n            \n            torch.nn.Linear(self.iels, 1024),\n            nn.LeakyReLU(0.2, inplace=True),\n            torch.nn.Linear(1024, latent_dim + n_c)\n        )\n\n        initialize_weights(self)\n        \n        if self.verbose:\n            print(\"Setting up {}...\\n\".format(self.name))\n            print(self.model)\n"
    },
    {
      "id": "eriklindernoren_PyTorch-GAN_61_3",
      "input_code": "    def __init__(self, wass_metric=False, verbose=False):\n        super(Discriminator_CNN, self).__init__()\n        \n        self.name = 'discriminator'\n        self.channels = 1\n        self.cshape = (128, 5, 5)\n        self.iels = int(np.prod(self.cshape))\n        self.lshape = (self.iels,)\n        self.wass = wass_metric\n        self.verbose = verbose\n        \n        self.model = nn.Sequential(\n            nn.Conv2d(self.channels, 64, 4, stride=2, bias=True),\n            nn.LeakyReLU(0.2, inplace=True),\n            nn.Conv2d(64, 128, 4, stride=2, bias=True),\n            nn.LeakyReLU(0.2, inplace=True),\n            \n            Reshape(self.lshape),\n            \n            torch.nn.Linear(self.iels, 1024),\n            nn.LeakyReLU(0.2, inplace=True),\n            torch.nn.Linear(1024, 1),\n        )\n        \n        if (not self.wass):\n            self.model = nn.Sequential(self.model, torch.nn.Sigmoid())\n\n        initialize_weights(self)\n\n        if self.verbose:\n            print(\"Setting up {}...\\n\".format(self.name))\n            print(self.model)\n"
    },
    {
      "id": "eriklindernoren_PyTorch-GAN_61_1",
      "input_code": "    def __init__(self, latent_dim, n_c, x_shape, verbose=False):\n        super(Generator_CNN, self).__init__()\n\n        self.name = 'generator'\n        self.latent_dim = latent_dim\n        self.n_c = n_c\n        self.x_shape = x_shape\n        self.ishape = (128, 7, 7)\n        self.iels = int(np.prod(self.ishape))\n        self.verbose = verbose\n        \n        self.model = nn.Sequential(\n            torch.nn.Linear(self.latent_dim + self.n_c, 1024),\n            nn.BatchNorm1d(1024),\n            nn.LeakyReLU(0.2, inplace=True),\n            torch.nn.Linear(1024, self.iels),\n            nn.BatchNorm1d(self.iels),\n            nn.LeakyReLU(0.2, inplace=True),\n        \n            Reshape(self.ishape),\n\n            nn.ConvTranspose2d(128, 64, 4, stride=2, padding=1, bias=True),\n            nn.BatchNorm2d(64),\n            nn.LeakyReLU(0.2, inplace=True),\n            \n            nn.ConvTranspose2d(64, 1, 4, stride=2, padding=1, bias=True),\n            nn.Sigmoid()\n        )\n\n        initialize_weights(self)\n\n        if self.verbose:\n            print(\"Setting up {}...\\n\".format(self.name))\n            print(self.model)\n"
    }
  ],
  "generated_inputs": [
    "<needimport>\nimport torch\nimport torch.nn as nn\nimport numpy as np\n\n<testcase_1>\nnet = nn.Sequential(\n    nn.Conv2d(1, 64, 4, stride=2, bias=True),\n    nn.LeakyReLU(0.2, inplace=True),\n    nn.Conv2d(64, 128, 4, stride=2, bias=True),\n    nn.LeakyReLU(0.2, inplace=True),\n    nn.ConvTranspose2d(128, 64, 4, stride=2, padding=1, bias=True),\n    nn.BatchNorm2d(64),\n    nn.LeakyReLU(0.2, inplace=True),\n    nn.ConvTranspose2d(64, 1, 4, stride=2, padding=1, bias=True),\n    nn.Sigmoid()\n)\n\n<testcase_2>\nnet = nn.Sequential(\n    nn.Conv2d(3, 32, 3, stride=1, padding=1, bias=True),\n    nn.ReLU(inplace=True),\n    nn.Conv2d(32, 64, 3, stride=1, padding=1, bias=True),\n    nn.ReLU(inplace=True),\n    nn.Conv2d(64, 128, 3, stride=1, padding=1, bias=True),\n    nn.ReLU(inplace=True),\n    nn.Linear(128 * 32 * 32, 1000),\n    nn.ReLU(inplace=True),\n    nn.Linear(1000, 10)\n)\n\n<testcase_3>\nnet = nn.Sequential(\n    nn.Linear(100, 256),\n    nn.ReLU(inplace=True),\n    nn.Linear(256, 512),\n    nn.ReLU(inplace=True),\n    nn.Linear(512, 1024),\n    nn.ReLU(inplace=True),\n    nn.Linear(1024, 2048),\n    nn.ReLU(inplace=True),\n    nn.Linear(2048, 4096),\n    nn.ReLU(inplace=True),\n    nn.Linear(4096, 8192),\n    nn.ReLU(inplace=True)\n)\n\n<testcase_4>\nnet = nn.Sequential(\n    nn.Conv2d(1, 16, 5, stride=1, padding=2, bias=True),\n    nn.ReLU(inplace=True),\n    nn.Conv2d(16, 32, 5, stride=1, padding=2, bias=True),\n    nn.ReLU(inplace=True),\n    nn.Conv2d(32, 64, 5, stride=1, padding=2, bias=True),\n    nn.ReLU(inplace=True),\n    nn.Linear(64 * 28 * 28, 100),\n    nn.ReLU(inplace=True),\n    nn.Linear(100, 10)\n)\n\n<testcase_5>\nnet = nn.Sequential(\n    nn.ConvTranspose2d(1, 32, 4, stride=2, padding=1, bias=True),\n    nn.BatchNorm2d(32),\n    nn.ReLU(inplace=True),\n    nn.ConvTranspose2d(32, 64, 4, stride=2, padding=1, bias=True),\n    nn.BatchNorm2d(64),\n    nn.ReLU(inplace=True),\n    nn.ConvTranspose2d(64, 128, 4, stride=2, padding=1, bias=True),\n    nn.BatchNorm2d(128),\n    nn.ReLU(inplace=True),\n    nn.ConvTranspose2d(128, 256, 4, stride=2, padding=1, bias=True),\n    nn.BatchNorm2d(256),\n    nn.ReLU(inplace=True),\n    nn.ConvTranspose2d(256, 512, 4, stride=2, padding=1, bias=True),\n    nn.BatchNorm2d(512),\n    nn.ReLU(inplace=True)\n)"
  ],
  "gpt_output": "<needimport>\nimport torch\nimport torch.nn as nn\nimport numpy as np\n\n<testcase_1>\nnet = nn.Sequential(\n    nn.Conv2d(1, 64, 4, stride=2, bias=True),\n    nn.LeakyReLU(0.2, inplace=True),\n    nn.Conv2d(64, 128, 4, stride=2, bias=True),\n    nn.LeakyReLU(0.2, inplace=True),\n    nn.ConvTranspose2d(128, 64, 4, stride=2, padding=1, bias=True),\n    nn.BatchNorm2d(64),\n    nn.LeakyReLU(0.2, inplace=True),\n    nn.ConvTranspose2d(64, 1, 4, stride=2, padding=1, bias=True),\n    nn.Sigmoid()\n)\n\n<testcase_2>\nnet = nn.Sequential(\n    nn.Conv2d(3, 32, 3, stride=1, padding=1, bias=True),\n    nn.ReLU(inplace=True),\n    nn.Conv2d(32, 64, 3, stride=1, padding=1, bias=True),\n    nn.ReLU(inplace=True),\n    nn.Conv2d(64, 128, 3, stride=1, padding=1, bias=True),\n    nn.ReLU(inplace=True),\n    nn.Linear(128 * 32 * 32, 1000),\n    nn.ReLU(inplace=True),\n    nn.Linear(1000, 10)\n)\n\n<testcase_3>\nnet = nn.Sequential(\n    nn.Linear(100, 256),\n    nn.ReLU(inplace=True),\n    nn.Linear(256, 512),\n    nn.ReLU(inplace=True),\n    nn.Linear(512, 1024),\n    nn.ReLU(inplace=True),\n    nn.Linear(1024, 2048),\n    nn.ReLU(inplace=True),\n    nn.Linear(2048, 4096),\n    nn.ReLU(inplace=True),\n    nn.Linear(4096, 8192),\n    nn.ReLU(inplace=True)\n)\n\n<testcase_4>\nnet = nn.Sequential(\n    nn.Conv2d(1, 16, 5, stride=1, padding=2, bias=True),\n    nn.ReLU(inplace=True),\n    nn.Conv2d(16, 32, 5, stride=1, padding=2, bias=True),\n    nn.ReLU(inplace=True),\n    nn.Conv2d(32, 64, 5, stride=1, padding=2, bias=True),\n    nn.ReLU(inplace=True),\n    nn.Linear(64 * 28 * 28, 100),\n    nn.ReLU(inplace=True),\n    nn.Linear(100, 10)\n)\n\n<testcase_5>\nnet = nn.Sequential(\n    nn.ConvTranspose2d(1, 32, 4, stride=2, padding=1, bias=True),\n    nn.BatchNorm2d(32),\n    nn.ReLU(inplace=True),\n    nn.ConvTranspose2d(32, 64, 4, stride=2, padding=1, bias=True),\n    nn.BatchNorm2d(64),\n    nn.ReLU(inplace=True),\n    nn.ConvTranspose2d(64, 128, 4, stride=2, padding=1, bias=True),\n    nn.BatchNorm2d(128),\n    nn.ReLU(inplace=True),\n    nn.ConvTranspose2d(128, 256, 4, stride=2, padding=1, bias=True),\n    nn.BatchNorm2d(256),\n    nn.ReLU(inplace=True),\n    nn.ConvTranspose2d(256, 512, 4, stride=2, padding=1, bias=True),\n    nn.BatchNorm2d(512),\n    nn.ReLU(inplace=True)\n)",
  "needimport": "import torch\nimport torch.nn as nn\nimport numpy as np\n\n",
  "test_results": {
    "ans1": "Sequential(\n  (0): Conv2d(1, 64, kernel_size=(4, 4), stride=(2, 2))\n  (1): LeakyReLU(negative_slope=0.2, inplace=True)\n  (2): Conv2d(64, 128, kernel_size=(4, 4), stride=(2, 2))\n  (3): LeakyReLU(negative_slope=0.2, inplace=True)\n  (4): ConvTranspose2d(128, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n  (5): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n  (6): LeakyReLU(negative_slope=0.2, inplace=True)\n  (7): ConvTranspose2d(64, 1, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n  (8): Sigmoid()\n)",
    "ans2": "Sequential(\n  (0): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n  (1): ReLU(inplace=True)\n  (2): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n  (3): ReLU(inplace=True)\n  (4): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n  (5): ReLU(inplace=True)\n  (6): Linear(in_features=131072, out_features=1000, bias=True)\n  (7): ReLU(inplace=True)\n  (8): Linear(in_features=1000, out_features=10, bias=True)\n)",
    "ans3": "Sequential(\n  (0): Linear(in_features=100, out_features=256, bias=True)\n  (1): ReLU(inplace=True)\n  (2): Linear(in_features=256, out_features=512, bias=True)\n  (3): ReLU(inplace=True)\n  (4): Linear(in_features=512, out_features=1024, bias=True)\n  (5): ReLU(inplace=True)\n  (6): Linear(in_features=1024, out_features=2048, bias=True)\n  (7): ReLU(inplace=True)\n  (8): Linear(in_features=2048, out_features=4096, bias=True)\n  (9): ReLU(inplace=True)\n  (10): Linear(in_features=4096, out_features=8192, bias=True)\n  (11): ReLU(inplace=True)\n)",
    "ans4": "Sequential(\n  (0): Conv2d(1, 16, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n  (1): ReLU(inplace=True)\n  (2): Conv2d(16, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n  (3): ReLU(inplace=True)\n  (4): Conv2d(32, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n  (5): ReLU(inplace=True)\n  (6): Linear(in_features=50176, out_features=100, bias=True)\n  (7): ReLU(inplace=True)\n  (8): Linear(in_features=100, out_features=10, bias=True)\n)",
    "ans5": "Sequential(\n  (0): ConvTranspose2d(1, 32, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n  (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n  (2): ReLU(inplace=True)\n  (3): ConvTranspose2d(32, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n  (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n  (5): ReLU(inplace=True)\n  (6): ConvTranspose2d(64, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n  (7): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n  (8): ReLU(inplace=True)\n  (9): ConvTranspose2d(128, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n  (10): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n  (11): ReLU(inplace=True)\n  (12): ConvTranspose2d(256, 512, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n  (13): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n  (14): ReLU(inplace=True)\n)"
  }
}