{
  "input_header": "def run_in_subprocess(cmd):",
  "input_docstring": null,
  "output_code": "def run_in_subprocess(cmd):\n    try:\n        with subprocess.Popen(\n            cmd, shell=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE\n        ) as return_info:\n            while True:\n                next_line = return_info.stdout.readline()\n                return_line = next_line.decode(\"utf-8\", \"ignore\").strip()\n                if return_line == \"\" and return_info.poll() != None:\n                    break\n                if return_line != \"\":\n                    logging.info(return_line)\n\n            err_lines = \"\"\n            while True:\n                next_line = return_info.stderr.readline()\n                return_line = next_line.decode(\"utf-8\", \"ignore\").strip()\n                if return_line == \"\" and return_info.poll() != None:\n                    break\n                if return_line != \"\":\n                    logging.info(return_line)\n                    err_lines += return_line + \"\\n\"\n\n            return_code = return_info.wait()\n            if return_code:\n                raise RuntimeError(err_lines)\n    except Exception as e:\n        raise e",
  "input_contexts": [
    {
      "id": "QwenLM_Qwen_179_1",
      "input_code": "def test_finetune(num_gpus, train_type, is_chat, docker_version, deepspeed):\n    cmd_docker = f\"docker run --gpus all --ipc=host --network=host --rm -v {os.getcwd()}/../../../Qwen:{DOCKER_MOUNT_DIR} {docker_version} /bin/bash -c \"\n    cmd = \"\"\n    is_ampere = torch.cuda.get_device_capability()[0] >= 8\n    if not is_ampere:\n        cmd = f\"pip uninstall -y flash-attn && \"\n\n    model_type = f\"{MODEL_TYPE}-Chat\" if is_chat == \"chat\" else MODEL_TYPE\n    model_type = f\"{model_type}-Int4\" if train_type == \"qlora\" else model_type\n    cmd += f\"\"\"torchrun --nproc_per_node {num_gpus} --nnodes 1 --node_rank 0 --master_addr localhost --master_port 12345 {DOCKER_MOUNT_DIR}/finetune.py \\\n    --model_name_or_path \"{DOCKER_TEST_DIR}/{model_type}/\" \\\n    --data_path  {DATA_DIR} \\\n    --output_dir \"{DOCKER_TEST_DIR}/output_qwen\" \\\n    --num_train_epochs 1 \\\n    --per_device_train_batch_size 1 \\\n    --per_device_eval_batch_size 1 \\\n    --gradient_accumulation_steps 2 \\\n    --evaluation_strategy \"no\" \\\n    --save_strategy \"steps\" \\\n    --save_steps 1000 \\\n    --save_total_limit 10 \\\n    --learning_rate 1e-5 \\\n    --weight_decay 0.1 \\\n    --adam_beta2 0.95 \\\n    --warmup_ratio 0.01 \\\n    --lr_scheduler_type \"cosine\" \\\n    --logging_steps 1 \\\n    --report_to \"none\" \\\n    --model_max_length 512\"\"\"\n    if deepspeed:\n        cmd += f\" --deepspeed {deepspeed}\"\n    if train_type == \"lora\":\n        cmd += \" --use_lora\"\n    elif train_type == \"qlora\":\n        cmd += \" --use_lora --q_lora\"\n    if (\n        (not is_ampere)\n        and train_type == \"lora\"\n        and (deepspeed and \"zero2\" in deepspeed)\n        and is_chat == \"base\"\n    ):\n        cmd += \" --fp16 True\"\n    snapshot_download(model_type, cache_dir=\".\", revision=\"master\")\n    run_in_subprocess(cmd_docker + f'\"{cmd}\"')\n    if train_type == \"full\":\n        assert os.path.exists(\"output_qwen/config.json\")\n    else:\n        assert os.path.exists(\"output_qwen/adapter_config.json\")\n    shutil.rmtree(\"output_qwen\")\n"
    },
    {
      "id": "QwenLM_Qwen_179_2",
      "input_code": "def test_inference_api(docker_version, use_cpu, use_int4):\n    container_name = \"test_inference_api\"\n    model_type = f\"{MODEL_TYPE}-Chat-Int4\" if use_int4 else f\"{MODEL_TYPE}-Chat\"\n    cmd_docker = f'docker run --gpus all --ipc=host --network=host --rm --name=\"{container_name}\" -p 8000:8000 -v {os.getcwd()}/../../../Qwen:{DOCKER_MOUNT_DIR} {docker_version} /bin/bash -c '\n    cmd = \"\"\n    is_ampere = torch.cuda.get_device_capability()[0] >= 8\n    if not is_ampere:\n        cmd += f\"pip uninstall -y flash-attn && \"\n\n    cmd += f\"\"\"python {DOCKER_MOUNT_DIR}/openai_api.py -c {DOCKER_TEST_DIR}/{model_type}\"\"\"\n\n    if use_cpu:\n        cmd += \" --cpu-only\"\n\n    snapshot_download(model_type, cache_dir=\".\", revision=\"master\")\n    print(cmd_docker + f'\"{cmd}\"')\n    run_in_subprocess(\n        f'docker rm -f {container_name} 2>/dev/null || echo \"The container does not exist.\"'\n    )\n    run_in_subprocess(\"nohup \" + cmd_docker + f'\"{cmd}\"' + \" > tmp.log 2>&1 &\")\n\n    while not TelnetPort(\"localhost\", 8000):\n        print(\"Wait for the model service start.\")\n        time.sleep(0.5)\n\n        if (\n            subprocess.run(\n                f\"docker inspect {container_name}\",\n                shell=True,\n                stdout=subprocess.DEVNULL,\n            ).returncode\n            != 0\n        ):\n            break\n    try:\n        simple_openai_api(f\"{MODEL_TYPE}-Chat\".split(\"/\")[-1])\n    except Exception as e:\n        time.sleep(1)\n        with open(\"tmp.log\") as f:\n            raise Exception(f\"{e} \\n {f.read()}\")\n\n    run_in_subprocess(f\"docker rm -f {container_name}\")\n"
    },
    {
      "id": "QwenLM_Qwen_179_3",
      "input_code": "def test_inference_vllm_fschat(num_gpus, use_int4):\n    model_type = f\"{MODEL_TYPE}-Chat-Int4\" if use_int4 else f\"{MODEL_TYPE}-Chat\"\n    container_name = \"test_inference_vllm_fschat\"\n    cmd_docker = f'docker run --gpus all --ipc=host --network=host --rm --name=\"{container_name}\" -p 8000:8000 -v {os.getcwd()}/../../../Qwen:{DOCKER_MOUNT_DIR} {DOCKER_VERSION_CU121} /bin/bash -c '\n    cmd = \"\"\n\n    cmd += f\"\"\"nohup python -m fastchat.serve.controller > /dev/null 2>&1 \\\n    & python -m fastchat.serve.openai_api_server --host localhost --port 8000 > /dev/null 2>&1 \\\n    & python -m fastchat.serve.vllm_worker --model-path {DOCKER_TEST_DIR}/{model_type} --tensor-parallel-size {num_gpus} --trust-remote-code\"\"\"\n\n    is_ampere = torch.cuda.get_device_capability()[0] >= 8\n    if not is_ampere or use_int4:\n        cmd += \" --dtype half\"\n\n    snapshot_download(model_type, cache_dir=\".\", revision=\"master\")\n    run_in_subprocess(\n        f'docker rm -f {container_name} 2>/dev/null || echo \"The container does not exist.\"'\n    )\n    print(cmd_docker + f'\"{cmd}\"')\n    run_in_subprocess(\"nohup \" + cmd_docker + f'\"{cmd}\"' + \" > tmp.log 2>&1 &\")\n\n    while not TelnetPort(\"localhost\", 21002):\n        print(\"Wait for the model service start.\")\n        time.sleep(0.5)\n\n        if (\n            subprocess.run(\n                f\"docker inspect {container_name}\",\n                shell=True,\n                stdout=subprocess.DEVNULL,\n            ).returncode\n            != 0\n        ):\n            break\n\n    try:\n        simple_openai_api(model_type.split(\"/\")[-1])\n    except Exception as e:\n        time.sleep(1)\n        with open(\"tmp.log\") as f:\n            raise Exception(f\"{e} \\n {f.read()}\")\n\n    run_in_subprocess(f\"docker rm -f {container_name}\")\n"
    }
  ],
  "generated_inputs": [
    "<needimport>\nimport subprocess\nimport logging\n\n<testcase_1>\ncmd = \"echo 'Hello, World!'\"\n\n<testcase_2>\ncmd = \"ls -la /\"\n\n<testcase_3>\ncmd = \"python -c 'print(\\\"Test subprocess execution\\\")'\"\n\n<testcase_4>\ncmd = \"uname -a\"\n\n<testcase_5>\ncmd = \"docker --version\""
  ],
  "gpt_output": "<needimport>\nimport subprocess\nimport logging\n\n<testcase_1>\ncmd = \"echo 'Hello, World!'\"\n\n<testcase_2>\ncmd = \"ls -la /\"\n\n<testcase_3>\ncmd = \"python -c 'print(\\\"Test subprocess execution\\\")'\"\n\n<testcase_4>\ncmd = \"uname -a\"\n\n<testcase_5>\ncmd = \"docker --version\"",
  "needimport": "import subprocess\nimport logging\n\n",
  "test_results": {
    "ans1": "'Hello, World!'",
    "ans2": "'ls' 不是内部或外部命令，也不是可运行的程序\n或批处理文件。\n",
    "ans3": "",
    "ans4": "'uname' 不是内部或外部命令，也不是可运行的程序\n或批处理文件。\n",
    "ans5": "'docker' 不是内部或外部命令，也不是可运行的程序\n或批处理文件。\n"
  }
}