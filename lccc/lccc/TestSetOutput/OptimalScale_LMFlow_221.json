{
  "input_header": "def get_model(self, model_args, *args, **kwargs):",
  "input_docstring": null,
  "output_code": "    def get_model(self, model_args, *args, **kwargs):\n        arch_type = model_args.arch_type\n        if arch_type == \"decoder_only\":\n            return HFDecoderModel(model_args, *args, **kwargs)\n        elif arch_type == \"text_regression\":\n            return HFTextRegressionModel(model_args, *args, **kwargs)\n        elif arch_type == \"encoder_decoder\" or \\\n                arch_type == \"vision_encoder_decoder\":\n            return HFEncoderDecoderModel(model_args, *args, **kwargs)\n        else:\n            raise NotImplementedError(\n                f\"model architecture type \\\"{arch_type}\\\" is not supported\"\n            )",
  "input_contexts": [
    {
      "id": "OptimalScale_LMFlow_221_15",
      "input_code": "def main():\n    pipeline_name = \"vllm_inferencer\"\n    PipelineArguments = AutoArguments.get_pipeline_args_class(pipeline_name)\n\n    parser = HfArgumentParser((\n        ModelArguments, \n        DatasetArguments,\n        PipelineArguments\n    ))\n    if len(sys.argv) == 2 and sys.argv[1].endswith(\".json\"):\n        model_args, data_args, pipeline_args = parser.parse_json_file(json_file=os.path.abspath(sys.argv[1]))\n    else:\n        model_args, data_args, pipeline_args = parser.parse_args_into_dataclasses()\n\n    dataset = Dataset(data_args)\n    model = AutoModel.get_model(model_args, tune_strategy='none')\n    inferencer = VLLMInferencer(model_args, data_args, pipeline_args)\n\n    res = inferencer.inference(\n        model,\n        dataset,\n        release_gpu=False,\n        enable_decode_inference_result=pipeline_args.enable_decode_inference_result,\n        enable_distributed_inference=pipeline_args.enable_distributed_inference,\n        distributed_inference_num_instances=pipeline_args.distributed_inference_num_instances,\n        inference_batch_size=pipeline_args.vllm_inference_batch_size,\n    )\n    \n    print(MEMORY_SAFE_VLLM_INFERENCE_FINISH_FLAG)\n"
    },
    {
      "id": "OptimalScale_LMFlow_221_4",
      "input_code": "def main():\n    pipeline_name = \"finetuner\"\n    PipelineArguments = AutoArguments.get_pipeline_args_class(pipeline_name)\n\n    parser = HfArgumentParser((ModelArguments, DatasetArguments, PipelineArguments))\n    if len(sys.argv) == 2 and sys.argv[1].endswith(\".json\"):\n        model_args, data_args, pipeline_args = parser.parse_json_file(json_file=os.path.abspath(sys.argv[1]))\n    else:\n        model_args, data_args, pipeline_args = parser.parse_args_into_dataclasses()\n\n    finetuner = AutoPipeline.get_pipeline(\n        pipeline_name=pipeline_name,\n        model_args=model_args,\n        data_args=data_args,\n        pipeline_args=pipeline_args,\n    )\n    dataset = Dataset(data_args)\n    model = AutoModel.get_model(model_args)\n\n    tuned_model = finetuner.tune(model=model, dataset=dataset)\n"
    },
    {
      "id": "OptimalScale_LMFlow_221_16",
      "input_code": "    def test_get_decoder_model(self):\n        model_args = ModelArguments(\n            arch_type=\"decoder_only\", model_name_or_path=MODEL_NAME)\n        model = AutoModel.get_model(model_args)\n        self.assertTrue(isinstance(model, HFDecoderModel))\n"
    },
    {
      "id": "OptimalScale_LMFlow_221_3",
      "input_code": "def main():\n    pipeline_name = \"dpov2_aligner\"\n    PipelineArguments = AutoArguments.get_pipeline_args_class(pipeline_name)\n\n    parser = HfArgumentParser((\n        ModelArguments, \n        ReferenceModelArguments,\n        DatasetArguments,\n        PipelineArguments\n    ))\n    if len(sys.argv) == 2 and sys.argv[1].endswith(\".json\"):\n        model_args, ref_model_args, data_args, pipeline_args = parser.parse_json_file(json_file=os.path.abspath(sys.argv[1]))\n    else:\n        model_args, ref_model_args, data_args, pipeline_args = parser.parse_args_into_dataclasses()\n        \n    ref_model_args_dict = remove_dataclass_attr_prefix(ref_model_args, \"reference_\")\n    ref_model_args = ModelArguments(**ref_model_args_dict)\n\n    train_dataset = Dataset(data_args)\n    eval_data_args = copy.deepcopy(data_args)\n    eval_data_args.dataset_path = pipeline_args.eval_dataset_path\n    eval_dataset = Dataset(eval_data_args)\n    model = AutoModel.get_model(model_args)\n    ref_model = AutoModel.get_model(ref_model_args)\n    aligner = AutoPipeline.get_pipeline(\n        pipeline_name=pipeline_name,\n        model_args=model_args,\n        data_args=data_args,\n        pipeline_args=pipeline_args,\n        ref_model_args=ref_model_args,\n    )\n\n    res = aligner.align(\n        model=model,\n        ref_model=ref_model,\n        train_dataset=train_dataset,\n        eval_dataset=eval_dataset,\n    )\n"
    },
    {
      "id": "OptimalScale_LMFlow_221_12",
      "input_code": "def main():\n    pipeline_name = \"inferencer\"\n    PipelineArguments = AutoArguments.get_pipeline_args_class(pipeline_name)\n\n    parser = HfArgumentParser((\n        VisModelArguments,\n        PipelineArguments,\n        ChatbotArguments,\n    ))\n\n    model_args, pipeline_args, chatbot_args = (\n        parser.parse_args_into_dataclasses()\n    )\n    inferencer_args = pipeline_args\n    with open (pipeline_args.deepspeed, \"r\") as f:\n        ds_config = json.load(f)\n    model = AutoModel.get_model(\n        model_args,\n        tune_strategy='none',\n        ds_config=ds_config,\n        device=pipeline_args.device,\n        custom_model=model_args.custom_model,\n        with_deepspeed=chatbot_args.with_deepspeed,\n    )\n\n    data_args = DatasetArguments(dataset_path=None)\n    dataset = Dataset(data_args, backend=\"dict\")\n\n    inferencer = AutoPipeline.get_pipeline(\n        pipeline_name=pipeline_name,\n        model_args=model_args,\n        data_args=data_args,\n        pipeline_args=pipeline_args,\n    )\n\n    model_name = model_args.model_name_or_path\n    if model_args.llm_model_name_or_path is not None:\n        model_name = model_name + \" with {}\".format(\n            model_args.llm_model_name_or_path\n        )\n    if model_args.lora_model_path is not None:\n        model_name += f\" + {model_args.lora_model_path}\"\n\n    guide_message = (\n        \"\\n\"\n        f\"#############################################################################\\n\"\n        f\"##   A {model_name} chatbot is now chatting with you!\\n\"\n        f\"##   The command for loading a new image: ###Load image:\\n\"\n        f\"#############################################################################\\n\"\n        \"\\n\"\n    )\n    print(guide_message)\n\n\n\n    end_string = chatbot_args.end_string\n    if chatbot_args.chatbot_type == \"mini_gpt\":\n        context = \"Give the following image: <Img>ImageContent</Img>. \" + \"You will be able to see the image once I provide it to you. Please answer my questions.\"\n        user_name = \"Human\"\n        sep = \"###\"\n\n    elif chatbot_args.chatbot_type == \"llava\":\n        context = \"A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions.\"\n        user_name = \"USER\"\n        sep = \" \"\n    else:\n        context = \"\"\n        user_name = \"\"\n        sep = \"###\"\n    prompt_structure = chatbot_args.prompt_structure\n\n    image_list = []\n    if chatbot_args.image_path is not None:\n        raw_image = Image.open(chatbot_args.image_path)\n    else:\n        img_url = 'https://storage.googleapis.com/sfr-vision-language-research/BLIP/demo.jpg'\n        raw_image = Image.open(requests.get(img_url, stream=True).raw).convert('RGB')\n    base_size = raw_image.size\n    image_list.append(np.array(raw_image))\n    input_text = chatbot_args.input_text\n    if chatbot_args.task == \"image_caption\" and len(input_text) == 0:\n        input_text = \"a photography of\"\n    if chatbot_args.chatbot_type == \"mini_gpt\":\n        context += sep + user_name + \": \" + \"<Img><ImageHere></Img> \"\n    elif chatbot_args.chatbot_type == \"llava\":\n        context += sep + user_name + \": \" + \"<image>\\n\"\n\n    text_after_loading_image = True\n\n    if model_args.use_prompt_cache:\n        if osp.exists(model_args.prompt_cache_path):\n            model.backend_model.load_prompt_cache(\n                model_args.prompt_cache_path)\n\n    if chatbot_args.task == \"image_caption\":\n        input_dataset = dataset.from_dict({\n            \"type\": \"image_text\",\n            \"instances\": [{\"images\": np.stack(image_list),\n                           \"text\":  input_text,}]\n        })\n        output = inferencer.inference(model, input_dataset)\n        print(output.backend_dataset['text'])\n    else:\n        while True:\n            input_text = input(\"User >>> \")\n            if input_text == \"exit\":\n                print(\"exit...\")\n                break\n            elif input_text.startswith(\"###Load image:\"):\n                image_path = input_text[14:]\n                try:\n                    raw_image = Image.open(image_path)\n                    raw_image = raw_image.resize(base_size)\n                    image_list.append(np.array(raw_image))\n                    if chatbot_args.chatbot_type == \"mini_gpt\":\n                        context += sep + user_name + \": \" + \"<Img><ImageHere></Img> \"\n                    elif chatbot_args.chatbot_type == \"llava\":\n                        context += sep + user_name + \": \" + \"<image>\\n\"\n                    else:\n                        raise NotImplementedError\n                    text_after_loading_image = True\n                    print(\"Finish loading image with path {}\".format(image_path))\n                    continue\n                except FileNotFoundError:\n                    print(\"Load image failed with path {}\".format(image_path))\n                    continue\n            elif input_text == \"reset\":\n                context = \"\"\n                print(\"Chat history cleared\")\n                continue\n            \n            if text_after_loading_image is False:\n                context += sep + user_name + \": \"\n            else:\n                text_after_loading_image = False\n            \n            if not input_text:\n                input_text = \" \"\n            context += prompt_structure.format(input_text=input_text)\n\n            context = context[-model.get_max_length():]\n            input_dataset = dataset.from_dict({\n                \"type\": \"image_text\",\n                \"instances\": [{\"images\": np.stack(image_list),\n                               \"text\":  context,}]\n            })\n            if chatbot_args.chatbot_type in [\"mini_gpt\", \"llava\"]:\n                remove_image_flag = True\n            else:\n                remove_image_flag = False\n            begin_time = time.time()\n            if not chatbot_args.stream_inference:\n                output_dataset = inferencer.inference(\n                    model,\n                    input_dataset,\n                    remove_image_flag=remove_image_flag,\n                    chatbot_type=chatbot_args.chatbot_type,)\n                response = output_dataset.backend_dataset['text']\n                print(response[0])\n                print(\"\\n\", end=\"\")\n                context += response[0]\n            else:\n                print(\"Bot: \", end=\"\")\n                print_index = 0\n\n                token_per_step = 4\n\n                for response, flag_break in inferencer.stream_inference(\n                    context=context,\n                    model=model,\n                    max_new_tokens=inferencer_args.max_new_tokens,\n                    token_per_step=token_per_step,\n                    temperature=inferencer_args.temperature,\n                    end_string=end_string,\n                    input_dataset=input_dataset\n                ):\n                    new_print_index = print_index\n                    for char in response[print_index:]:\n                        if end_string is not None and char == end_string[0]:\n                            if new_print_index + len(end_string) >= len(response):\n                                break\n\n                        new_print_index += 1\n                        print(char, end=\"\", flush=True)\n\n                    print_index = new_print_index\n\n                    if flag_break:\n                        break\n                print(\"\\n\", end=\"\")\n\n                context += response + \"\\n\"\n            \n            if model_args.use_prompt_cache:\n                osp.makedirs(osp.dirname(model_args.prompt_cache_path),\n                             exist_ok=True)\n                model.backend_model.save_prompt_cache(\n                            model_args.prompt_cache_path)\n            end_time = time.time()\n            print(\"Whole data and model forward time\", end_time - begin_time)\n"
    },
    {
      "id": "OptimalScale_LMFlow_221_17",
      "input_code": "    def test_get_unsupported_model(self):\n        model_args = ModelArguments(\n            arch_type=\"unsupported model\", model_name_or_path=MODEL_NAME)\n        with self.assertRaises(NotImplementedError):\n            model = AutoModel.get_model(model_args)\n"
    },
    {
      "id": "OptimalScale_LMFlow_221_2",
      "input_code": "def main():\n    pipeline_name = \"inferencer\"\n    PipelineArguments = AutoArguments.get_pipeline_args_class(pipeline_name)\n\n    parser = HfArgumentParser((\n        ModelArguments,\n        PipelineArguments,\n        ChatbotArguments,\n    ))\n    model_args, pipeline_args, chatbot_args = (\n        parser.parse_args_into_dataclasses()\n    )\n    inferencer_args = pipeline_args\n\n    with open (pipeline_args.deepspeed, \"r\") as f:\n        ds_config = json.load(f)\n\n    model = AutoModel.get_model(\n        model_args,\n        tune_strategy='none',\n        ds_config=ds_config,\n        device=pipeline_args.device,\n        use_accelerator=True,\n    )\n\n    data_args = DatasetArguments(dataset_path=None)\n    dataset = Dataset(data_args)\n\n    inferencer = AutoPipeline.get_pipeline(\n        pipeline_name=pipeline_name,\n        model_args=model_args,\n        data_args=data_args,\n        pipeline_args=pipeline_args,\n    )\n\n    model_name = model_args.model_name_or_path\n    if model_args.lora_model_path is not None:\n        model_name += f\" + {model_args.lora_model_path}\"\n\n    guide_message = (\n        \"\\n\"\n        f\"#############################################################################\\n\"\n        f\"##   A {model_name} chatbot is now chatting with you!\\n\"\n        f\"#############################################################################\\n\"\n        \"\\n\"\n    )\n    print(guide_message)\n\n    context = \"\"\n\n    end_string = chatbot_args.end_string\n    prompt_structure = chatbot_args.prompt_structure\n\n    while True:\n        input_text = input(\"User >>> \")\n        if input_text == \"exit\":\n            print(\"exit...\")\n            break\n        elif input_text == \"reset\":\n            context = \"\"\n            print(\"Chat history cleared\")\n            continue\n        if not input_text:\n            input_text = \" \"\n\n        context += prompt_structure.format(input_text=input_text)\n        context = context[-model.get_max_length():]\n\n        input_dataset = dataset.from_dict({\n            \"type\": \"text_only\",\n            \"instances\": [ { \"text\": context } ]\n        })\n\n        print(\"Bot: \", end=\"\")\n        print_index = 0\n\n        token_per_step = chatbot_args.num_token_per_step\n\n        for response, flag_break in inferencer.stream_inference(\n            context=context,\n            model=model,\n            max_new_tokens=inferencer_args.max_new_tokens,\n            token_per_step=token_per_step,\n            temperature=inferencer_args.temperature,\n            end_string=end_string,\n            input_dataset=input_dataset\n        ):\n            new_print_index = print_index\n            for char in response[print_index:]:\n                if end_string is not None and char == end_string[0]:\n                    if new_print_index + len(end_string) >= len(response):\n                        break\n\n                new_print_index += 1\n                print(char, end=\"\", flush=True)\n\n            print_index = new_print_index\n\n            if flag_break:\n                break\n        print(\"\\n\", end=\"\")\n\n        context += response + \"\\n\"\n"
    },
    {
      "id": "OptimalScale_LMFlow_221_13",
      "input_code": "def start_inferencer(\n    request_queue,\n    response_queue,\n    model_args,\n    pipeline_name,\n    pipeline_args,\n    data_args,\n    dataset,\n    chatbot_args,\n):\n    with open(pipeline_args.deepspeed, \"r\") as f:\n        ds_config = json.load(f)\n\n    model = AutoModel.get_model(\n        model_args,\n        tune_strategy='none',\n        ds_config=ds_config,\n        device=pipeline_args.device,\n        custom_model=model_args.custom_model,\n    )\n\n    inferencer = AutoPipeline.get_pipeline(\n        pipeline_name=pipeline_name,\n        model_args=model_args,\n        data_args=data_args,\n        pipeline_args=pipeline_args,\n    )\n\n    while True:\n        if not request_queue.empty():\n            request_queue.put(\"busy\")\n            request = request_queue.get()\n\n            context = request[0]\n            max_new_tokens = request[1]\n            token_per_step = request[2]\n            temperature = request[3]\n            end_string = request[4]\n            input_dataset = request[5]\n            remove_image_flag = request[6]\n\n            break_in_the_middle = False\n            for response_text, flag_break in inferencer.stream_inference(\n                context=context,\n                model=model,\n                max_new_tokens=max_new_tokens,\n                token_per_step=token_per_step,\n                temperature=temperature,\n                end_string=end_string,\n                input_dataset=input_dataset,\n                remove_image_flag=remove_image_flag,\n            ):\n                response_queue.put((response_text, flag_break))\n                if flag_break:\n                    break_in_the_middle = True\n                    break\n\n            if not break_in_the_middle:\n                response_text = ''\n                flag_break = True\n                response_queue.put((response_text, flag_break))\n\n            mark = \"\"\n            while mark != \"busy\":\n                mark = request_queue.get()\n\n        time.sleep(0.001)\n"
    },
    {
      "id": "OptimalScale_LMFlow_221_9",
      "input_code": "def main():\n    pipeline_name = \"raft_aligner\"\n    PipelineArguments = AutoArguments.get_pipeline_args_class(pipeline_name)\n\n    parser = HfArgumentParser((\n        ModelArguments,\n        DatasetArguments,\n        PipelineArguments,\n        RewardArguments,\n    ))\n    if len(sys.argv) == 2 and sys.argv[1].endswith(\".json\"):\n        model_args, data_args, pipeline_args, reward_args = parser.parse_json_file(\n            json_file=os.path.abspath(sys.argv[1])\n        )\n    else:\n        model_args, data_args, pipeline_args, reward_args = parser.parse_args_into_dataclasses()\n\n    aligner = AutoPipeline.get_pipeline(\n        pipeline_name=pipeline_name,\n        model_args=model_args,\n        data_args=data_args,\n        pipeline_args=pipeline_args,\n    )\n    dataset = Dataset(data_args)\n    model = AutoModel.get_model(model_args)\n\n    reward_function = get_reward_function(reward_args, pipeline_args)\n\n    reward_model_args = ModelArguments(arch_type=\"text_regression\")\n    reward_model = AutoModel.get_model(reward_model_args)\n    reward_model.register_inference_function(reward_function)\n\n    aligned_model = aligner.align(\n        model=model,\n        dataset=dataset,\n        reward_model=reward_model,\n    )\n"
    },
    {
      "id": "OptimalScale_LMFlow_221_7",
      "input_code": "def main():\n    parser = HfArgumentParser((ModelArguments, MergeLoraArguments))\n    if len(sys.argv) == 2 and sys.argv[1].endswith(\".json\"):\n        model_args, merge_lora_args = parser.parse_json_file(json_file=os.path.abspath(sys.argv[1]))\n    else:\n        model_args, merge_lora_args = parser.parse_args_into_dataclasses()\n        \n    if merge_lora_args.device == 'gpu':\n        raise NotImplementedError('Merging LoRA weight using GPU not supported yet. Please use cpu.')\n\n    model_args.use_lora = True\n    model = AutoModel.get_model(\n        model_args, \n        tune_strategy='none', \n        device=merge_lora_args.device,\n        ds_config=merge_lora_args.ds_config\n    )\n    model.activate_model_for_inference()\n    model.merge_lora_weights()\n    model.save(merge_lora_args.output_model_path, save_full_model=True)\n"
    },
    {
      "id": "OptimalScale_LMFlow_221_6",
      "input_code": "def main():\n    pipeline_name = \"inferencer\"\n    PipelineArguments = AutoArguments.get_pipeline_args_class(pipeline_name)\n\n    parser = HfArgumentParser((\n        ModelArguments,\n        PipelineArguments,\n    ))\n    model_args, pipeline_args = parser.parse_args_into_dataclasses()\n    inferencer_args = pipeline_args\n\n    with open (pipeline_args.deepspeed, \"r\") as f:\n        ds_config = json.load(f)\n\n    model = AutoModel.get_model(\n        model_args,\n        tune_strategy='none',\n        ds_config=ds_config,\n        device=pipeline_args.device,\n        use_accelerator=True,\n    )\n\n    data_args = DatasetArguments(dataset_path=None)\n    dataset = Dataset(data_args)\n\n    inferencer = AutoPipeline.get_pipeline(\n        pipeline_name=pipeline_name,\n        model_args=model_args,\n        data_args=data_args,\n        pipeline_args=pipeline_args,\n    )\n\n    model_name = model_args.model_name_or_path\n    if model_args.lora_model_path is not None:\n        model_name += f\" + {model_args.lora_model_path}\"\n\n    while True:\n        input_text = input(\"User >>> \")\n        input_text = input_text[-model.get_max_length():]\n\n        input_dataset = dataset.from_dict({\n            \"type\": \"text_only\",\n            \"instances\": [ { \"text\": input_text } ]\n        })\n\n        output_dataset = inferencer.inference(\n            model=model,\n            dataset=input_dataset,\n            max_new_tokens=inferencer_args.max_new_tokens,\n            temperature=inferencer_args.temperature,\n        )\n        output = output_dataset.to_dict()[\"instances\"][0][\"text\"]\n        print(output)\n"
    },
    {
      "id": "OptimalScale_LMFlow_221_11",
      "input_code": "def main():\n    pipeline_name = \"rm_inferencer\"\n    PipelineArguments = AutoArguments.get_pipeline_args_class(pipeline_name)\n\n    parser = HfArgumentParser((\n        ModelArguments, \n        DatasetArguments,\n        PipelineArguments\n    ))\n    if len(sys.argv) == 2 and sys.argv[1].endswith(\".json\"):\n        model_args, data_args, pipeline_args = parser.parse_json_file(json_file=os.path.abspath(sys.argv[1]))\n    else:\n        model_args, data_args, pipeline_args = parser.parse_args_into_dataclasses()\n\n    dataset = Dataset(data_args)\n    model = AutoModel.get_model(model_args, tune_strategy='none', use_accelerator=pipeline_args.use_accelerator)\n    inferencer = AutoPipeline.get_pipeline(\n        pipeline_name=pipeline_name,\n        model_args=model_args,\n        data_args=data_args,\n        pipeline_args=pipeline_args\n    )\n\n    res = inferencer.inference(\n        model,\n        dataset,\n    )\n    \n    if pipeline_args.save_results:\n        res.save(pipeline_args.results_path)\n"
    },
    {
      "id": "OptimalScale_LMFlow_221_8",
      "input_code": "def main():\n    setup_logger()\n\n    pipeline_name = \"finetuner\"\n    PipelineArguments = AutoArguments.get_pipeline_args_class(pipeline_name)\n\n    parser = HfArgumentParser((\n        ModelArguments,\n        DatasetArguments,\n        PipelineArguments,\n        MultistageFinetuneArgs,\n    ))\n    if len(sys.argv) == 2 and sys.argv[1].endswith(\".json\"):\n        model_args, data_args, pipeline_args = parser.parse_json_file(\n            json_file=os.path.abspath(sys.argv[1])\n        )\n    else:\n        model_args, data_args, pipeline_args, multistage_args = parser.parse_args_into_dataclasses()\n\n    full_dataset = Dataset(data_args)\n\n    finetuner_args = pipeline_args\n    logger.warning(\n        \"force set `--overwrite_output_dir True`\"\n        \" as required by multistage finetuning\"\n    )\n    finetuner_args.overwrite_output_dir = True\n\n    num_train_epochs = finetuner_args.num_train_epochs\n    if abs(num_train_epochs - int(num_train_epochs + 0.5)) > 1e-6:\n        raise ValueError(\"only support int-typed `--num_train_epochs`\")\n\n    output_dir = finetuner_args.output_dir\n    run_name = finetuner_args.run_name\n    finetuner_args.num_train_epochs = 1\n    shuffle_seed = multistage_args.shuffle_base_seed\n\n    for epoch in range(int(num_train_epochs)):\n        shuffle_seed = generate_new_seed(shuffle_seed)\n        dataset_list = shuffle_and_split_data(\n            full_dataset,\n            num_split=multistage_args.num_stages_per_epoch,\n            seed=shuffle_seed,\n        )\n        if epoch < multistage_args.start_epoch:\n            logging.info(f\"skip epoch {epoch}\")\n            continue\n\n        for stage, dataset in enumerate(dataset_list):\n            is_main_process = (finetuner_args.local_process_index == 0)\n            if is_main_process:\n                logger.setLevel(logging.INFO)\n                logger.info(f\"========== epoch {epoch} stage {stage} ==========\")\n\n            model = AutoModel.get_model(model_args)\n            finetuner_args.output_dir = (\n                output_dir + f\"_epoch-{epoch}_stage-{stage}\"\n            )\n            finetuner_args.run_name = run_name + f\"_epoch-{epoch}_stage-{stage}\"\n\n            finetuner = AutoPipeline.get_pipeline(\n                pipeline_name=pipeline_name,\n                model_args=model_args,\n                data_args=data_args,\n                pipeline_args=finetuner_args,\n            )\n\n            tuned_model = finetuner.tune(\n                model=model,\n                dataset=dataset,\n                transform_dataset_in_place=False,\n            )\n            model_args.model_name_or_path = finetuner_args.output_dir\n            del model\n            del tuned_model\n            del finetuner\n            tuned_model = None\n            model = None\n            finetuner = None\n            gc.collect()\n\n\n    tuned_model.save(output_dir)\n"
    },
    {
      "id": "OptimalScale_LMFlow_221_1",
      "input_code": "def main():\n    logging.basicConfig()\n    pipeline_name = \"evaluator\"\n    PipelineArguments = AutoArguments.get_pipeline_args_class(pipeline_name)\n\n    parser = HfArgumentParser((\n        ModelArguments, PipelineArguments, BenchmarkingArguments\n    ))\n    model_args, pipeline_args, benchmarking_args = parser.parse_args_into_dataclasses()\n\n    with open (pipeline_args.deepspeed, \"r\") as f:\n        ds_config = json.load(f)\n    dataset_name = benchmarking_args.dataset_name\n    if is_lmflow_local_benchmarking(dataset_name):\n        model = AutoModel.get_model(model_args, tune_strategy='none', ds_config=ds_config)\n        run_lmflow_local_benchmarking(dataset_name,pipeline_name,model_args,pipeline_args,model)\n    elif is_lm_evaluation_benchmarking(dataset_name):\n        model = model_args.model_name_or_path\n        run_lm_evaluation_benchmarking(dataset_name, model)\n    else:\n        raise NotImplementedError(\n            f\"benchmarking dataset {dataset_name} \"\n            \" is not supported\"\n        )\n"
    },
    {
      "id": "OptimalScale_LMFlow_221_10",
      "input_code": "def main():\n    pipeline_name = \"rm_tuner\"\n    PipelineArguments = AutoArguments.get_pipeline_args_class(pipeline_name)\n\n    parser = HfArgumentParser((ModelArguments, DatasetArguments, PipelineArguments))\n    if len(sys.argv) == 2 and sys.argv[1].endswith(\".json\"):\n        model_args, data_args, pipeline_args = parser.parse_json_file(json_file=os.path.abspath(sys.argv[1]))\n    else:\n        model_args, data_args, pipeline_args = parser.parse_args_into_dataclasses()\n\n    finetuner = AutoPipeline.get_pipeline(\n        pipeline_name=pipeline_name,\n        model_args=model_args,\n        data_args=data_args,\n        pipeline_args=pipeline_args,\n    )\n    dataset = Dataset(data_args)\n    model = AutoModel.get_model(model_args)\n        \n    tuned_model = finetuner.tune(model=model, dataset=dataset)\n"
    },
    {
      "id": "OptimalScale_LMFlow_221_5",
      "input_code": "def main():\n    pipeline_name = \"finetuner\"\n    PipelineArguments = AutoArguments.get_pipeline_args_class(pipeline_name)\n\n    parser = HfArgumentParser((VisModelArguments, MultiModalDatasetArguments, PipelineArguments))\n    if len(sys.argv) == 2 and sys.argv[1].endswith(\".json\"):\n        model_args, data_args, pipeline_args = parser.parse_json_file(json_file=os.path.abspath(sys.argv[1]))\n    else:\n        model_args, data_args, pipeline_args = parser.parse_args_into_dataclasses()\n\n    finetuner = AutoPipeline.get_pipeline(\n        pipeline_name=pipeline_name,\n        model_args=model_args,\n        data_args=data_args,\n        pipeline_args=pipeline_args,\n    )\n    model = AutoModel.get_model(model_args, tune_strategy='none',\n                                ds_config=pipeline_args.deepspeed,\n                                custom_model=True,\n                                with_deepspeed=False,\n                                pipeline_args=pipeline_args)\n    for param in model.backend_model.parameters():\n        param.requires_grad = False\n    if \"language_projection\" in pipeline_args.finetune_part:\n        for param in model.backend_model.language_projection.parameters():\n            param.requires_grad = True\n    if \"language_model\" in pipeline_args.finetune_part:\n        for param in model.backend_model.language_model.parameters():\n            param.requires_grad = True\n    if \"vision_model\" in pipeline_args.finetune_part:\n        for param in model.backend_model.vision_model.parameters():\n            param.requires_grad = True\n\n    dataset = Dataset(data_args, backend=\"custom_multi_modal\")\n    data_collator = DataCollatorForSupervisedDataset(tokenizer=model.tokenizer)\n\n    tuned_model = finetuner.tune(\n        model=model, dataset=dataset, data_collator=data_collator)\n"
    },
    {
      "id": "OptimalScale_LMFlow_221_14",
      "input_code": "def main():\n    pipeline_name = \"vllm_inferencer\"\n    PipelineArguments = AutoArguments.get_pipeline_args_class(pipeline_name)\n\n    parser = HfArgumentParser((\n        ModelArguments, \n        DatasetArguments,\n        PipelineArguments\n    ))\n    if len(sys.argv) == 2 and sys.argv[1].endswith(\".json\"):\n        model_args, data_args, pipeline_args = parser.parse_json_file(json_file=os.path.abspath(sys.argv[1]))\n    else:\n        model_args, data_args, pipeline_args = parser.parse_args_into_dataclasses()\n\n    dataset = Dataset(data_args)\n    model = AutoModel.get_model(model_args, tune_strategy='none')\n    inferencer = AutoPipeline.get_pipeline(\n        pipeline_name=pipeline_name,\n        model_args=model_args,\n        data_args=data_args,\n        pipeline_args=pipeline_args\n    )\n\n    res = inferencer.inference(\n        model,\n        dataset,\n        release_gpu=False,\n        enable_decode_inference_result=pipeline_args.enable_decode_inference_result,\n        enable_distributed_vllm_inference=pipeline_args.enable_distributed_vllm_inference,\n    )\n"
    }
  ],
  "generated_inputs": [
    "<needimport>\nimport json\nimport os\n\n<testcase_1>\nmodel_args = ModelArguments(arch_type=\"decoder_only\", model_name_or_path=\"gpt2\")\nargs = ()\nkwargs = {}\n\n<testcase_2>\nmodel_args = ModelArguments(arch_type=\"text_regression\", model_name_or_path=\"bert-base-uncased\")\nargs = ()\nkwargs = {\"tune_strategy\": \"none\"}\n\n<testcase_3>\nmodel_args = ModelArguments(arch_type=\"encoder_decoder\", model_name_or_path=\"t5-base\")\nargs = ()\nkwargs = {\"device\": \"cpu\", \"ds_config\": {\"train_batch_size\": 8}}\n\n<testcase_4>\nmodel_args = ModelArguments(arch_type=\"vision_encoder_decoder\", model_name_or_path=\"vit-gpt2\")\nargs = ()\nkwargs = {\"custom_model\": True, \"with_deepspeed\": False}\n\n<testcase_5>\nmodel_args = ModelArguments(arch_type=\"decoder_only\", model_name_or_path=\"gpt-neo\")\nargs = ()\nkwargs = {\"use_accelerator\": True, \"device\": \"cuda\"}"
  ],
  "gpt_output": "<needimport>\nimport json\nimport os\n\n<testcase_1>\nmodel_args = ModelArguments(arch_type=\"decoder_only\", model_name_or_path=\"gpt2\")\nargs = ()\nkwargs = {}\n\n<testcase_2>\nmodel_args = ModelArguments(arch_type=\"text_regression\", model_name_or_path=\"bert-base-uncased\")\nargs = ()\nkwargs = {\"tune_strategy\": \"none\"}\n\n<testcase_3>\nmodel_args = ModelArguments(arch_type=\"encoder_decoder\", model_name_or_path=\"t5-base\")\nargs = ()\nkwargs = {\"device\": \"cpu\", \"ds_config\": {\"train_batch_size\": 8}}\n\n<testcase_4>\nmodel_args = ModelArguments(arch_type=\"vision_encoder_decoder\", model_name_or_path=\"vit-gpt2\")\nargs = ()\nkwargs = {\"custom_model\": True, \"with_deepspeed\": False}\n\n<testcase_5>\nmodel_args = ModelArguments(arch_type=\"decoder_only\", model_name_or_path=\"gpt-neo\")\nargs = ()\nkwargs = {\"use_accelerator\": True, \"device\": \"cuda\"}",
  "needimport": "import json\nimport os\n\n",
  "test_results": {
    "ans1": {
      "info": "DecoderModel(gpt2)"
    },
    "ans2": {
      "info": "TextRegressionModel(bert-base-uncased)"
    },
    "ans3": {
      "info": "EncoderDecoderModel(t5-base)"
    },
    "ans4": {
      "info": "EncoderDecoderModel(vit-gpt2)"
    },
    "ans5": {
      "info": "DecoderModel(gpt-neo)"
    }
  }
}