{
  "input_header": "def normalize_arg(x):",
  "input_docstring": "Normalize user provided arguments to blockwise or map_blocks\n\nWe do a few things:\n\n1.  If they are string literals that might collide with blockwise_token then we\n    quote them\n2.  IF they are large (as defined by sizeof) then we put them into the\n    graph on their own by using dask.delayed",
  "output_code": "def normalize_arg(x):\n    \n    if is_dask_collection(x):\n        return x\n    elif isinstance(x, str) and re.match(r\"_\\d+\", x):\n        return delayed(x)\n    elif isinstance(x, list) and len(x) >= 10:\n        return delayed(x)\n    elif sizeof(x) > 1e6:\n        return delayed(x)\n    else:\n        return x",
  "input_contexts": [
    {
      "id": "dask_dask_663_1",
      "input_code": "def blockwise(\n    func,\n    out_ind,\n    *args,\n    name=None,\n    token=None,\n    dtype=None,\n    adjust_chunks=None,\n    new_axes=None,\n    align_arrays=True,\n    concatenate=None,\n    meta=None,\n    **kwargs,\n):\n    \n    out = name\n    new_axes = new_axes or {}\n\n    if len(set(out_ind)) != len(out_ind):\n        raise ValueError(\n            \"Repeated elements not allowed in output index\",\n            [k for k, v in toolz.frequencies(out_ind).items() if v > 1],\n        )\n    new = (\n        set(out_ind)\n        - {a for arg in args[1::2] if arg is not None for a in arg}\n        - set(new_axes or ())\n    )\n    if new:\n        raise ValueError(\"Unknown dimension\", new)\n\n    from dask.array.core import normalize_arg, unify_chunks\n\n    if align_arrays:\n        chunkss, arrays = unify_chunks(*args)\n    else:\n        arginds = [(a, i) for (a, i) in toolz.partition(2, args) if i is not None]\n        chunkss = {}\n        for arg, ind in arginds:\n            for c, i in zip(arg.chunks, ind):\n                if i not in chunkss or len(c) > len(chunkss[i]):\n                    chunkss[i] = c\n        arrays = args[::2]\n\n    for k, v in new_axes.items():\n        if not isinstance(v, tuple):\n            v = (v,)\n        chunkss[k] = v\n\n    arginds = list(zip(arrays, args[1::2]))\n    numblocks = {}\n\n    dependencies = []\n    arrays = []\n\n    argindsstr = []\n\n    for arg, ind in arginds:\n        if ind is None:\n            arg = normalize_arg(arg)\n            arg, collections = _blockwise_unpack_collections_task_spec(arg)\n\n            dependencies.extend(collections)\n        else:\n            if (\n                hasattr(arg, \"ndim\")\n                and hasattr(ind, \"__len__\")\n                and arg.ndim != len(ind)\n            ):\n                raise ValueError(\n                    \"Index string %s does not match array dimension %d\"\n                    % (ind, arg.ndim)\n                )\n            if not isinstance(arg, ArrayBlockwiseDep):\n                numblocks[arg.name] = arg.numblocks\n                arrays.append(arg)\n                arg = arg.name\n        argindsstr.extend((arg, ind))\n\n    kwargs2 = {}\n    for k, v in kwargs.items():\n        v = normalize_arg(v)\n        v, collections = _blockwise_unpack_collections_task_spec(v)\n        dependencies.extend(collections)\n        kwargs2[k] = v\n\n    if not out:\n        out = \"{}-{}\".format(\n            token or utils.funcname(func).strip(\"_\"),\n            base.tokenize(func, out_ind, argindsstr, dtype, **kwargs),\n        )\n\n    graph = core_blockwise(\n        func,\n        out,\n        out_ind,\n        *argindsstr,\n        numblocks=numblocks,\n        dependencies=dependencies,\n        new_axes=new_axes,\n        concatenate=concatenate,\n        **kwargs2,\n    )\n    graph = HighLevelGraph.from_collections(\n        out, graph, dependencies=arrays + dependencies\n    )\n\n    chunks = [chunkss[i] for i in out_ind]\n    if adjust_chunks:\n        for i, ind in enumerate(out_ind):\n            if ind in adjust_chunks:\n                if callable(adjust_chunks[ind]):\n                    chunks[i] = tuple(map(adjust_chunks[ind], chunks[i]))\n                elif isinstance(adjust_chunks[ind], numbers.Integral):\n                    chunks[i] = tuple(adjust_chunks[ind] for _ in chunks[i])\n                elif isinstance(adjust_chunks[ind], (tuple, list)):\n                    if len(adjust_chunks[ind]) != len(chunks[i]):\n                        raise ValueError(\n                            f\"Dimension {i} has {len(chunks[i])} blocks, adjust_chunks \"\n                            f\"specified with {len(adjust_chunks[ind])} blocks\"\n                        )\n                    chunks[i] = tuple(adjust_chunks[ind])\n                else:\n                    raise NotImplementedError(\n                        \"adjust_chunks values must be callable, int, or tuple\"\n                    )\n    chunks = tuple(chunks)\n\n    if meta is None:\n        from dask.array.utils import compute_meta\n\n        meta = compute_meta(func, dtype, *args[::2], **kwargs)\n    return new_da_object(graph, out, chunks, meta=meta, dtype=dtype)\n"
    }
  ],
  "generated_inputs": [
    "<needimport>\nimport dask\nimport re\n\n<testcase_1>\nx = \"example_string\"\n\n<testcase_2>\nx = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n\n<testcase_3>\nx = dask.delayed(lambda: \"delayed_string\")()\n\n<testcase_4>\nx = \"_12345\"\n\n<testcase_5>\nx = [i for i in range(1000000)]"
  ],
  "gpt_output": "<needimport>\nimport dask\nimport re\n\n<testcase_1>\nx = \"example_string\"\n\n<testcase_2>\nx = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n\n<testcase_3>\nx = dask.delayed(lambda: \"delayed_string\")()\n\n<testcase_4>\nx = \"_12345\"\n\n<testcase_5>\nx = [i for i in range(1000000)]",
  "needimport": "import dask\nimport re\n\n",
  "test_results": {
    "ans1": "example_string",
    "ans2": "Delayed('list-ea38d049-9306-4a31-94dd-33a56c8f0095')",
    "ans3": "Delayed('lambda-f54282e2-40d3-4883-b9da-55b6e8e7730d')",
    "ans4": "Delayed('str-505f067e-a8ca-4ffd-a555-65bf29600bcc')",
    "ans5": "Delayed('list-5077c3ba-2422-4dbc-8ddc-ff3d30ca002d')"
  }
}