{
  "input_header": "def set_recovering(job_id: int, task_id: int, callback_func: CallbackType):",
  "input_docstring": "Set the task to recovering state, and update the job duration.",
  "output_code": "def set_recovering(job_id: int, task_id: int, callback_func: CallbackType):\n    \n    logger.info('=== Recovering... ===')\n    with db_utils.safe_cursor(_DB_PATH) as cursor:\n        cursor.execute(\n            \"\"\"\\\n                UPDATE spot SET\n                status=(?), job_duration=job_duration+(?)-last_recovered_at\n                WHERE spot_job_id=(?) AND\n                task_id=(?)\"\"\",\n            (ManagedJobStatus.RECOVERING.value, time.time(), job_id, task_id))\n    callback_func('RECOVERING')",
  "input_contexts": [
    {
      "id": "skypilot-org_skypilot_1627_1",
      "input_code": "    def _run_one_task(self, task_id: int, task: 'sky.Task') -> bool:\n        \n\n        callback_func = managed_job_utils.event_callback_func(\n            job_id=self._job_id, task_id=task_id, task=task)\n        if task.run is None:\n            logger.info(f'Skip running task {task_id} ({task.name}) due to its '\n                        'run commands being empty.')\n            managed_job_state.set_started(job_id=self._job_id,\n                                          task_id=task_id,\n                                          start_time=time.time(),\n                                          callback_func=callback_func)\n            managed_job_state.set_succeeded(job_id=self._job_id,\n                                            task_id=task_id,\n                                            end_time=time.time(),\n                                            callback_func=callback_func)\n            return True\n        usage_lib.messages.usage.update_task_id(task_id)\n        task_id_env_var = task.envs[constants.TASK_ID_ENV_VAR]\n        submitted_at = time.time()\n        if task_id == 0:\n            submitted_at = backend_utils.get_timestamp_from_run_timestamp(\n                self._backend.run_timestamp)\n        assert task.name is not None, task\n        cluster_name = managed_job_utils.generate_managed_job_cluster_name(\n            task.name, self._job_id)\n        self._strategy_executor = recovery_strategy.StrategyExecutor.make(\n            cluster_name, self._backend, task, self._job_id)\n        managed_job_state.set_submitted(\n            self._job_id,\n            task_id,\n            self._backend.run_timestamp,\n            submitted_at,\n            resources_str=backend_utils.get_task_resources_str(\n                task, is_managed_job=True),\n            specs={\n                'max_restarts_on_errors':\n                    self._strategy_executor.max_restarts_on_errors\n            },\n            callback_func=callback_func)\n        logger.info(\n            f'Submitted managed job {self._job_id} (task: {task_id}, name: '\n            f'{task.name!r}); {constants.TASK_ID_ENV_VAR}: {task_id_env_var}')\n\n        logger.info('Started monitoring.')\n        managed_job_state.set_starting(job_id=self._job_id,\n                                       task_id=task_id,\n                                       callback_func=callback_func)\n        remote_job_submitted_at = self._strategy_executor.launch()\n        assert remote_job_submitted_at is not None, remote_job_submitted_at\n\n        managed_job_state.set_started(job_id=self._job_id,\n                                      task_id=task_id,\n                                      start_time=remote_job_submitted_at,\n                                      callback_func=callback_func)\n\n        while True:\n            time.sleep(managed_job_utils.JOB_STATUS_CHECK_GAP_SECONDS)\n\n            try:\n                backend_utils.check_network_connection()\n            except exceptions.NetworkError:\n                logger.info('Network is not available. Retrying again in '\n                            f'{managed_job_utils.JOB_STATUS_CHECK_GAP_SECONDS} '\n                            'seconds.')\n                continue\n\n            job_status = managed_job_utils.get_job_status(\n                self._backend, cluster_name)\n\n            if job_status == job_lib.JobStatus.SUCCEEDED:\n                end_time = managed_job_utils.get_job_timestamp(\n                    self._backend, cluster_name, get_end_time=True)\n                managed_job_state.set_succeeded(self._job_id,\n                                                task_id,\n                                                end_time=end_time,\n                                                callback_func=callback_func)\n                logger.info(\n                    f'Managed job {self._job_id} (task: {task_id}) SUCCEEDED. '\n                    f'Cleaning up the cluster {cluster_name}.')\n                clusters = backend_utils.get_clusters(\n                    cluster_names=[cluster_name],\n                    refresh=False,\n                    include_controller=False)\n                if clusters:\n                    assert len(clusters) == 1, (clusters, cluster_name)\n                    handle = clusters[0].get('handle')\n                    self._download_log_and_stream(task_id, handle)\n                managed_job_utils.terminate_cluster(cluster_name=cluster_name)\n                return True\n\n            if (job_status is not None and not job_status.is_terminal() and\n                    task.num_nodes == 1):\n                continue\n\n            if job_status in job_lib.JobStatus.user_code_failure_states():\n                time.sleep(5)\n\n            (cluster_status,\n             handle) = backend_utils.refresh_cluster_status_handle(\n                 cluster_name,\n                 force_refresh_statuses=set(status_lib.ClusterStatus))\n\n            if cluster_status != status_lib.ClusterStatus.UP:\n                cluster_status_str = ('' if cluster_status is None else\n                                      f' (status: {cluster_status.value})')\n                logger.info(\n                    f'Cluster is preempted or failed{cluster_status_str}. '\n                    'Recovering...')\n            else:\n                if job_status is not None and not job_status.is_terminal():\n                    continue\n                elif job_status in job_lib.JobStatus.user_code_failure_states():\n                    end_time = managed_job_utils.get_job_timestamp(\n                        self._backend, cluster_name, get_end_time=True)\n                    logger.info(\n                        'The user job failed. Please check the logs below.\\n'\n                        f'== Logs of the user job (ID: {self._job_id}) ==\\n')\n\n                    self._download_log_and_stream(task_id, handle)\n                    managed_job_status = (\n                        managed_job_state.ManagedJobStatus.FAILED)\n                    if job_status == job_lib.JobStatus.FAILED_SETUP:\n                        managed_job_status = (\n                            managed_job_state.ManagedJobStatus.FAILED_SETUP)\n                    failure_reason = (\n                        'To see the details, run: '\n                        f'sky jobs logs --controller {self._job_id}')\n                    should_restart_on_failure = (\n                        self._strategy_executor.should_restart_on_failure())\n                    if should_restart_on_failure:\n                        max_restarts = (\n                            self._strategy_executor.max_restarts_on_errors)\n                        logger.info(\n                            f'User program crashed '\n                            f'({managed_job_status.value}). '\n                            f'Retry the job as max_restarts_on_errors is '\n                            f'set to {max_restarts}. '\n                            f'[{self._strategy_executor.restart_cnt_on_failure}'\n                            f'/{max_restarts}]')\n                    else:\n                        managed_job_state.set_failed(\n                            self._job_id,\n                            task_id,\n                            failure_type=managed_job_status,\n                            failure_reason=failure_reason,\n                            end_time=end_time,\n                            callback_func=callback_func)\n                        return False\n                else:\n                    assert job_status is None, job_status\n                    logger.info('Failed to fetch the job status while the '\n                                'cluster is healthy. Try to recover the job '\n                                '(the cluster will not be restarted).')\n            if handle is not None:\n                resources = handle.launched_resources\n                assert resources is not None, handle\n                if resources.need_cleanup_after_preemption_or_failure():\n                    logger.info('Cleaning up the preempted or failed cluster'\n                                '...')\n                    managed_job_utils.terminate_cluster(cluster_name)\n\n            managed_job_state.set_recovering(job_id=self._job_id,\n                                             task_id=task_id,\n                                             callback_func=callback_func)\n            recovered_time = self._strategy_executor.recover()\n            managed_job_state.set_recovered(self._job_id,\n                                            task_id,\n                                            recovered_time=recovered_time,\n                                            callback_func=callback_func)\n"
    }
  ],
  "generated_inputs": [
    "<needimport>\nimport time\nfrom typing import Callable\n\n<testcase_1>\njob_id = 101\ntask_id = 1\ncallback_func = lambda status: print(f\"Callback called with status: {status}\")\n\n<testcase_2>\njob_id = 202\ntask_id = 2\ncallback_func = lambda status: print(f\"Job is now in {status} state\")\n\n<testcase_3>\njob_id = 303\ntask_id = 3\ncallback_func = lambda status: print(f\"Status updated to: {status}\")\n\n<testcase_4>\njob_id = 404\ntask_id = 4\ncallback_func = lambda status: print(f\"Current status: {status}\")\n\n<testcase_5>\njob_id = 505\ntask_id = 5\ncallback_func = lambda status: print(f\"Task status changed to: {status}\")"
  ],
  "gpt_output": "<needimport>\nimport time\nfrom typing import Callable\n\n<testcase_1>\njob_id = 101\ntask_id = 1\ncallback_func = lambda status: print(f\"Callback called with status: {status}\")\n\n<testcase_2>\njob_id = 202\ntask_id = 2\ncallback_func = lambda status: print(f\"Job is now in {status} state\")\n\n<testcase_3>\njob_id = 303\ntask_id = 3\ncallback_func = lambda status: print(f\"Status updated to: {status}\")\n\n<testcase_4>\njob_id = 404\ntask_id = 4\ncallback_func = lambda status: print(f\"Current status: {status}\")\n\n<testcase_5>\njob_id = 505\ntask_id = 5\ncallback_func = lambda status: print(f\"Task status changed to: {status}\")",
  "needimport": "import time\nfrom typing import Callable\n\n",
  "test_results": {
    "ans1": "Recovery set for job 101, task 1",
    "ans2": "Recovery set for job 202, task 2",
    "ans3": "Recovery set for job 303, task 3",
    "ans4": "Recovery set for job 404, task 4",
    "ans5": "Recovery set for job 505, task 5"
  }
}