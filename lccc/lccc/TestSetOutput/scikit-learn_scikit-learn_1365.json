{
  "input_header": "def fast_mcd( X, support_fraction=None, cov_computation_method=empirical_covariance, random_state=None, ):",
  "input_docstring": "Estimate the Minimum Covariance Determinant matrix.\n\nRead more in the :ref:`User Guide <robust_covariance>`.\n\nParameters\n----------\nX : array-like of shape (n_samples, n_features)\n    The data matrix, with p features and n samples.\n\nsupport_fraction : float, default=None\n    The proportion of points to be included in the support of the raw\n    MCD estimate. Default is `None`, which implies that the minimum\n    value of `support_fraction` will be used within the algorithm:\n    `(n_samples + n_features + 1) / 2 * n_samples`. This parameter must be\n    in the range (0, 1).\n\ncov_computation_method : callable,             default=:func:`sklearn.covariance.empirical_covariance`\n    The function which will be used to compute the covariance.\n    Must return an array of shape (n_features, n_features).\n\nrandom_state : int, RandomState instance or None, default=None\n    Determines the pseudo random number generator for shuffling the data.\n    Pass an int for reproducible results across multiple function calls.\n    See :term:`Glossary <random_state>`.\n\nReturns\n-------\nlocation : ndarray of shape (n_features,)\n    Robust location of the data.\n\ncovariance : ndarray of shape (n_features, n_features)\n    Robust covariance of the features.\n\nsupport : ndarray of shape (n_samples,), dtype=bool\n    A mask of the observations that have been used to compute\n    the robust location and covariance estimates of the data set.\n\nNotes\n-----\nThe FastMCD algorithm has been introduced by Rousseuw and Van Driessen\nin \"A Fast Algorithm for the Minimum Covariance Determinant Estimator,\n1999, American Statistical Association and the American Society\nfor Quality, TECHNOMETRICS\".\nThe principle is to compute robust estimates and random subsets before\npooling them into a larger subsets, and finally into the full data set.\nDepending on the size of the initial sample, we have one, two or three\nsuch computation levels.\n\nNote that only raw estimates are returned. If one is interested in\nthe correction and reweighting steps described in [RouseeuwVan]_,\nsee the MinCovDet object.\n\nReferences\n----------\n\n.. [RouseeuwVan] A Fast Algorithm for the Minimum Covariance\n    Determinant Estimator, 1999, American Statistical Association\n    and the American Society for Quality, TECHNOMETRICS\n\n.. [Butler1993] R. W. Butler, P. L. Davies and M. Jhun,\n    Asymptotics For The Minimum Covariance Determinant Estimator,\n    The Annals of Statistics, 1993, Vol. 21, No. 3, 1385-1400",
  "output_code": "def fast_mcd(\n    X,\n    support_fraction=None,\n    cov_computation_method=empirical_covariance,\n    random_state=None,\n):\n    \n    random_state = check_random_state(random_state)\n\n    X = check_array(X, ensure_min_samples=2, estimator=\"fast_mcd\")\n    n_samples, n_features = X.shape\n\n    if support_fraction is None:\n        n_support = min(int(np.ceil(0.5 * (n_samples + n_features + 1))), n_samples)\n    else:\n        n_support = int(support_fraction * n_samples)\n\n    if n_features == 1:\n        if n_support < n_samples:\n            X_sorted = np.sort(np.ravel(X))\n            diff = X_sorted[n_support:] - X_sorted[: (n_samples - n_support)]\n            halves_start = np.where(diff == np.min(diff))[0]\n            location = (\n                0.5\n                * (X_sorted[n_support + halves_start] + X_sorted[halves_start]).mean()\n            )\n            support = np.zeros(n_samples, dtype=bool)\n            X_centered = X - location\n            support[np.argsort(np.abs(X_centered), 0)[:n_support]] = True\n            covariance = np.asarray([[np.var(X[support])]])\n            location = np.array([location])\n            precision = linalg.pinvh(covariance)\n            dist = (np.dot(X_centered, precision) * (X_centered)).sum(axis=1)\n        else:\n            support = np.ones(n_samples, dtype=bool)\n            covariance = np.asarray([[np.var(X)]])\n            location = np.asarray([np.mean(X)])\n            X_centered = X - location\n            precision = linalg.pinvh(covariance)\n            dist = (np.dot(X_centered, precision) * (X_centered)).sum(axis=1)\n    if (n_samples > 500) and (n_features > 1):\n        n_subsets = n_samples // 300\n        n_samples_subsets = n_samples // n_subsets\n        samples_shuffle = random_state.permutation(n_samples)\n        h_subset = int(np.ceil(n_samples_subsets * (n_support / float(n_samples))))\n        n_trials_tot = 500\n        n_best_sub = 10\n        n_trials = max(10, n_trials_tot // n_subsets)\n        n_best_tot = n_subsets * n_best_sub\n        all_best_locations = np.zeros((n_best_tot, n_features))\n        try:\n            all_best_covariances = np.zeros((n_best_tot, n_features, n_features))\n        except MemoryError:\n            n_best_tot = 10\n            all_best_covariances = np.zeros((n_best_tot, n_features, n_features))\n            n_best_sub = 2\n        for i in range(n_subsets):\n            low_bound = i * n_samples_subsets\n            high_bound = low_bound + n_samples_subsets\n            current_subset = X[samples_shuffle[low_bound:high_bound]]\n            best_locations_sub, best_covariances_sub, _, _ = select_candidates(\n                current_subset,\n                h_subset,\n                n_trials,\n                select=n_best_sub,\n                n_iter=2,\n                cov_computation_method=cov_computation_method,\n                random_state=random_state,\n            )\n            subset_slice = np.arange(i * n_best_sub, (i + 1) * n_best_sub)\n            all_best_locations[subset_slice] = best_locations_sub\n            all_best_covariances[subset_slice] = best_covariances_sub\n        n_samples_merged = min(1500, n_samples)\n        h_merged = int(np.ceil(n_samples_merged * (n_support / float(n_samples))))\n        if n_samples > 1500:\n            n_best_merged = 10\n        else:\n            n_best_merged = 1\n        selection = random_state.permutation(n_samples)[:n_samples_merged]\n        locations_merged, covariances_merged, supports_merged, d = select_candidates(\n            X[selection],\n            h_merged,\n            n_trials=(all_best_locations, all_best_covariances),\n            select=n_best_merged,\n            cov_computation_method=cov_computation_method,\n            random_state=random_state,\n        )\n        if n_samples < 1500:\n            location = locations_merged[0]\n            covariance = covariances_merged[0]\n            support = np.zeros(n_samples, dtype=bool)\n            dist = np.zeros(n_samples)\n            support[selection] = supports_merged[0]\n            dist[selection] = d[0]\n        else:\n            locations_full, covariances_full, supports_full, d = select_candidates(\n                X,\n                n_support,\n                n_trials=(locations_merged, covariances_merged),\n                select=1,\n                cov_computation_method=cov_computation_method,\n                random_state=random_state,\n            )\n            location = locations_full[0]\n            covariance = covariances_full[0]\n            support = supports_full[0]\n            dist = d[0]\n    elif n_features > 1:\n        n_trials = 30\n        n_best = 10\n        locations_best, covariances_best, _, _ = select_candidates(\n            X,\n            n_support,\n            n_trials=n_trials,\n            select=n_best,\n            n_iter=2,\n            cov_computation_method=cov_computation_method,\n            random_state=random_state,\n        )\n        locations_full, covariances_full, supports_full, d = select_candidates(\n            X,\n            n_support,\n            n_trials=(locations_best, covariances_best),\n            select=1,\n            cov_computation_method=cov_computation_method,\n            random_state=random_state,\n        )\n        location = locations_full[0]\n        covariance = covariances_full[0]\n        support = supports_full[0]\n        dist = d[0]\n\n    return location, covariance, support, dist",
  "input_contexts": [
    {
      "id": "scikit-learn_scikit-learn_1365_1",
      "input_code": "    def fit(self, X, y=None):\n        \n        X = validate_data(self, X, ensure_min_samples=2, estimator=\"MinCovDet\")\n        random_state = check_random_state(self.random_state)\n        n_samples, n_features = X.shape\n        if (linalg.svdvals(np.dot(X.T, X)) > 1e-8).sum() != n_features:\n            warnings.warn(\n                \"The covariance matrix associated to your dataset is not full rank\"\n            )\n        raw_location, raw_covariance, raw_support, raw_dist = fast_mcd(\n            X,\n            support_fraction=self.support_fraction,\n            cov_computation_method=self._nonrobust_covariance,\n            random_state=random_state,\n        )\n        if self.assume_centered:\n            raw_location = np.zeros(n_features)\n            raw_covariance = self._nonrobust_covariance(\n                X[raw_support], assume_centered=True\n            )\n            precision = linalg.pinvh(raw_covariance)\n            raw_dist = np.sum(np.dot(X, precision) * X, 1)\n        self.raw_location_ = raw_location\n        self.raw_covariance_ = raw_covariance\n        self.raw_support_ = raw_support\n        self.location_ = raw_location\n        self.support_ = raw_support\n        self.dist_ = raw_dist\n        self.correct_covariance(X)\n        self.reweight_covariance(X)\n\n        return self\n"
    },
    {
      "id": "scikit-learn_scikit-learn_1365_2",
      "input_code": "def test_fast_mcd_on_invalid_input():\n    X = np.arange(100)\n    msg = \"Expected 2D array, got 1D array instead\"\n    with pytest.raises(ValueError, match=msg):\n        fast_mcd(X)\n"
    }
  ],
  "generated_inputs": [
    "<needimport>\nimport numpy as np\nfrom sklearn.covariance import empirical_covariance\nfrom sklearn.utils import check_random_state, check_array\n\n<testcase_1>\nX = np.random.rand(100, 5)\nsupport_fraction = 0.5\ncov_computation_method = empirical_covariance\nrandom_state = 42\n\n<testcase_2>\nX = np.random.rand(50, 3)\nsupport_fraction = 0.7\ncov_computation_method = empirical_covariance\nrandom_state = None\n\n<testcase_3>\nX = np.random.rand(200, 10)\nsupport_fraction = None\ncov_computation_method = empirical_covariance\nrandom_state = np.random.RandomState(0)\n\n<testcase_4>\nX = np.random.rand(300, 8)\nsupport_fraction = 0.6\ncov_computation_method = empirical_covariance\nrandom_state = 123\n\n<testcase_5>\nX = np.random.rand(150, 4)\nsupport_fraction = 0.8\ncov_computation_method = empirical_covariance\nrandom_state = np.random.RandomState(1)"
  ],
  "gpt_output": "<needimport>\nimport numpy as np\nfrom sklearn.covariance import empirical_covariance\nfrom sklearn.utils import check_random_state, check_array\n\n<testcase_1>\nX = np.random.rand(100, 5)\nsupport_fraction = 0.5\ncov_computation_method = empirical_covariance\nrandom_state = 42\n\n<testcase_2>\nX = np.random.rand(50, 3)\nsupport_fraction = 0.7\ncov_computation_method = empirical_covariance\nrandom_state = None\n\n<testcase_3>\nX = np.random.rand(200, 10)\nsupport_fraction = None\ncov_computation_method = empirical_covariance\nrandom_state = np.random.RandomState(0)\n\n<testcase_4>\nX = np.random.rand(300, 8)\nsupport_fraction = 0.6\ncov_computation_method = empirical_covariance\nrandom_state = 123\n\n<testcase_5>\nX = np.random.rand(150, 4)\nsupport_fraction = 0.8\ncov_computation_method = empirical_covariance\nrandom_state = np.random.RandomState(1)",
  "needimport": "import numpy as np\nfrom sklearn.covariance import empirical_covariance\nfrom sklearn.utils import check_random_state, check_array\n\n",
  "test_results": {
    "ans1": [
      [
        0.538083786743351,
        0.47331254298230846,
        0.47335308011134036,
        0.5269649494702985,
        0.5200590267493467
      ],
      [
        [
          0.0864461342199242,
          -0.003075305598893154,
          -0.0053758501798589695,
          0.005019441611907843,
          0.01045563417121979
        ],
        [
          -0.003075305598893154,
          0.08511869257134148,
          -0.007902242604335663,
          0.0014458717157923293,
          -0.0016287219059578756
        ],
        [
          -0.0053758501798589695,
          -0.007902242604335663,
          0.08079530818048838,
          -0.0046118092105149975,
          -0.02567402693965787
        ],
        [
          0.005019441611907843,
          0.0014458717157923293,
          -0.0046118092105149975,
          0.07462377798018092,
          -0.018376179860954904
        ],
        [
          0.01045563417121979,
          -0.0016287219059578756,
          -0.02567402693965787,
          -0.018376179860954904,
          0.09174356157302732
        ]
      ],
      [
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false
      ],
      [
        Infinity,
        Infinity,
        Infinity,
        Infinity,
        Infinity,
        Infinity,
        Infinity,
        Infinity,
        Infinity,
        Infinity,
        Infinity,
        Infinity,
        Infinity,
        Infinity,
        Infinity,
        Infinity,
        Infinity,
        Infinity,
        Infinity,
        Infinity,
        Infinity,
        Infinity,
        Infinity,
        Infinity,
        Infinity,
        Infinity,
        Infinity,
        Infinity,
        Infinity,
        Infinity,
        Infinity,
        Infinity,
        Infinity,
        Infinity,
        Infinity,
        Infinity,
        Infinity,
        Infinity,
        Infinity,
        Infinity,
        Infinity,
        Infinity,
        Infinity,
        Infinity,
        Infinity,
        Infinity,
        Infinity,
        Infinity,
        Infinity,
        Infinity,
        Infinity,
        Infinity,
        Infinity,
        Infinity,
        Infinity,
        Infinity,
        Infinity,
        Infinity,
        Infinity,
        Infinity,
        Infinity,
        Infinity,
        Infinity,
        Infinity,
        Infinity,
        Infinity,
        Infinity,
        Infinity,
        Infinity,
        Infinity,
        Infinity,
        Infinity,
        Infinity,
        Infinity,
        Infinity,
        Infinity,
        Infinity,
        Infinity,
        Infinity,
        Infinity,
        Infinity,
        Infinity,
        Infinity,
        Infinity,
        Infinity,
        Infinity,
        Infinity,
        Infinity,
        Infinity,
        Infinity,
        Infinity,
        Infinity,
        Infinity,
        Infinity,
        Infinity,
        Infinity,
        Infinity,
        Infinity,
        Infinity,
        Infinity
      ]
    ],
    "ans2": [
      [
        0.46971677911297127,
        0.5024575880510863,
        0.49811793453090364
      ],
      [
        [
          0.05967308537130596,
          0.01128113445597256,
          0.0060819842764353715
        ],
        [
          0.01128113445597256,
          0.06669392744724824,
          0.01056576352441129
        ],
        [
          0.0060819842764353715,
          0.01056576352441129,
          0.09381093337548745
        ]
      ],
      [
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false
      ],
      [
        Infinity,
        Infinity,
        Infinity,
        Infinity,
        Infinity,
        Infinity,
        Infinity,
        Infinity,
        Infinity,
        Infinity,
        Infinity,
        Infinity,
        Infinity,
        Infinity,
        Infinity,
        Infinity,
        Infinity,
        Infinity,
        Infinity,
        Infinity,
        Infinity,
        Infinity,
        Infinity,
        Infinity,
        Infinity,
        Infinity,
        Infinity,
        Infinity,
        Infinity,
        Infinity,
        Infinity,
        Infinity,
        Infinity,
        Infinity,
        Infinity,
        Infinity,
        Infinity,
        Infinity,
        Infinity,
        Infinity,
        Infinity,
        Infinity,
        Infinity,
        Infinity,
        Infinity,
        Infinity,
        Infinity,
        Infinity,
        Infinity,
        Infinity
      ]
    ],
    "ans3": [
      [
        0.41772864999173126,
        0.49345543639236,
        0.511283464794371,
        0.48916103925917437,
        0.5133959317219446,
        0.5188389052960662,
        0.48792553861361415,
        0.5070154700880951,
        0.5024335669033653,
        0.49906478513625663
      ],
      [
        [
          0.07593357068062632,
          -0.004016890549743479,
          0.014471444224664613,
          0.011190524896690427,
          -0.0027991012999554568,
          -0.003817527982092763,
          -0.0005291856036768808,
          -0.0004476632760773481,
          0.004973795069815211,
          -0.0016396100804253583
        ],
        [
          -0.004016890549743479,
          0.07773147645409485,
          -0.005812751808029529,
          -0.002962419722289201,
          -0.00036787018168920737,
          -0.00400372509151903,
          0.010141388176985703,
          0.008020510234143682,
          0.005646466931431443,
          -0.010882124781120278
        ],
        [
          0.014471444224664613,
          -0.005812751808029529,
          0.09488148156381193,
          0.015085483419355627,
          -0.0015434857736485502,
          -0.012739116866751055,
          0.0027895651020034464,
          -0.007716196116087867,
          -0.003024771623571334,
          -0.00462125363262784
        ],
        [
          0.011190524896690427,
          -0.002962419722289201,
          0.015085483419355627,
          0.07469072751958099,
          -0.0015353505938436776,
          0.00025446881463621844,
          -0.0029307107786478576,
          0.002326992511191622,
          0.002274280202381665,
          -0.001512359086025741
        ],
        [
          -0.0027991012999554568,
          -0.00036787018168920737,
          -0.0015434857736485502,
          -0.0015353505938436776,
          0.09318892883439311,
          0.003591360771074363,
          0.002767869144191093,
          0.0010948922422577589,
          -0.006033188100104525,
          -0.0003734367707405589
        ],
        [
          -0.003817527982092763,
          -0.00400372509151903,
          -0.012739116866751055,
          0.00025446881463621844,
          0.003591360771074363,
          0.07504177571745409,
          -0.011585973883235142,
          0.0005093106642857658,
          -0.013210755876168698,
          0.0012137064464289979
        ],
        [
          -0.0005291856036768808,
          0.010141388176985703,
          0.0027895651020034464,
          -0.0029307107786478576,
          0.002767869144191093,
          -0.011585973883235142,
          0.07949620592314122,
          -0.000903955690156272,
          -0.002918527169761203,
          -0.004317557949719935
        ],
        [
          -0.0004476632760773481,
          0.008020510234143682,
          -0.007716196116087867,
          0.002326992511191622,
          0.0010948922422577589,
          0.0005093106642857658,
          -0.000903955690156272,
          0.07619786318451176,
          0.011009669777893537,
          0.001900441750806577
        ],
        [
          0.004973795069815211,
          0.005646466931431443,
          -0.003024771623571334,
          0.002274280202381665,
          -0.006033188100104525,
          -0.013210755876168698,
          -0.002918527169761203,
          0.011009669777893537,
          0.08202662322360373,
          0.003808735005228616
        ],
        [
          -0.0016396100804253583,
          -0.010882124781120278,
          -0.00462125363262784,
          -0.001512359086025741,
          -0.0003734367707405589,
          0.0012137064464289979,
          -0.004317557949719935,
          0.001900441750806577,
          0.003808735005228616,
          0.0773906070306047
        ]
      ],
      [
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false
      ],
      [
        Infinity,
        Infinity,
        Infinity,
        Infinity,
        Infinity,
        Infinity,
        Infinity,
        Infinity,
        Infinity,
        Infinity,
        Infinity,
        Infinity,
        Infinity,
        Infinity,
        Infinity,
        Infinity,
        Infinity,
        Infinity,
        Infinity,
        Infinity,
        Infinity,
        Infinity,
        Infinity,
        Infinity,
        Infinity,
        Infinity,
        Infinity,
        Infinity,
        Infinity,
        Infinity,
        Infinity,
        Infinity,
        Infinity,
        Infinity,
        Infinity,
        Infinity,
        Infinity,
        Infinity,
        Infinity,
        Infinity,
        Infinity,
        Infinity,
        Infinity,
        Infinity,
        Infinity,
        Infinity,
        Infinity,
        Infinity,
        Infinity,
        Infinity,
        Infinity,
        Infinity,
        Infinity,
        Infinity,
        Infinity,
        Infinity,
        Infinity,
        Infinity,
        Infinity,
        Infinity,
        Infinity,
        Infinity,
        Infinity,
        Infinity,
        Infinity,
        Infinity,
        Infinity,
        Infinity,
        Infinity,
        Infinity,
        Infinity,
        Infinity,
        Infinity,
        Infinity,
        Infinity,
        Infinity,
        Infinity,
        Infinity,
        Infinity,
        Infinity,
        Infinity,
        Infinity,
        Infinity,
        Infinity,
        Infinity,
        Infinity,
        Infinity,
        Infinity,
        Infinity,
        Infinity,
        Infinity,
        Infinity,
        Infinity,
        Infinity,
        Infinity,
        Infinity,
        Infinity,
        Infinity,
        Infinity,
        Infinity,
        Infinity,
        Infinity,
        Infinity,
        Infinity,
        Infinity,
        Infinity,
        Infinity,
        Infinity,
        Infinity,
        Infinity,
        Infinity,
        Infinity,
        Infinity,
        Infinity,
        Infinity,
        Infinity,
        Infinity,
        Infinity,
        Infinity,
        Infinity,
        Infinity,
        Infinity,
        Infinity,
        Infinity,
        Infinity,
        Infinity,
        Infinity,
        Infinity,
        Infinity,
        Infinity,
        Infinity,
        Infinity,
        Infinity,
        Infinity,
        Infinity,
        Infinity,
        Infinity,
        Infinity,
        Infinity,
        Infinity,
        Infinity,
        Infinity,
        Infinity,
        Infinity,
        Infinity,
        Infinity,
        Infinity,
        Infinity,
        Infinity,
        Infinity,
        Infinity,
        Infinity,
        Infinity,
        Infinity,
        Infinity,
        Infinity,
        Infinity,
        Infinity,
        Infinity,
        Infinity,
        Infinity,
        Infinity,
        Infinity,
        Infinity,
        Infinity,
        Infinity,
        Infinity,
        Infinity,
        Infinity,
        Infinity,
        Infinity,
        Infinity,
        Infinity,
        Infinity,
        Infinity,
        Infinity,
        Infinity,
        Infinity,
        Infinity,
        Infinity,
        Infinity,
        Infinity,
        Infinity,
        Infinity,
        Infinity,
        Infinity,
        Infinity,
        Infinity,
        Infinity,
        Infinity,
        Infinity,
        Infinity,
        Infinity,
        Infinity,
        Infinity,
        Infinity,
        Infinity,
        Infinity,
        Infinity,
        Infinity
      ]
    ],
    "ans4": [
      [
        0.5169117666156251,
        0.5199611305039505,
        0.49845721926630376,
        0.4896752921318155,
        0.5180179280875852,
        0.4826803555999155,
        0.4517724331899038,
        0.556530996948341
      ],
      [
        [
          0.08024848126833986,
          0.0004643466868412808,
          -0.0025979038408494846,
          0.005734932399957672,
          -0.0014798608687746947,
          -0.008882457809731847,
          -0.0013173618359335678,
          0.010570063523647096
        ],
        [
          0.0004643466868412808,
          0.08775259119547499,
          0.007819425988298042,
          -0.00604861029986201,
          0.007297865369787296,
          -0.0034567754680529888,
          0.011136264817610769,
          -0.008325199274069772
        ],
        [
          -0.0025979038408494846,
          0.007819425988298042,
          0.08249658347663148,
          -0.006555812978822435,
          -0.0016226657834994866,
          0.008271008531140055,
          0.005971439241958916,
          -0.00192742028325917
        ],
        [
          0.005734932399957672,
          -0.00604861029986201,
          -0.006555812978822435,
          0.08738236305987278,
          -0.0014577592345354894,
          0.0038467499227587336,
          0.003669682521700435,
          0.002323550150373011
        ],
        [
          -0.0014798608687746947,
          0.007297865369787296,
          -0.0016226657834994866,
          -0.0014577592345354894,
          0.09147081394170375,
          -0.002749199810428114,
          0.008370850617591952,
          -0.0007264782409604512
        ],
        [
          -0.008882457809731847,
          -0.0034567754680529888,
          0.008271008531140055,
          0.0038467499227587336,
          -0.002749199810428114,
          0.07482939826956073,
          0.0038438898312029277,
          0.005473998537827248
        ],
        [
          -0.0013173618359335678,
          0.011136264817610769,
          0.005971439241958916,
          0.003669682521700435,
          0.008370850617591952,
          0.0038438898312029277,
          0.07634305608065849,
          0.0029053755093543343
        ],
        [
          0.010570063523647096,
          -0.008325199274069772,
          -0.00192742028325917,
          0.002323550150373011,
          -0.0007264782409604512,
          0.005473998537827248,
          0.0029053755093543343,
          0.08065292216252795
        ]
      ],
      [
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false
      ],
      [
        Infinity,
        Infinity,
        Infinity,
        Infinity,
        Infinity,
        Infinity,
        Infinity,
        Infinity,
        Infinity,
        Infinity,
        Infinity,
        Infinity,
        Infinity,
        Infinity,
        Infinity,
        Infinity,
        Infinity,
        Infinity,
        Infinity,
        Infinity,
        Infinity,
        Infinity,
        Infinity,
        Infinity,
        Infinity,
        Infinity,
        Infinity,
        Infinity,
        Infinity,
        Infinity,
        Infinity,
        Infinity,
        Infinity,
        Infinity,
        Infinity,
        Infinity,
        Infinity,
        Infinity,
        Infinity,
        Infinity,
        Infinity,
        Infinity,
        Infinity,
        Infinity,
        Infinity,
        Infinity,
        Infinity,
        Infinity,
        Infinity,
        Infinity,
        Infinity,
        Infinity,
        Infinity,
        Infinity,
        Infinity,
        Infinity,
        Infinity,
        Infinity,
        Infinity,
        Infinity,
        Infinity,
        Infinity,
        Infinity,
        Infinity,
        Infinity,
        Infinity,
        Infinity,
        Infinity,
        Infinity,
        Infinity,
        Infinity,
        Infinity,
        Infinity,
        Infinity,
        Infinity,
        Infinity,
        Infinity,
        Infinity,
        Infinity,
        Infinity,
        Infinity,
        Infinity,
        Infinity,
        Infinity,
        Infinity,
        Infinity,
        Infinity,
        Infinity,
        Infinity,
        Infinity,
        Infinity,
        Infinity,
        Infinity,
        Infinity,
        Infinity,
        Infinity,
        Infinity,
        Infinity,
        Infinity,
        Infinity,
        Infinity,
        Infinity,
        Infinity,
        Infinity,
        Infinity,
        Infinity,
        Infinity,
        Infinity,
        Infinity,
        Infinity,
        Infinity,
        Infinity,
        Infinity,
        Infinity,
        Infinity,
        Infinity,
        Infinity,
        Infinity,
        Infinity,
        Infinity,
        Infinity,
        Infinity,
        Infinity,
        Infinity,
        Infinity,
        Infinity,
        Infinity,
        Infinity,
        Infinity,
        Infinity,
        Infinity,
        Infinity,
        Infinity,
        Infinity,
        Infinity,
        Infinity,
        Infinity,
        Infinity,
        Infinity,
        Infinity,
        Infinity,
        Infinity,
        Infinity,
        Infinity,
        Infinity,
        Infinity,
        Infinity,
        Infinity,
        Infinity,
        Infinity,
        Infinity,
        Infinity,
        Infinity,
        Infinity,
        Infinity,
        Infinity,
        Infinity,
        Infinity,
        Infinity,
        Infinity,
        Infinity,
        Infinity,
        Infinity,
        Infinity,
        Infinity,
        Infinity,
        Infinity,
        Infinity,
        Infinity,
        Infinity,
        Infinity,
        Infinity,
        Infinity,
        Infinity,
        Infinity,
        Infinity,
        Infinity,
        Infinity,
        Infinity,
        Infinity,
        Infinity,
        Infinity,
        Infinity,
        Infinity,
        Infinity,
        Infinity,
        Infinity,
        Infinity,
        Infinity,
        Infinity,
        Infinity,
        Infinity,
        Infinity,
        Infinity,
        Infinity,
        Infinity,
        Infinity,
        Infinity,
        Infinity,
        Infinity,
        Infinity,
        Infinity,
        Infinity,
        Infinity,
        Infinity,
        Infinity,
        Infinity,
        Infinity,
        Infinity,
        Infinity,
        Infinity,
        Infinity,
        Infinity,
        Infinity,
        Infinity,
        Infinity,
        Infinity,
        Infinity,
        Infinity,
        Infinity,
        Infinity,
        Infinity,
        Infinity,
        Infinity,
        Infinity,
        Infinity,
        Infinity,
        Infinity,
        Infinity,
        Infinity,
        Infinity,
        Infinity,
        Infinity,
        Infinity,
        Infinity,
        Infinity,
        Infinity,
        Infinity,
        Infinity,
        Infinity,
        Infinity,
        Infinity,
        Infinity,
        Infinity,
        Infinity,
        Infinity,
        Infinity,
        Infinity,
        Infinity,
        Infinity,
        Infinity,
        Infinity,
        Infinity,
        Infinity,
        Infinity,
        Infinity,
        Infinity,
        Infinity,
        Infinity,
        Infinity,
        Infinity,
        Infinity,
        Infinity,
        Infinity,
        Infinity,
        Infinity,
        Infinity,
        Infinity,
        Infinity,
        Infinity,
        Infinity,
        Infinity,
        Infinity,
        Infinity,
        Infinity,
        Infinity,
        Infinity,
        Infinity,
        Infinity,
        Infinity,
        Infinity,
        Infinity,
        Infinity,
        Infinity,
        Infinity,
        Infinity,
        Infinity,
        Infinity,
        Infinity,
        Infinity,
        Infinity,
        Infinity,
        Infinity,
        Infinity,
        Infinity,
        Infinity,
        Infinity,
        Infinity,
        Infinity,
        Infinity
      ]
    ],
    "ans5": [
      [
        0.4614705319360429,
        0.5226750502692488,
        0.47644586436991115,
        0.5490082861082191
      ],
      [
        [
          0.08863914612112532,
          0.0012362800615410765,
          -0.0020934054612016663,
          -0.002185779996323873
        ],
        [
          0.0012362800615410765,
          0.07935969384436685,
          -0.0010729344724866303,
          0.0067175374439095185
        ],
        [
          -0.0020934054612016663,
          -0.0010729344724866303,
          0.078499121031789,
          -0.005649911645858106
        ],
        [
          -0.002185779996323873,
          0.0067175374439095185,
          -0.005649911645858106,
          0.06999893382592332
        ]
      ],
      [
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false
      ],
      [
        Infinity,
        Infinity,
        Infinity,
        Infinity,
        Infinity,
        Infinity,
        Infinity,
        Infinity,
        Infinity,
        Infinity,
        Infinity,
        Infinity,
        Infinity,
        Infinity,
        Infinity,
        Infinity,
        Infinity,
        Infinity,
        Infinity,
        Infinity,
        Infinity,
        Infinity,
        Infinity,
        Infinity,
        Infinity,
        Infinity,
        Infinity,
        Infinity,
        Infinity,
        Infinity,
        Infinity,
        Infinity,
        Infinity,
        Infinity,
        Infinity,
        Infinity,
        Infinity,
        Infinity,
        Infinity,
        Infinity,
        Infinity,
        Infinity,
        Infinity,
        Infinity,
        Infinity,
        Infinity,
        Infinity,
        Infinity,
        Infinity,
        Infinity,
        Infinity,
        Infinity,
        Infinity,
        Infinity,
        Infinity,
        Infinity,
        Infinity,
        Infinity,
        Infinity,
        Infinity,
        Infinity,
        Infinity,
        Infinity,
        Infinity,
        Infinity,
        Infinity,
        Infinity,
        Infinity,
        Infinity,
        Infinity,
        Infinity,
        Infinity,
        Infinity,
        Infinity,
        Infinity,
        Infinity,
        Infinity,
        Infinity,
        Infinity,
        Infinity,
        Infinity,
        Infinity,
        Infinity,
        Infinity,
        Infinity,
        Infinity,
        Infinity,
        Infinity,
        Infinity,
        Infinity,
        Infinity,
        Infinity,
        Infinity,
        Infinity,
        Infinity,
        Infinity,
        Infinity,
        Infinity,
        Infinity,
        Infinity,
        Infinity,
        Infinity,
        Infinity,
        Infinity,
        Infinity,
        Infinity,
        Infinity,
        Infinity,
        Infinity,
        Infinity,
        Infinity,
        Infinity,
        Infinity,
        Infinity,
        Infinity,
        Infinity,
        Infinity,
        Infinity,
        Infinity,
        Infinity,
        Infinity,
        Infinity,
        Infinity,
        Infinity,
        Infinity,
        Infinity,
        Infinity,
        Infinity,
        Infinity,
        Infinity,
        Infinity,
        Infinity,
        Infinity,
        Infinity,
        Infinity,
        Infinity,
        Infinity,
        Infinity,
        Infinity,
        Infinity,
        Infinity,
        Infinity,
        Infinity,
        Infinity,
        Infinity,
        Infinity,
        Infinity,
        Infinity,
        Infinity,
        Infinity
      ]
    ]
  }
}