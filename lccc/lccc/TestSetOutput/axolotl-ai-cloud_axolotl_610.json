{
  "input_header": "def barrier():",
  "input_docstring": "Acts as a barrier to wait for all processes. This ensures that all processes\nreach the barrier before proceeding further.",
  "output_code": "def barrier():\n    \n    if is_distributed():\n        dist.barrier()",
  "input_contexts": [
    {
      "id": "axolotl-ai-cloud_axolotl_610_4",
      "input_code": "def compute_and_broadcast(fn):\n    \n    cur_device = f\"{get_device_type()}:{get_current_device()}\"\n    if is_main_process():\n        value_scalar = fn()\n        value_tensor = torch.tensor(\n            value_scalar, device=cur_device, dtype=torch.float32\n        )\n    else:\n        value_tensor = torch.tensor(\n            0.0, device=cur_device, dtype=torch.float32\n        )\n\n    barrier()\n    dist.broadcast(value_tensor, src=0)\n\n    if value_tensor == value_tensor.int():\n        return int(value_tensor.item())\n    return float(value_tensor.item())\n"
    },
    {
      "id": "axolotl-ai-cloud_axolotl_610_2",
      "input_code": "def bench_eval_callback_factory(trainer, tokenizer):\n    accuracy = evaluate.load(\"accuracy\")\n    abcd_idx = [\n        tokenizer(\"A\", add_special_tokens=False).input_ids[0],\n        tokenizer(\"B\", add_special_tokens=False).input_ids[0],\n        tokenizer(\"C\", add_special_tokens=False).input_ids[0],\n        tokenizer(\"D\", add_special_tokens=False).input_ids[0],\n        tokenizer(\"E\", add_special_tokens=False).input_ids[0],\n        tokenizer(\"F\", add_special_tokens=False).input_ids[0],\n        tokenizer(\"G\", add_special_tokens=False).input_ids[0],\n    ]\n    bench_split = \"eval\"\n\n    def transform_bench_subject(example):\n        parts = example[\"subject\"].split(\":\")\n        first_part = (\n            parts[0].strip().lower().replace(\"-\", \"_\")\n        )\n        second_part = (\n            parts[1].strip().replace(\"-\", \"_\") if len(parts) > 1 else \"all\"\n        )\n\n        return {\"name\": first_part, \"subject\": second_part}\n\n    if trainer.args.bench_dataset == \"mmlu-zs\":\n        bench_dataset = load_dataset(\n            \"openaccess-ai-collective/mmlu-evals\",\n            data_files={\n                \"eval\": \"zero_shot_mmlu_val.json\",\n                \"test\": \"zero_shot_mmlu_test.json\",\n            },\n        )\n    elif trainer.args.bench_dataset in [\"mmlu\", \"mmlu-fs\"]:\n        bench_dataset = load_dataset(\n            \"openaccess-ai-collective/mmlu-evals\",\n            data_files={\n                \"eval\": \"five_shot_mmlu_val.json\",\n                \"test\": \"five_shot_mmlu_test.json\",\n            },\n        )\n    elif \"/\" in trainer.args.bench_dataset:\n        bench_ds = trainer.args.bench_dataset\n        bench_ds_name = \"/\".join(bench_ds.split(\"/\", 2)[:2])\n        bench_ds_data_file = \"/\".join(bench_ds.split(\"/\", 2)[2:])\n        bench_dataset = load_dataset(\n            bench_ds_name,\n            data_files={\n                \"eval\": bench_ds_data_file,\n            },\n        )\n        bench_dataset[\"eval\"] = bench_dataset[\"eval\"].map(transform_bench_subject)\n    else:\n        raise ValueError(\n            f\"unhandled value `{trainer.args.bench_dataset}` for bench_dataset training args\"\n        )\n    bench_dataset = bench_dataset[trainer.args.bench_split]\n    if trainer.args.max_bench_samples is not None:\n        bench_dataset = bench_dataset.select(range(trainer.args.max_bench_samples))\n\n    def tokenize_evals(example):\n        source = f\"{tokenizer.bos_token}{example['input']}\"\n        target = f\"{example['output']}{tokenizer.eos_token}\"\n\n        tokenized_source = tokenizer(\n            source,\n            max_length=2048,\n            truncation=True,\n            add_special_tokens=False,\n        )\n        tokenized_target = tokenizer(\n            target,\n            max_length=2048,\n            truncation=True,\n            add_special_tokens=False,\n        )\n        input_ids = tokenized_source[\"input_ids\"] + tokenized_target[\"input_ids\"]\n        labels = [IGNORE_INDEX] * len(tokenized_source[\"input_ids\"]) + tokenized_target[\n            \"input_ids\"\n        ]\n\n        return {\n            \"input_ids\": input_ids,\n            \"labels\": labels,\n            \"subject\": example[\"subject\"],\n        }\n\n    with zero_first(is_main_process()):\n        bench_dataset = bench_dataset.map(tokenize_evals)\n        bench_dataset = bench_dataset.filter(lambda x: x[\"labels\"][-2] in abcd_idx)\n\n    class BenchEvalCallback(TrainerCallback):\n        \n\n        def on_evaluate(\n            self,\n            args: AxolotlTrainingArguments,\n            state: TrainerState,            control: TrainerControl,            metrics: Dict[str, float],            **kwargs,        ):\n            data_loader = trainer.get_bench_dataloader(\n                bench_dataset.remove_columns([\"input\", \"subject\", \"output\", \"name\"])\n            )\n            trainer.model.eval()\n            preds, refs = [], []\n            loss_bench = 0\n            for batch in tqdm(data_loader, total=len(data_loader)):\n                (loss, logits, labels) = trainer.prediction_step(\n                    trainer.model,\n                    batch,\n                    prediction_loss_only=False,\n                )\n                for i, logit in enumerate(logits):\n                    label_non_zero_id = (batch[\"labels\"][i] != IGNORE_INDEX).nonzero()[\n                        0\n                    ][0]\n                    logit_abcd = logit[label_non_zero_id - 1][abcd_idx]\n                    preds.append(torch.argmax(logit_abcd).item())\n                labels = labels[labels != IGNORE_INDEX].view(-1, 2)[:, 0]\n                refs += [\n                    abcd_idx.index(label) if label in abcd_idx else -1\n                    for label in labels.tolist()\n                ]\n                loss_bench += loss.item()\n            bench_name = bench_dataset[\"name\"]\n            bench_names: dict = {s: {\"refs\": [], \"preds\": []} for s in set(bench_name)}\n            for s, p, r in zip(bench_name, preds, refs):\n                bench_names[s][\"preds\"].append(p)\n                bench_names[s][\"refs\"].append(r)\n            barrier()\n            local_bench_names = bench_names\n            gathered_bench_names: List[Dict] = [{} for _ in range(get_world_size())]\n\n            loss_bench_ranks = gather_scalar_from_all_ranks(\n                lambda: loss_bench, get_world_size()\n            )\n            len_data_loader_ranks = gather_scalar_from_all_ranks(\n                lambda: len(data_loader), get_world_size()\n            )\n\n            results = {}\n            if is_distributed() and not is_main_process():\n                dist.gather_object(local_bench_names, dst=0)\n            else:\n                if is_distributed():\n                    dist.gather_object(local_bench_names, gathered_bench_names, dst=0)\n                else:\n                    gathered_bench_names = [local_bench_names]\n                bench_loss = sum(loss_bench_ranks) / sum(len_data_loader_ranks)\n                results = {f\"{bench_split}_bench_loss\": bench_loss}\n\n                combined_bench_names: Dict[str, Dict[str, List]] = {}\n                for bench_name in gathered_bench_names:\n                    for name, data in bench_name.items():\n                        if name not in combined_bench_names:\n                            combined_bench_names[name] = {\"refs\": [], \"preds\": []}\n                        combined_bench_names[name][\"refs\"].extend(data[\"refs\"])\n                        combined_bench_names[name][\"preds\"].extend(data[\"preds\"])\n\n                bench_scores = []\n                bench_refs = []\n                bench_preds = []\n                for (\n                    bench_name\n                ) in combined_bench_names:\n                    bench_score = accuracy.compute(\n                        references=combined_bench_names[bench_name][\"refs\"],\n                        predictions=combined_bench_names[bench_name][\"preds\"],\n                    )[\"accuracy\"]\n                    bench_refs.extend(combined_bench_names[bench_name][\"refs\"])\n                    bench_preds.extend(combined_bench_names[bench_name][\"preds\"])\n                    if not pd.isna(bench_score):\n                        results[\n                            f\"{bench_split}_bench_accuracy_{bench_name}\"\n                        ] = bench_score\n                        bench_scores.append(bench_score)\n                    else:\n                        results[f\"{bench_split}_bench_accuracy_{bench_name}\"] = 0.0\n                        bench_scores.append(0.0)\n                results[f\"{bench_split}_bench_average_accuracy\"] = np.mean(bench_scores)\n                results[f\"{bench_split}_bench_total_accuracy\"] = accuracy.compute(\n                    references=bench_refs, predictions=bench_preds\n                )[\"accuracy\"]\n                trainer.log(results)\n\n            results = broadcast_dict(results)\n            for key, val in results.items():\n                metrics[key] = val\n\n    return BenchEvalCallback\n"
    },
    {
      "id": "axolotl-ai-cloud_axolotl_610_3",
      "input_code": "def zero_first(is_main):\n    \n    if not is_main:\n        barrier()\n    yield\n    if is_main:\n        barrier()\n"
    },
    {
      "id": "axolotl-ai-cloud_axolotl_610_1",
      "input_code": "def merge_and_save(\n    model: peft.LoraModel,\n    model_src: str,\n    model_dst: str,\n    reinit: bool = False,\n    quantized: bool = False,\n    cpu_offload: bool = False,\n    actually_save: bool = True,\n):\n    modules = find_lora_modules(model)\n\n    if not quantized:\n        for module_name, target in modules.items():\n            active_adapter = target.active_adapter\n            if isinstance(active_adapter, list):\n                active_adapter = active_adapter[0]\n            update = target.get_delta_weight(active_adapter).detach()\n            target.weight.data += update\n\n            if reinit:\n                for adapter_name in target.lora_A:\n                    target.reset_lora_parameters(adapter_name, True)\n                for adapter_name in target.lora_embedding_A:\n                    target.reset_lora_parameters(adapter_name, True)\n        return\n\n    os.makedirs(model_dst, exist_ok=True)\n    shard_paths = sharded_paths(model_src, modules.keys())\n    out_shard_paths = {}\n\n    unique_shards = list(set(shard_paths.values()))\n    for shard_path in unique_shards:\n        out_tensors = {}\n        if shard_path.endswith(\".safetensors\"):\n            in_tensors = st.load_file(str(Path(model_src) / shard_path))\n        else:\n            in_tensors = torch.load(Path(model_src) / shard_path)\n            if \"state_dict\" in in_tensors:\n                in_tensors = in_tensors[\"state_dict\"]\n\n        for module_name, target in modules.items():\n            key = module_name + \".weight\"\n            if key not in shard_paths or shard_paths[key] != shard_path:\n                continue\n\n            orig_weight = in_tensors[key]\n            old_dev = target.weight.device\n            math_dev = \"cpu\" if cpu_offload else old_dev\n\n            delta_weight = lora_delta_weight(target, math_dev)\n            new_weight = orig_weight.to(math_dev) + delta_weight\n            del delta_weight\n\n            if actually_save:\n                out_tensors[key] = new_weight.half().cpu()\n\n            update_weights(target, new_weight, reinit=reinit, device=old_dev)\n\n        if actually_save:\n            out_shard_name = shard_path\n            if out_shard_name.startswith(\"pytorch_model\"):\n                out_shard_name = (\n                    out_shard_name.replace(\"pytorch_model\", \"model\").rstrip(\".bin\")\n                    + \".safetensors\"\n                )\n\n            for module_name in in_tensors:\n                if module_name not in out_tensors:\n                    out_tensors[module_name] = in_tensors[module_name].half()\n                out_shard_paths[module_name] = out_shard_name\n\n            shard_fn = str(Path(model_dst) / out_shard_name)\n            LOG.info(f\"saving tensors to {shard_fn}\")\n            st.save_file(out_tensors, shard_fn, metadata={\"format\": \"pt\"})\n\n        barrier()\n        del in_tensors\n        del out_tensors\n        torch.cuda.empty_cache()\n\n    if actually_save and len(unique_shards) > 1:\n        with open(\n            str(Path(model_dst, \"model.safetensors.index.json\")), \"w\", encoding=\"utf-8\"\n        ) as file:\n            json.dump({\"metadata\": {}, \"weight_map\": out_shard_paths}, file)\n"
    }
  ],
  "generated_inputs": [
    "<needimport>\nimport torch\nimport torch.distributed as dist\nimport os\n\n<testcase_1>\nos.environ['WORLD_SIZE'] = '4'\ndist.init_process_group(backend='nccl', rank=0, world_size=4)\nbarrier()\n\n<testcase_2>\nos.environ['WORLD_SIZE'] = '1'\ndist.init_process_group(backend='gloo', rank=0, world_size=1)\nbarrier()\n\n<testcase_3>\nos.environ['WORLD_SIZE'] = '2'\ndist.init_process_group(backend='nccl', rank=1, world_size=2)\nbarrier()\n\n<testcase_4>\nos.environ['WORLD_SIZE'] = '8'\ndist.init_process_group(backend='gloo', rank=3, world_size=8)\nbarrier()\n\n<testcase_5>\nos.environ['WORLD_SIZE'] = '16'\ndist.init_process_group(backend='nccl', rank=15, world_size=16)\nbarrier()"
  ],
  "gpt_output": "<needimport>\nimport torch\nimport torch.distributed as dist\nimport os\n\n<testcase_1>\nos.environ['WORLD_SIZE'] = '4'\ndist.init_process_group(backend='nccl', rank=0, world_size=4)\nbarrier()\n\n<testcase_2>\nos.environ['WORLD_SIZE'] = '1'\ndist.init_process_group(backend='gloo', rank=0, world_size=1)\nbarrier()\n\n<testcase_3>\nos.environ['WORLD_SIZE'] = '2'\ndist.init_process_group(backend='nccl', rank=1, world_size=2)\nbarrier()\n\n<testcase_4>\nos.environ['WORLD_SIZE'] = '8'\ndist.init_process_group(backend='gloo', rank=3, world_size=8)\nbarrier()\n\n<testcase_5>\nos.environ['WORLD_SIZE'] = '16'\ndist.init_process_group(backend='nccl', rank=15, world_size=16)\nbarrier()",
  "needimport": "import torch\nimport torch.distributed as dist\nimport os\n\n",
  "test_results": {
    "ans1": "barrier_failed",
    "ans2": "barrier_failed",
    "ans3": "barrier_failed",
    "ans4": "barrier_failed",
    "ans5": "barrier_failed"
  }
}