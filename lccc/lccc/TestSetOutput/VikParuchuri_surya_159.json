{
  "input_header": "def to(self, device_dtype: torch.device | str | None = None):",
  "input_docstring": null,
  "output_code": "    def to(self, device_dtype: torch.device | str | None = None):\n        if self.model:\n            self.model.to(device_dtype)\n        else:\n            raise ValueError(\"Model not loaded\")",
  "input_contexts": [
    {
      "id": "VikParuchuri_surya_159_5",
      "input_code": "    def forward(\n        self,\n        hidden_states: torch.Tensor,\n        input_dimensions: Tuple[int, int],\n        head_mask: Optional[torch.FloatTensor] = None,\n        output_attentions: Optional[bool] = False,\n        always_partition: Optional[bool] = False,\n    ) -> Tuple[torch.Tensor]:\n        height, width = input_dimensions\n\n        if self.positional_encoding is not None:\n            hidden_states = hidden_states + self.positional_encoding.to(hidden_states.dtype).to(hidden_states.device)\n\n        for i, layer_module in enumerate(self.blocks):\n            layer_head_mask = head_mask[i] if head_mask is not None else None\n\n            layer_outputs = layer_module(\n                hidden_states, input_dimensions, layer_head_mask, output_attentions, always_partition\n            )\n\n            hidden_states = layer_outputs[0]\n\n        hidden_states_before_downsampling = hidden_states\n        if self.downsample is not None:\n            height_downsampled, width_downsampled = (height + 1) // 2, (width + 1) // 2\n            output_dimensions = (height, width, height_downsampled, width_downsampled)\n            hidden_states = self.downsample(hidden_states_before_downsampling, input_dimensions)\n        else:\n            output_dimensions = (height, width, height, width)\n\n        stage_outputs = (hidden_states, hidden_states_before_downsampling, output_dimensions)\n\n        if output_attentions:\n            stage_outputs += layer_outputs[1:]\n        return stage_outputs\n"
    },
    {
      "id": "VikParuchuri_surya_159_10",
      "input_code": "    def batch_ocr_error_detection(\n            self,\n            texts: List[str],\n            batch_size: Optional[int] = None\n    ):\n        if batch_size is None:\n            batch_size = self.get_batch_size()\n\n        num_batches = math.ceil(len(texts) / batch_size)\n        texts_processed = self.processor(texts, padding='longest', truncation=True, return_tensors='pt')\n        predictions = []\n        for batch_idx in tqdm(range(num_batches)):\n            start_idx, end_idx = batch_idx * batch_size, (batch_idx + 1) * batch_size\n            batch_input_ids = texts_processed.input_ids[start_idx:end_idx].to(self.model.device)\n            batch_attention_mask = texts_processed.attention_mask[start_idx:end_idx].to(self.model.device)\n\n            with torch.inference_mode():\n                pred = self.model(batch_input_ids, attention_mask=batch_attention_mask)\n                logits = pred.logits.detach().cpu().numpy().astype(np.float32)\n                predictions.extend(np.argmax(logits, axis=1).tolist())\n\n        return OCRErrorDetectionResult(\n            texts=texts,\n            labels=[ID2LABEL[p] for p in predictions]\n        )\n"
    },
    {
      "id": "VikParuchuri_surya_159_9",
      "input_code": "    def forward(self, boxes: torch.LongTensor, input_box_counts: torch.LongTensor):\n        cx, cy, w, h, xskew, yskew, label = boxes.to(torch.long).unbind(dim=-1)\n\n        xskew_actual = ((xskew - self.config.bbox_size // 2) / 2).to(torch.long)\n        yskew_actual = ((yskew - self.config.bbox_size // 2) / 2).to(torch.long)\n\n        x1 = (cx - w // 2 - xskew_actual).clamp(0, self.config.bbox_size).to(torch.long)\n        y1 = (cy - h // 2 - yskew_actual).clamp(0, self.config.bbox_size).to(torch.long)\n        x2 = (cx + w // 2 - xskew_actual).clamp(0, self.config.bbox_size).to(torch.long)\n        y2 = (cy + h // 2 + yskew_actual).clamp(0, self.config.bbox_size).to(torch.long)\n        x3 = (cx + w // 2 + xskew_actual).clamp(0, self.config.bbox_size).to(torch.long)\n        y3 = (cy + h // 2 + yskew_actual).clamp(0, self.config.bbox_size).to(torch.long)\n        x4 = (cx - w // 2 + xskew_actual).clamp(0, self.config.bbox_size).to(torch.long)\n        y4 = (cy - h // 2 - yskew_actual).clamp(0, self.config.bbox_size).to(torch.long)\n\n        label_embeds = self.label_embed(label)\n        size_embeds = self.w_embed(w) + self.h_embed(h) + self.cx_embed(cx) + self.cy_embed(cy)\n        skew_embeds = self.xskew_embed(xskew) + self.yskew_embed(yskew)\n        corner_embeds = self.x1_embed(x1) + self.y1_embed(y1) + self.x2_embed(x2) + self.y2_embed(y2) + self.x3_embed(x3) + self.y3_embed(y3) + self.x4_embed(x4) + self.y4_embed(y4)\n        embedded = label_embeds + size_embeds + skew_embeds + corner_embeds\n\n        return embedded\n"
    },
    {
      "id": "VikParuchuri_surya_159_8",
      "input_code": "    def model(\n        self,\n        device=settings.TORCH_DEVICE_MODEL,\n        dtype=settings.MODEL_DTYPE\n    ) -> SuryaLayoutModel:\n        if device is None:\n            device = settings.TORCH_DEVICE_MODEL\n        if dtype is None:\n            dtype = settings.MODEL_DTYPE\n\n        config = SuryaLayoutConfig.from_pretrained(self.checkpoint, revision=self.revision)\n        decoder_config = config.decoder\n        decoder = SuryaLayoutDecoderConfig(**decoder_config)\n        config.decoder = decoder\n\n        encoder_config = config.encoder\n        encoder = DonutSwinLayoutConfig(**encoder_config)\n        config.encoder = encoder\n\n        model = SuryaLayoutModel.from_pretrained(self.checkpoint, config=config, torch_dtype=dtype, revision=self.revision)\n        model = model.to(device)\n        model = model.eval()\n\n        if settings.LAYOUT_STATIC_CACHE:\n            torch.set_float32_matmul_precision('high')\n            torch._dynamo.config.cache_size_limit = 16\n            torch._dynamo.config.suppress_errors = False\n\n            print(f\"Compiling layout model {self.checkpoint} on device {device} with dtype {dtype}\")\n            model.encoder = torch.compile(model.encoder)\n            model.decoder = torch.compile(model.decoder)\n\n        print(f\"Loaded layout model {self.checkpoint} on device {device} with dtype {dtype}\")\n        return model\n"
    },
    {
      "id": "VikParuchuri_surya_159_14",
      "input_code": "    def model(\n        self,\n        device=settings.TORCH_DEVICE_MODEL,\n        dtype=settings.MODEL_DTYPE\n    ) -> OCREncoderDecoderModel:\n        if device is None:\n            device = settings.TORCH_DEVICE_MODEL\n        if dtype is None:\n            dtype = settings.MODEL_DTYPE\n\n        config = SuryaOCRConfig.from_pretrained(self.checkpoint, revision=self.revision)\n        decoder_config = config.decoder\n        decoder = SuryaOCRDecoderConfig(**decoder_config)\n        config.decoder = decoder\n\n        encoder_config = config.encoder\n        encoder = DonutSwinConfig(**encoder_config)\n        config.encoder = encoder\n\n        text_encoder_config = config.text_encoder\n        text_encoder = SuryaOCRTextEncoderConfig(**text_encoder_config)\n        config.text_encoder = text_encoder\n\n        model = OCREncoderDecoderModel.from_pretrained(self.checkpoint, config=config, torch_dtype=dtype, revision=self.revision)\n        model = model.to(device)\n        model = model.eval()\n\n        if settings.RECOGNITION_STATIC_CACHE:\n            torch.set_float32_matmul_precision('high')\n            torch._dynamo.config.cache_size_limit = 16\n            torch._dynamo.config.suppress_errors = False\n\n            print(f\"Compiling recognition model {self.checkpoint} on device {device} with dtype {dtype}\")\n            model.encoder = torch.compile(model.encoder)\n            model.decoder = torch.compile(model.decoder)\n            model.text_encoder = torch.compile(model.text_encoder)\n\n        print(f\"Loaded recognition model {self.checkpoint} on device {device} with dtype {dtype}\")\n        return model\n"
    },
    {
      "id": "VikParuchuri_surya_159_4",
      "input_code": "    def _update_static_cache(self, key_states, value_states, **cache_kwargs):\n        cache_position = cache_kwargs.get(\"cache_position\")\n        k_out, v_out = self.key_states.to(key_states.device), self.value_states.to(value_states.device)\n\n        k_out[:, :, cache_position] = key_states.to(k_out.dtype)\n        v_out[:, :, cache_position] = value_states.to(v_out.dtype)\n\n        self.key_states, self.value_states = k_out, v_out\n        return k_out, v_out\n"
    },
    {
      "id": "VikParuchuri_surya_159_2",
      "input_code": "    def batch_table_recognition(\n            self,\n            images: List,\n            batch_size=None) -> List[TableResult]:\n        assert all([isinstance(image, Image.Image) for image in images])\n        if batch_size is None:\n            batch_size = self.get_batch_size()\n\n        if len(images) == 0:\n            return []\n\n        query_items = []\n        for image in images:\n            query_items.append({\n                \"polygon\": [[0, 0], [image.width, 0], [image.width, image.height], [0, image.height]],\n                \"category\": CATEGORY_TO_ID[\"Table\"],\n                \"colspan\": 0,\n                \"merges\": 0,\n                \"is_header\": 0\n            })\n\n        output_order = []\n        for i in tqdm(range(0, len(images), batch_size), desc=\"Recognizing tables\"):\n            batch_query_items = query_items[i:i + batch_size]\n\n            batch_images = images[i:i + batch_size]\n            batch_images = [image.convert(\"RGB\") for image in batch_images]\n\n            current_batch_size = len(batch_images)\n\n            orig_sizes = [image.size for image in batch_images]\n            model_inputs = self.processor(images=batch_images, query_items=batch_query_items)\n\n            batch_pixel_values = model_inputs[\"pixel_values\"]\n\n            batch_input_ids = model_inputs[\"input_ids\"].to(self.model.device)\n            batch_pixel_values = torch.tensor(np.array(batch_pixel_values), dtype=self.model.dtype).to(self.model.device)\n\n            shaper = LabelShaper()\n\n            with torch.inference_mode():\n                encoder_hidden_states = self.model.encoder(pixel_values=batch_pixel_values).last_hidden_state\n\n            rowcol_predictions = self.inference_loop(\n                encoder_hidden_states,\n                batch_input_ids,\n                current_batch_size,\n                batch_size\n            )\n\n            row_query_items = []\n            row_encoder_hidden_states = []\n            idx_map = []\n            columns = []\n            for j, img_predictions in enumerate(rowcol_predictions):\n                for row_prediction in img_predictions:\n                    polygon = shaper.convert_bbox_to_polygon(row_prediction[\"bbox\"])\n                    if row_prediction[\"category\"] == CATEGORY_TO_ID[\"Table-row\"]:\n                        row_query_items.append({\n                            \"polygon\": polygon,\n                            \"category\": row_prediction[\"category\"],\n                            \"colspan\": 0,\n                            \"merges\": 0,\n                            \"is_header\": int(row_prediction[\"is_header\"] == 1)\n                        })\n                        row_encoder_hidden_states.append(encoder_hidden_states[j])\n                        idx_map.append(j)\n                    elif row_prediction[\"category\"] == CATEGORY_TO_ID[\"Table-column\"]:\n                        columns.append({\n                            \"polygon\": polygon,\n                            \"category\": row_prediction[\"category\"],\n                            \"colspan\": 0,\n                            \"merges\": 0,\n                            \"is_header\": int(row_prediction[\"is_header\"] == 1)\n                        })\n\n            row_encoder_hidden_states = torch.stack(row_encoder_hidden_states)\n            row_inputs = self.processor(images=None, query_items=row_query_items, columns=columns, convert_images=False)\n            row_input_ids = row_inputs[\"input_ids\"].to(self.model.device)\n            cell_predictions = []\n            for j in range(0, len(row_input_ids), batch_size):\n                cell_batch_hidden_states = row_encoder_hidden_states[j:j + batch_size]\n                cell_batch_input_ids = row_input_ids[j:j + batch_size]\n                cell_batch_size = len(cell_batch_input_ids)\n                cell_predictions.extend(\n                    self.inference_loop(cell_batch_hidden_states, cell_batch_input_ids, cell_batch_size, batch_size)\n                )\n\n            result = self.decode_batch_predictions(rowcol_predictions, cell_predictions, orig_sizes, idx_map, shaper)\n            output_order.extend(result)\n\n        return output_order\n"
    },
    {
      "id": "VikParuchuri_surya_159_12",
      "input_code": "    def forward(\n        self,\n        query: torch.Tensor,\n        key: torch.Tensor,\n        value: torch.Tensor,\n        mask: torch.Tensor,\n        head_mask: Optional[torch.Tensor] = None,\n        output_attentions: bool = False,\n    ) -> Tuple[torch.Tensor, ...]:\n        \n        batch_size, q_length, dim = query.size()\n\n        dim_per_head = self.dim // self.n_heads\n\n        def reshape(x: torch.Tensor) -> torch.Tensor:\n            \n            return x.view(batch_size, -1, self.n_heads, dim_per_head)\n\n        query_states = reshape(self.q_lin(query))\n        key_states = reshape(self.k_lin(key))\n        value_states = reshape(self.v_lin(value))\n\n        attn_dropout = self.config.attention_dropout if self.training else 0.0\n\n\n        if query_states.dtype == torch.float32:\n            if torch.is_autocast_enabled():\n                target_dtype = torch.get_autocast_gpu_dtype()\n            elif hasattr(self.config, \"_pre_quantization_dtype\"):\n                target_dtype = self.config._pre_quantization_dtype\n            else:\n                target_dtype = self.q_lin.weight.dtype\n\n            query_states = query_states.to(target_dtype)\n            key_states = key_states.to(target_dtype)\n            value_states = value_states.to(target_dtype)\n\n        attn_weights = self._flash_attention_forward(\n            query_states, key_states, value_states, mask, q_length, dropout=attn_dropout\n        )\n\n        attn_weights_reshaped = attn_weights.reshape(batch_size, q_length, self.n_heads * dim_per_head)\n        attn_output = self.out_lin(attn_weights_reshaped)\n\n        if output_attentions:\n            return (attn_output, attn_weights)\n        else:\n            return (attn_output,)\n"
    },
    {
      "id": "VikParuchuri_surya_159_13",
      "input_code": "    def resize_position_embeddings(self, new_num_position_embeddings: int):\n        \n        num_position_embeds_diff = new_num_position_embeddings - self.config.max_position_embeddings\n\n        if num_position_embeds_diff == 0:\n            return\n\n        self.config.max_position_embeddings = new_num_position_embeddings\n\n        old_position_embeddings_weight = self.embeddings.position_embeddings.weight.clone()\n\n        self.embeddings.position_embeddings = nn.Embedding(self.config.max_position_embeddings, self.config.dim)\n\n        if self.config.sinusoidal_pos_embds:\n            create_sinusoidal_embeddings(\n                n_pos=self.config.max_position_embeddings, dim=self.config.dim, out=self.position_embeddings.weight\n            )\n        else:\n            with torch.no_grad():\n                if num_position_embeds_diff > 0:\n                    self.embeddings.position_embeddings.weight[:-num_position_embeds_diff] = nn.Parameter(\n                        old_position_embeddings_weight\n                    )\n                else:\n                    self.embeddings.position_embeddings.weight = nn.Parameter(\n                        old_position_embeddings_weight[:num_position_embeds_diff]\n                    )\n        self.embeddings.position_embeddings.to(self.device)\n"
    },
    {
      "id": "VikParuchuri_surya_159_6",
      "input_code": "    def model(\n            self,\n            device: Optional[torch.device | str] = None,\n            dtype: Optional[torch.dtype | str] = None\n    ) -> EfficientViTForSemanticSegmentation:\n        if device is None:\n            device = settings.TORCH_DEVICE_MODEL\n        if dtype is None:\n            dtype = settings.MODEL_DTYPE\n\n        config = EfficientViTConfig.from_pretrained(self.checkpoint, revision=self.revision)\n        model = EfficientViTForSemanticSegmentation.from_pretrained(\n            self.checkpoint,\n            torch_dtype=dtype,\n            config=config,\n            revision=self.revision\n        )\n        model = model.to(device)\n        model = model.eval()\n\n        if settings.DETECTOR_STATIC_CACHE:\n            torch.set_float32_matmul_precision('high')\n            torch._dynamo.config.cache_size_limit = 1\n            torch._dynamo.config.suppress_errors = False\n\n            print(f\"Compiling detection model {self.checkpoint} on device {device} with dtype {dtype}\")\n            model = torch.compile(model)\n\n        print(f\"Loaded detection model {self.checkpoint} on device {device} with dtype {dtype}\")\n        return model\n"
    },
    {
      "id": "VikParuchuri_surya_159_1",
      "input_code": "    def batch_layout_detection(\n            self,\n            images: List[Image.Image],\n            batch_size: int | None = None,\n            top_k: int = 5\n    ) -> List[LayoutResult]:\n        assert all([isinstance(image, Image.Image) for image in images])\n        if batch_size is None:\n            batch_size = self.get_batch_size()\n\n        slicer = ImageSlicer(settings.LAYOUT_SLICE_MIN, settings.LAYOUT_SLICE_SIZE)\n\n        batches = []\n        img_counts = [slicer.slice_count(image) for image in images]\n\n        start_idx = 0\n        end_idx = 1\n        while end_idx < len(img_counts):\n            if any([\n                sum(img_counts[start_idx:end_idx]) >= batch_size,\n                sum(img_counts[start_idx:end_idx + 1]) > batch_size,\n            ]):\n                batches.append((start_idx, end_idx))\n                start_idx = end_idx\n            end_idx += 1\n\n        if start_idx < len(img_counts):\n            batches.append((start_idx, len(img_counts)))\n\n        results = []\n        for (start_idx, end_idx) in tqdm(batches, desc=\"Recognizing layout\"):\n            batch_results = []\n            batch_images = images[start_idx:end_idx]\n            batch_images = [image.convert(\"RGB\") for image in batch_images]\n            batch_images, tile_positions = slicer.slice(batch_images)\n            current_batch_size = len(batch_images)\n\n            orig_sizes = [image.size for image in batch_images]\n            model_inputs = self.processor(batch_images)\n\n            batch_pixel_values = model_inputs[\"pixel_values\"]\n            batch_pixel_values = torch.tensor(np.array(batch_pixel_values), dtype=self.model.dtype).to(self.model.device)\n\n            pause_token = [self.model.config.decoder.pause_token_id] * 7\n            start_token = [self.model.config.decoder.bos_token_id] * 7\n            batch_decoder_input = [\n                [start_token] + [pause_token] * self.model.config.decoder.pause_token_count\n                for _ in range(current_batch_size)\n            ]\n            batch_decoder_input = torch.tensor(np.stack(batch_decoder_input, axis=0), dtype=torch.long,\n                                               device=self.model.device)\n            inference_token_count = batch_decoder_input.shape[1]\n\n            decoder_position_ids = torch.ones_like(batch_decoder_input[0, :, 0], dtype=torch.int64,\n                                                   device=self.model.device).cumsum(0) - 1\n            self.model.decoder.model._setup_cache(self.model.config, batch_size, self.model.device, self.model.dtype)\n\n            batch_predictions = [[] for _ in range(current_batch_size)]\n\n            with torch.inference_mode():\n                encoder_hidden_states = self.model.encoder(pixel_values=batch_pixel_values)[0]\n\n                token_count = 0\n                all_done = torch.zeros(current_batch_size, dtype=torch.bool, device=self.model.device)\n\n                while token_count < settings.LAYOUT_MAX_BOXES:\n                    is_prefill = token_count == 0\n                    return_dict = self.model.decoder(\n                        input_boxes=batch_decoder_input,\n                        encoder_hidden_states=encoder_hidden_states,\n                        cache_position=decoder_position_ids,\n                        use_cache=True,\n                        prefill=is_prefill\n                    )\n\n                    decoder_position_ids = decoder_position_ids[-1:] + 1\n                    box_logits = return_dict[\"bbox_logits\"][:current_batch_size, -1, :].detach()\n                    class_logits = return_dict[\"class_logits\"][:current_batch_size, -1, :].detach()\n\n                    class_preds = class_logits.argmax(-1)\n                    box_preds = box_logits * self.model.config.decoder.bbox_size\n\n                    done = (class_preds == self.model.decoder.config.eos_token_id) | (\n                                class_preds == self.model.decoder.config.pad_token_id)\n\n                    all_done = all_done | done\n                    if all_done.all():\n                        break\n\n                    batch_decoder_input = torch.cat([box_preds.unsqueeze(1), class_preds.unsqueeze(1).unsqueeze(1)],\n                                                    dim=-1)\n\n                    for j, (pred, status) in enumerate(zip(batch_decoder_input, all_done)):\n                        if not status:\n                            preds = pred[0].detach().cpu()\n                            prediction = {\n                                \"preds\": preds,\n                                \"token\": preds,\n                                \"polygon\": prediction_to_polygon(\n                                    preds,\n                                    orig_sizes[j],\n                                    self.model.config.decoder.bbox_size,\n                                    self.model.config.decoder.skew_scaler\n                                ),\n                                \"label\": preds[6].item() - self.model.decoder.config.special_token_count,\n                                \"class_logits\": class_logits[j].detach().cpu(),\n                                \"orig_size\": orig_sizes[j]\n                            }\n                            prediction[\"text_label\"] = ID_TO_LABEL.get(int(prediction[\"label\"]), None)\n                            if all([\n                                prediction[\"text_label\"] in [\"PageHeader\", \"PageFooter\"],\n                                prediction[\"polygon\"][0][1] < prediction[\"orig_size\"][1] * .8,\n                                prediction[\"polygon\"][2][1] > prediction[\"orig_size\"][1] * .2,\n                                prediction[\"polygon\"][0][0] < prediction[\"orig_size\"][0] * .8,\n                                prediction[\"polygon\"][2][0] > prediction[\"orig_size\"][0] * .2\n                            ]):\n                                prediction[\"class_logits\"][int(preds[6].item())] = 0\n                                new_prediction = prediction[\"class_logits\"].argmax(-1).item()\n                                prediction[\"label\"] = new_prediction - self.model.decoder.config.special_token_count\n                                prediction[\"token\"][6] = new_prediction\n                                batch_decoder_input[j, -1, 6] = new_prediction\n\n                            prediction[\"top_k_probs\"], prediction[\"top_k_indices\"] = torch.topk(\n                                torch.nn.functional.softmax(prediction[\"class_logits\"], dim=-1), k=top_k, dim=-1)\n                            del prediction[\"class_logits\"]\n                            batch_predictions[j].append(prediction)\n\n                    token_count += inference_token_count\n                    inference_token_count = batch_decoder_input.shape[1]\n                    batch_decoder_input = batch_decoder_input.to(torch.long)\n\n            for j, (pred_dict, orig_size) in enumerate(zip(batch_predictions, orig_sizes)):\n                boxes = []\n                preds = [p for p in pred_dict if\n                         p[\"token\"][6] > self.model.decoder.config.special_token_count]\n                if len(preds) > 0:\n                    polygons = [p[\"polygon\"] for p in preds]\n                    labels = [p[\"label\"] for p in preds]\n                    top_k_probs = [p[\"top_k_probs\"] for p in preds]\n                    top_k_indices = [p[\"top_k_indices\"] - self.model.decoder.config.special_token_count for p in preds]\n\n                    for z, (poly, label, top_k_prob, top_k_index) in enumerate(\n                            zip(polygons, labels, top_k_probs, top_k_indices)):\n                        top_k_dict = {\n                            ID_TO_LABEL.get(int(l)): prob.item()\n                            for (l, prob) in zip(top_k_index, top_k_prob) if l > 0\n                        }\n                        l = ID_TO_LABEL[int(label)]\n                        lb = LayoutBox(\n                            polygon=poly,\n                            label=l,\n                            position=z,\n                            top_k=top_k_dict,\n                            confidence=top_k_dict[l]\n                        )\n                        boxes.append(lb)\n                boxes = clean_boxes(boxes)\n                result = LayoutResult(\n                    bboxes=boxes,\n                    image_bbox=[0, 0, orig_size[0], orig_size[1]]\n                )\n                batch_results.append(result)\n\n            assert len(batch_results) == len(tile_positions)\n            batch_results = slicer.join(batch_results, tile_positions)\n            results.extend(batch_results)\n\n        assert len(results) == len(images)\n        return results\n"
    },
    {
      "id": "VikParuchuri_surya_159_16",
      "input_code": "    def forward(self, boxes: torch.LongTensor, *args):\n        boxes = boxes.to(torch.long).clamp(0, self.config.vocab_size)\n\n        boxes_unbound = boxes.to(torch.long).unbind(dim=-1)\n        cx, cy, w, h, xskew, yskew = boxes_unbound[self.component_idxs[\"bbox\"][0]:self.component_idxs[\"bbox\"][1]]\n        category = boxes_unbound[self.component_idxs[\"category\"][0]:self.component_idxs[\"category\"][1]][0]\n        merges = boxes_unbound[self.component_idxs[\"merges\"][0]:self.component_idxs[\"merges\"][1]][0]\n        colspan = boxes_unbound[self.component_idxs[\"colspan\"][0]:self.component_idxs[\"colspan\"][1]][0]\n\n        xskew_actual = ((xskew - self.config.bbox_size // 2) / 2).to(torch.long)\n        yskew_actual = ((yskew - self.config.bbox_size // 2) / 2).to(torch.long)\n\n        x1 = (cx - w // 2 - xskew_actual).clamp(0, self.config.bbox_size).to(torch.long)\n        y1 = (cy - h // 2 - yskew_actual).clamp(0, self.config.bbox_size).to(torch.long)\n        x3 = (cx + w // 2 + xskew_actual).clamp(0, self.config.bbox_size).to(torch.long)\n        y3 = (cy + h // 2 + yskew_actual).clamp(0, self.config.bbox_size).to(torch.long)\n\n        size_embeds = self.w_embed(w) + self.h_embed(h) + self.cx_embed(cx) + self.cy_embed(cy)\n        skew_embeds = self.xskew_embed(xskew) + self.yskew_embed(yskew)\n        corner_embeds = self.x1_embed(x1) + self.y1_embed(y1) + self.x3_embed(x3) + self.y3_embed(y3)\n        box_embeds = size_embeds + skew_embeds + corner_embeds\n\n        property_embeds = self.category_embed(category) + self.merge_embed(merges) + self.colspan_embed(colspan)\n\n        embedded = torch.cat([box_embeds, property_embeds], dim=-1)\n        return embedded\n"
    },
    {
      "id": "VikParuchuri_surya_159_15",
      "input_code": "    def model(\n            self,\n            device=settings.TORCH_DEVICE_MODEL,\n            dtype=settings.MODEL_DTYPE\n    ) -> TableRecEncoderDecoderModel:\n        if device is None:\n            device = settings.TORCH_DEVICE_MODEL\n        if dtype is None:\n            dtype = settings.MODEL_DTYPE\n\n        config = SuryaTableRecConfig.from_pretrained(self.checkpoint, revision=self.revision)\n        decoder_config = config.decoder\n        decoder = SuryaTableRecDecoderConfig(**decoder_config)\n        config.decoder = decoder\n\n        encoder_config = config.encoder\n        encoder = DonutSwinTableRecConfig(**encoder_config)\n        config.encoder = encoder\n\n        model = TableRecEncoderDecoderModel.from_pretrained(self.checkpoint, config=config, torch_dtype=dtype, revision=self.revision)\n\n        model = model.to(device)\n        model = model.eval()\n\n        if settings.TABLE_REC_STATIC_CACHE:\n            torch.set_float32_matmul_precision('high')\n            torch._dynamo.config.cache_size_limit = 16\n            torch._dynamo.config.suppress_errors = False\n\n            print(f\"Compiling table recognition model {self.checkpoint} on device {device} with dtype {dtype}\")\n            model.encoder = torch.compile(model.encoder)\n            model.decoder = torch.compile(model.decoder)\n\n        print(f\"Loaded table recognition model {self.checkpoint} on device {device} with dtype {dtype}\")\n        return model\n"
    },
    {
      "id": "VikParuchuri_surya_159_7",
      "input_code": "    def _attn(self, q, k, v):\n        dtype = v.dtype\n        q, k, v = q.float(), k.float(), v.float()\n        kv = k.transpose(-1, -2) @ v\n        out = q @ kv\n        out = out[..., :-1] / (out[..., -1:] + self.eps)\n        return out.to(dtype)\n"
    },
    {
      "id": "VikParuchuri_surya_159_11",
      "input_code": "    def model(\n        self,\n        device=settings.TORCH_DEVICE_MODEL,\n        dtype=settings.MODEL_DTYPE\n    ) -> DistilBertForSequenceClassification:\n        if device is None:\n            device = settings.TORCH_DEVICE_MODEL\n        if dtype is None:\n            dtype = settings.MODEL_DTYPE\n\n        config = DistilBertConfig.from_pretrained(self.checkpoint, revision=self.revision)\n        model = DistilBertForSequenceClassification.from_pretrained(\n            self.checkpoint,\n            torch_dtype=dtype,\n            config=config,\n            revision=self.revision\n        ).to(device).eval()\n\n        if settings.OCR_ERROR_STATIC_CACHE:\n            torch.set_float32_matmul_precision('high')\n            torch._dynamo.config.cache_size_limit = 1\n            torch._dynamo.config.suppress_errors = False\n\n            print(f\"Compiling detection model {self.checkpoint} on device {device} with dtype {dtype}\")\n            model = torch.compile(model)\n\n        return model\n"
    },
    {
      "id": "VikParuchuri_surya_159_3",
      "input_code": "    def forward(self, x, position_ids, seq_len=None):\n        self.inv_freq.to(x.device)\n        inv_freq_expanded = self.inv_freq[None, :, None].float().expand(position_ids.shape[0], -1, 1)\n        position_ids_expanded = position_ids[:, None, :].float()\n\n        freqs = (inv_freq_expanded.float() @ position_ids_expanded.float()).transpose(1, 2)\n        emb = torch.cat((freqs, freqs), dim=-1)\n        cos = emb.cos()\n        sin = emb.sin()\n        return cos.to(dtype=x.dtype), sin.to(dtype=x.dtype)\n"
    }
  ],
  "generated_inputs": [
    "<needimport>\nimport torch\n\n<testcase_1>\ndevice_dtype = torch.device('cuda')\n\n<testcase_2>\ndevice_dtype = 'cpu'\n\n<testcase_3>\ndevice_dtype = torch.device('cuda:0')\n\n<testcase_4>\ndevice_dtype = 'cuda'\n\n<testcase_5>\ndevice_dtype = None"
  ],
  "gpt_output": "<needimport>\nimport torch\n\n<testcase_1>\ndevice_dtype = torch.device('cuda')\n\n<testcase_2>\ndevice_dtype = 'cpu'\n\n<testcase_3>\ndevice_dtype = torch.device('cuda:0')\n\n<testcase_4>\ndevice_dtype = 'cuda'\n\n<testcase_5>\ndevice_dtype = None",
  "needimport": "import torch\n\n",
  "test_results": {
    "ans1": null,
    "ans2": true,
    "ans3": null,
    "ans4": null,
    "ans5": null
  }
}