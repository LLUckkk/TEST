{
  "input_header": "def auto_configure_device_map(num_gpus: int) -> Dict[str, int]:",
  "input_docstring": null,
  "output_code": "def auto_configure_device_map(num_gpus: int) -> Dict[str, int]:\n    num_trans_layers = 28\n    per_gpu_layers = 30 / num_gpus\n\n    device_map = {\n        'transformer.embedding.word_embeddings': 0,\n        'transformer.encoder.final_layernorm': 0,\n        'transformer.output_layer': 0,\n        'transformer.rotary_pos_emb': 0,\n        'lm_head': 0\n    }\n\n    used = 2\n    gpu_target = 0\n    for i in range(num_trans_layers):\n        if used >= per_gpu_layers:\n            gpu_target += 1\n            used = 0\n        assert gpu_target < num_gpus\n        device_map[f'transformer.encoder.layers.{i}'] = gpu_target\n        used += 1\n\n    return device_map",
  "input_contexts": [
    {
      "id": "THUDM_ChatGLM2-6B_20_1",
      "input_code": "def load_model_on_gpus(checkpoint_path: Union[str, os.PathLike], num_gpus: int = 2,\n                       device_map: Optional[Dict[str, int]] = None, **kwargs) -> Module:\n    if num_gpus < 2 and device_map is None:\n        model = AutoModel.from_pretrained(checkpoint_path, trust_remote_code=True, **kwargs).half().cuda()\n    else:\n        from accelerate import dispatch_model\n\n        model = AutoModel.from_pretrained(checkpoint_path, trust_remote_code=True, **kwargs).half()\n\n        if device_map is None:\n            device_map = auto_configure_device_map(num_gpus)\n\n        model = dispatch_model(model, device_map=device_map)\n\n    return model\n"
    }
  ],
  "generated_inputs": [
    "<needimport>\nimport os\nfrom typing import Dict\n\n<testcase_1>\nnum_gpus = 1\n\n<testcase_2>\nnum_gpus = 2\n\n<testcase_3>\nnum_gpus = 4\n\n<testcase_4>\nnum_gpus = 8\n\n<testcase_5>\nnum_gpus = 16"
  ],
  "gpt_output": "<needimport>\nimport os\nfrom typing import Dict\n\n<testcase_1>\nnum_gpus = 1\n\n<testcase_2>\nnum_gpus = 2\n\n<testcase_3>\nnum_gpus = 4\n\n<testcase_4>\nnum_gpus = 8\n\n<testcase_5>\nnum_gpus = 16",
  "needimport": "import os\nfrom typing import Dict\n\n",
  "test_results": {
    "ans1": {
      "transformer.embedding.word_embeddings": 0,
      "transformer.encoder.final_layernorm": 0,
      "transformer.output_layer": 0,
      "transformer.rotary_pos_emb": 0,
      "lm_head": 0,
      "transformer.encoder.layers.0": 0,
      "transformer.encoder.layers.1": 0,
      "transformer.encoder.layers.2": 0,
      "transformer.encoder.layers.3": 0,
      "transformer.encoder.layers.4": 0,
      "transformer.encoder.layers.5": 0,
      "transformer.encoder.layers.6": 0,
      "transformer.encoder.layers.7": 0,
      "transformer.encoder.layers.8": 0,
      "transformer.encoder.layers.9": 0,
      "transformer.encoder.layers.10": 0,
      "transformer.encoder.layers.11": 0,
      "transformer.encoder.layers.12": 0,
      "transformer.encoder.layers.13": 0,
      "transformer.encoder.layers.14": 0,
      "transformer.encoder.layers.15": 0,
      "transformer.encoder.layers.16": 0,
      "transformer.encoder.layers.17": 0,
      "transformer.encoder.layers.18": 0,
      "transformer.encoder.layers.19": 0,
      "transformer.encoder.layers.20": 0,
      "transformer.encoder.layers.21": 0,
      "transformer.encoder.layers.22": 0,
      "transformer.encoder.layers.23": 0,
      "transformer.encoder.layers.24": 0,
      "transformer.encoder.layers.25": 0,
      "transformer.encoder.layers.26": 0,
      "transformer.encoder.layers.27": 0
    },
    "ans2": {
      "transformer.embedding.word_embeddings": 0,
      "transformer.encoder.final_layernorm": 0,
      "transformer.output_layer": 0,
      "transformer.rotary_pos_emb": 0,
      "lm_head": 0,
      "transformer.encoder.layers.0": 0,
      "transformer.encoder.layers.1": 0,
      "transformer.encoder.layers.2": 0,
      "transformer.encoder.layers.3": 0,
      "transformer.encoder.layers.4": 0,
      "transformer.encoder.layers.5": 0,
      "transformer.encoder.layers.6": 0,
      "transformer.encoder.layers.7": 0,
      "transformer.encoder.layers.8": 0,
      "transformer.encoder.layers.9": 0,
      "transformer.encoder.layers.10": 0,
      "transformer.encoder.layers.11": 0,
      "transformer.encoder.layers.12": 0,
      "transformer.encoder.layers.13": 1,
      "transformer.encoder.layers.14": 1,
      "transformer.encoder.layers.15": 1,
      "transformer.encoder.layers.16": 1,
      "transformer.encoder.layers.17": 1,
      "transformer.encoder.layers.18": 1,
      "transformer.encoder.layers.19": 1,
      "transformer.encoder.layers.20": 1,
      "transformer.encoder.layers.21": 1,
      "transformer.encoder.layers.22": 1,
      "transformer.encoder.layers.23": 1,
      "transformer.encoder.layers.24": 1,
      "transformer.encoder.layers.25": 1,
      "transformer.encoder.layers.26": 1,
      "transformer.encoder.layers.27": 1
    },
    "ans3": {
      "transformer.embedding.word_embeddings": 0,
      "transformer.encoder.final_layernorm": 0,
      "transformer.output_layer": 0,
      "transformer.rotary_pos_emb": 0,
      "lm_head": 0,
      "transformer.encoder.layers.0": 0,
      "transformer.encoder.layers.1": 0,
      "transformer.encoder.layers.2": 0,
      "transformer.encoder.layers.3": 0,
      "transformer.encoder.layers.4": 0,
      "transformer.encoder.layers.5": 0,
      "transformer.encoder.layers.6": 1,
      "transformer.encoder.layers.7": 1,
      "transformer.encoder.layers.8": 1,
      "transformer.encoder.layers.9": 1,
      "transformer.encoder.layers.10": 1,
      "transformer.encoder.layers.11": 1,
      "transformer.encoder.layers.12": 1,
      "transformer.encoder.layers.13": 1,
      "transformer.encoder.layers.14": 2,
      "transformer.encoder.layers.15": 2,
      "transformer.encoder.layers.16": 2,
      "transformer.encoder.layers.17": 2,
      "transformer.encoder.layers.18": 2,
      "transformer.encoder.layers.19": 2,
      "transformer.encoder.layers.20": 2,
      "transformer.encoder.layers.21": 2,
      "transformer.encoder.layers.22": 3,
      "transformer.encoder.layers.23": 3,
      "transformer.encoder.layers.24": 3,
      "transformer.encoder.layers.25": 3,
      "transformer.encoder.layers.26": 3,
      "transformer.encoder.layers.27": 3
    },
    "ans4": {
      "transformer.embedding.word_embeddings": 0,
      "transformer.encoder.final_layernorm": 0,
      "transformer.output_layer": 0,
      "transformer.rotary_pos_emb": 0,
      "lm_head": 0,
      "transformer.encoder.layers.0": 0,
      "transformer.encoder.layers.1": 0,
      "transformer.encoder.layers.2": 1,
      "transformer.encoder.layers.3": 1,
      "transformer.encoder.layers.4": 1,
      "transformer.encoder.layers.5": 1,
      "transformer.encoder.layers.6": 2,
      "transformer.encoder.layers.7": 2,
      "transformer.encoder.layers.8": 2,
      "transformer.encoder.layers.9": 2,
      "transformer.encoder.layers.10": 3,
      "transformer.encoder.layers.11": 3,
      "transformer.encoder.layers.12": 3,
      "transformer.encoder.layers.13": 3,
      "transformer.encoder.layers.14": 4,
      "transformer.encoder.layers.15": 4,
      "transformer.encoder.layers.16": 4,
      "transformer.encoder.layers.17": 4,
      "transformer.encoder.layers.18": 5,
      "transformer.encoder.layers.19": 5,
      "transformer.encoder.layers.20": 5,
      "transformer.encoder.layers.21": 5,
      "transformer.encoder.layers.22": 6,
      "transformer.encoder.layers.23": 6,
      "transformer.encoder.layers.24": 6,
      "transformer.encoder.layers.25": 6,
      "transformer.encoder.layers.26": 7,
      "transformer.encoder.layers.27": 7
    },
    "ans5": {
      "transformer.embedding.word_embeddings": 0,
      "transformer.encoder.final_layernorm": 0,
      "transformer.output_layer": 0,
      "transformer.rotary_pos_emb": 0,
      "lm_head": 0,
      "transformer.encoder.layers.0": 1,
      "transformer.encoder.layers.1": 1,
      "transformer.encoder.layers.2": 2,
      "transformer.encoder.layers.3": 2,
      "transformer.encoder.layers.4": 3,
      "transformer.encoder.layers.5": 3,
      "transformer.encoder.layers.6": 4,
      "transformer.encoder.layers.7": 4,
      "transformer.encoder.layers.8": 5,
      "transformer.encoder.layers.9": 5,
      "transformer.encoder.layers.10": 6,
      "transformer.encoder.layers.11": 6,
      "transformer.encoder.layers.12": 7,
      "transformer.encoder.layers.13": 7,
      "transformer.encoder.layers.14": 8,
      "transformer.encoder.layers.15": 8,
      "transformer.encoder.layers.16": 9,
      "transformer.encoder.layers.17": 9,
      "transformer.encoder.layers.18": 10,
      "transformer.encoder.layers.19": 10,
      "transformer.encoder.layers.20": 11,
      "transformer.encoder.layers.21": 11,
      "transformer.encoder.layers.22": 12,
      "transformer.encoder.layers.23": 12,
      "transformer.encoder.layers.24": 13,
      "transformer.encoder.layers.25": 13,
      "transformer.encoder.layers.26": 14,
      "transformer.encoder.layers.27": 14
    }
  }
}