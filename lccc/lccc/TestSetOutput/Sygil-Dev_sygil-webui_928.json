{
  "input_header": "def get_sigmas(self, n=None):",
  "input_docstring": null,
  "output_code": "    def get_sigmas(self, n=None):\n        if n is None:\n            return append_zero(self.sigmas.flip(0))\n        t_max = len(self.sigmas) - 1\n        t = torch.linspace(t_max, 0, n, device=self.sigmas.device)\n        return append_zero(self.t_to_sigma(t))",
  "input_contexts": [
    {
      "id": "Sygil-Dev_sygil-webui_928_4",
      "input_code": "    def heun_sampling(\n        self,\n        ac,\n        x,\n        S,\n        cond,\n        unconditional_conditioning=None,\n        unconditional_guidance_scale=1,\n        extra_args=None,\n        callback=None,\n        disable=None,\n        s_churn=0.0,\n        s_tmin=0.0,\n        s_tmax=float(\"inf\"),\n        s_noise=1.0,\n    ):\n        \n        extra_args = {} if extra_args is None else extra_args\n\n        cvd = CompVisDenoiser(alphas_cumprod=ac)\n        sigmas = cvd.get_sigmas(S)\n        x = x * sigmas[0]\n\n        s_in = x.new_ones([x.shape[0]]).half()\n        for i in trange(len(sigmas) - 1, disable=disable):\n            gamma = (\n                min(s_churn / (len(sigmas) - 1), 2**0.5 - 1)\n                if s_tmin <= sigmas[i] <= s_tmax\n                else 0.0\n            )\n            eps = torch.randn_like(x) * s_noise\n            sigma_hat = (sigmas[i] * (gamma + 1)).half()\n            if gamma > 0:\n                x = x + eps * (sigma_hat**2 - sigmas[i] ** 2) ** 0.5\n\n            s_i = sigma_hat * s_in\n            x_in = torch.cat([x] * 2)\n            t_in = torch.cat([s_i] * 2)\n            cond_in = torch.cat([unconditional_conditioning, cond])\n            c_out, c_in = [\n                append_dims(tmp, x_in.ndim) for tmp in cvd.get_scalings(t_in)\n            ]\n            eps = self.apply_model(x_in * c_in, cvd.sigma_to_t(t_in), cond_in)\n            e_t_uncond, e_t = (x_in + eps * c_out).chunk(2)\n            denoised = e_t_uncond + unconditional_guidance_scale * (e_t - e_t_uncond)\n\n            d = to_d(x, sigma_hat, denoised)\n            if callback is not None:\n                callback(\n                    {\n                        \"x\": x,\n                        \"i\": i,\n                        \"sigma\": sigmas[i],\n                        \"sigma_hat\": sigma_hat,\n                        \"denoised\": denoised,\n                    }\n                )\n            dt = sigmas[i + 1] - sigma_hat\n            if sigmas[i + 1] == 0:\n                x = x + d * dt\n            else:\n                x_2 = x + d * dt\n                s_i = sigmas[i + 1] * s_in\n                x_in = torch.cat([x_2] * 2)\n                t_in = torch.cat([s_i] * 2)\n                cond_in = torch.cat([unconditional_conditioning, cond])\n                c_out, c_in = [\n                    append_dims(tmp, x_in.ndim) for tmp in cvd.get_scalings(t_in)\n                ]\n                eps = self.apply_model(x_in * c_in, cvd.sigma_to_t(t_in), cond_in)\n                e_t_uncond, e_t = (x_in + eps * c_out).chunk(2)\n                denoised_2 = e_t_uncond + unconditional_guidance_scale * (\n                    e_t - e_t_uncond\n                )\n\n                d_2 = to_d(x_2, sigmas[i + 1], denoised_2)\n                d_prime = (d + d_2) / 2\n                x = x + d_prime * dt\n        return x\n"
    },
    {
      "id": "Sygil-Dev_sygil-webui_928_7",
      "input_code": "    def lms_sampling(\n        self,\n        ac,\n        x,\n        S,\n        cond,\n        unconditional_conditioning=None,\n        unconditional_guidance_scale=1,\n        extra_args=None,\n        callback=None,\n        disable=None,\n        order=4,\n    ):\n        extra_args = {} if extra_args is None else extra_args\n        s_in = x.new_ones([x.shape[0]])\n\n        cvd = CompVisDenoiser(ac)\n        sigmas = cvd.get_sigmas(S)\n        x = x * sigmas[0]\n\n        ds = []\n        for i in trange(len(sigmas) - 1, disable=disable):\n            s_i = sigmas[i] * s_in\n            x_in = torch.cat([x] * 2)\n            t_in = torch.cat([s_i] * 2)\n            cond_in = torch.cat([unconditional_conditioning, cond])\n            c_out, c_in = [\n                append_dims(tmp, x_in.ndim) for tmp in cvd.get_scalings(t_in)\n            ]\n            eps = self.apply_model(x_in * c_in, cvd.sigma_to_t(t_in), cond_in)\n            e_t_uncond, e_t = (x_in + eps * c_out).chunk(2)\n            denoised = e_t_uncond + unconditional_guidance_scale * (e_t - e_t_uncond)\n\n            d = to_d(x, sigmas[i], denoised)\n            ds.append(d)\n            if len(ds) > order:\n                ds.pop(0)\n            if callback is not None:\n                callback(\n                    {\n                        \"x\": x,\n                        \"i\": i,\n                        \"sigma\": sigmas[i],\n                        \"sigma_hat\": sigmas[i],\n                        \"denoised\": denoised,\n                    }\n                )\n            cur_order = min(i + 1, order)\n            coeffs = [\n                linear_multistep_coeff(cur_order, sigmas.cpu(), i, j)\n                for j in range(cur_order)\n            ]\n            x = x + sum(coeff * d for coeff, d in zip(coeffs, reversed(ds)))\n        return x\n"
    },
    {
      "id": "Sygil-Dev_sygil-webui_928_3",
      "input_code": "    def euler_ancestral_sampling(\n        self,\n        ac,\n        x,\n        S,\n        cond,\n        unconditional_conditioning=None,\n        unconditional_guidance_scale=1,\n        extra_args=None,\n        callback=None,\n        disable=None,\n    ):\n        \n        extra_args = {} if extra_args is None else extra_args\n\n        cvd = CompVisDenoiser(ac)\n        sigmas = cvd.get_sigmas(S)\n        x = x * sigmas[0]\n\n        s_in = x.new_ones([x.shape[0]]).half()\n        for i in trange(len(sigmas) - 1, disable=disable):\n            s_i = sigmas[i] * s_in\n            x_in = torch.cat([x] * 2)\n            t_in = torch.cat([s_i] * 2)\n            cond_in = torch.cat([unconditional_conditioning, cond])\n            c_out, c_in = [\n                append_dims(tmp, x_in.ndim) for tmp in cvd.get_scalings(t_in)\n            ]\n            eps = self.apply_model(x_in * c_in, cvd.sigma_to_t(t_in), cond_in)\n            e_t_uncond, e_t = (x_in + eps * c_out).chunk(2)\n            denoised = e_t_uncond + unconditional_guidance_scale * (e_t - e_t_uncond)\n\n            sigma_down, sigma_up = get_ancestral_step(sigmas[i], sigmas[i + 1])\n            if callback is not None:\n                callback(\n                    {\n                        \"x\": x,\n                        \"i\": i,\n                        \"sigma\": sigmas[i],\n                        \"sigma_hat\": sigmas[i],\n                        \"denoised\": denoised,\n                    }\n                )\n            d = to_d(x, sigmas[i], denoised)\n            dt = sigma_down - sigmas[i]\n            x = x + d * dt\n            x = x + torch.randn_like(x) * sigma_up\n        return x\n"
    },
    {
      "id": "Sygil-Dev_sygil-webui_928_13",
      "input_code": "    def sample(\n        self,\n        S,\n        conditioning,\n        batch_size,\n        shape,\n        verbose,\n        unconditional_guidance_scale,\n        unconditional_conditioning,\n        eta,\n        x_T,\n        img_callback: Callable = None,\n    ):\n        sigmas = self.model_wrap.get_sigmas(S)\n        x = x_T * sigmas[0]\n        model_wrap_cfg = CFGDenoiser(self.model_wrap)\n\n        samples_ddim = K.sampling.__dict__[f\"sample_{self.schedule}\"](\n            model_wrap_cfg,\n            x,\n            sigmas,\n            extra_args={\n                \"cond\": conditioning,\n                \"uncond\": unconditional_conditioning,\n                \"cond_scale\": unconditional_guidance_scale,\n            },\n            disable=False,\n            callback=partial(KDiffusionSampler.img_callback_wrapper, img_callback),\n        )\n\n        return samples_ddim, None\n"
    },
    {
      "id": "Sygil-Dev_sygil-webui_928_2",
      "input_code": "    def euler_sampling(\n        self,\n        ac,\n        x,\n        S,\n        cond,\n        unconditional_conditioning=None,\n        unconditional_guidance_scale=1,\n        extra_args=None,\n        callback=None,\n        disable=None,\n        s_churn=0.0,\n        s_tmin=0.0,\n        s_tmax=float(\"inf\"),\n        s_noise=1.0,\n    ):\n        \n        extra_args = {} if extra_args is None else extra_args\n        cvd = CompVisDenoiser(ac)\n        sigmas = cvd.get_sigmas(S)\n        x = x * sigmas[0]\n\n        s_in = x.new_ones([x.shape[0]]).half()\n        for i in trange(len(sigmas) - 1, disable=disable):\n            gamma = (\n                min(s_churn / (len(sigmas) - 1), 2**0.5 - 1)\n                if s_tmin <= sigmas[i] <= s_tmax\n                else 0.0\n            )\n            eps = torch.randn_like(x) * s_noise\n            sigma_hat = (sigmas[i] * (gamma + 1)).half()\n            if gamma > 0:\n                x = x + eps * (sigma_hat**2 - sigmas[i] ** 2) ** 0.5\n\n            s_i = sigma_hat * s_in\n            x_in = torch.cat([x] * 2)\n            t_in = torch.cat([s_i] * 2)\n            cond_in = torch.cat([unconditional_conditioning, cond])\n            c_out, c_in = [\n                append_dims(tmp, x_in.ndim) for tmp in cvd.get_scalings(t_in)\n            ]\n            eps = self.apply_model(x_in * c_in, cvd.sigma_to_t(t_in), cond_in)\n            e_t_uncond, e_t = (x_in + eps * c_out).chunk(2)\n            denoised = e_t_uncond + unconditional_guidance_scale * (e_t - e_t_uncond)\n\n            d = to_d(x, sigma_hat, denoised)\n            if callback is not None:\n                callback(\n                    {\n                        \"x\": x,\n                        \"i\": i,\n                        \"sigma\": sigmas[i],\n                        \"sigma_hat\": sigma_hat,\n                        \"denoised\": denoised,\n                    }\n                )\n            dt = sigmas[i + 1] - sigma_hat\n            x = x + d * dt\n        return x\n"
    },
    {
      "id": "Sygil-Dev_sygil-webui_928_14",
      "input_code": "def img2img(\n    prompt: str,\n    image_editor_mode: str,\n    mask_mode: str,\n    mask_blur_strength: int,\n    mask_restore: bool,\n    ddim_steps: int,\n    sampler_name: str,\n    toggles: List[int],\n    realesrgan_model_name: str,\n    n_iter: int,\n    cfg_scale: float,\n    denoising_strength: float,\n    seed: int,\n    height: int,\n    width: int,\n    resize_mode: int,\n    init_info: any = None,\n    init_info_mask: any = None,\n    fp=None,\n    job_info: JobInfo = None,\n):\n    outpath = opt.outdir_img2img or opt.outdir or \"outputs/img2img-samples\"\n    seed = seed_to_int(seed)\n\n    batch_size = 1\n\n    prompt_matrix = 0 in toggles\n    normalize_prompt_weights = 1 in toggles\n    loopback = 2 in toggles\n    random_seed_loopback = 3 in toggles\n    skip_save = 4 not in toggles\n    skip_grid = 5 not in toggles\n    sort_samples = 6 in toggles\n    write_info_files = 7 in toggles\n    write_sample_info_to_log_file = 8 in toggles\n    jpg_sample = 9 in toggles\n    do_color_correction = 10 in toggles\n    filter_nsfw = 11 in toggles\n    use_GFPGAN = 12 in toggles\n    use_RealESRGAN = 13 in toggles\n    ModelLoader([\"model\"], True, False)\n    if use_GFPGAN and not use_RealESRGAN:\n        ModelLoader([\"GFPGAN\"], True, False)\n        ModelLoader([\"RealESRGAN\"], False, True)\n    if use_RealESRGAN and not use_GFPGAN:\n        ModelLoader([\"GFPGAN\"], False, True)\n        ModelLoader([\"RealESRGAN\"], True, False, realesrgan_model_name)\n    if use_RealESRGAN and use_GFPGAN:\n        ModelLoader([\"GFPGAN\", \"RealESRGAN\"], True, False, realesrgan_model_name)\n    if sampler_name == \"DDIM\":\n        sampler = DDIMSampler(model)\n    elif sampler_name == \"k_dpm_2_a\":\n        sampler = KDiffusionSampler(model, \"dpm_2_ancestral\")\n    elif sampler_name == \"k_dpm_2\":\n        sampler = KDiffusionSampler(model, \"dpm_2\")\n    elif sampler_name == \"k_euler_a\":\n        sampler = KDiffusionSampler(model, \"euler_ancestral\")\n    elif sampler_name == \"k_euler\":\n        sampler = KDiffusionSampler(model, \"euler\")\n    elif sampler_name == \"k_heun\":\n        sampler = KDiffusionSampler(model, \"heun\")\n    elif sampler_name == \"k_lms\":\n        sampler = KDiffusionSampler(model, \"lms\")\n    else:\n        raise Exception(\"Unknown sampler: \" + sampler_name)\n\n    if image_editor_mode == \"Mask\":\n        init_img = init_info_mask[\"image\"]\n        init_img_transparency = (\n            ImageOps.invert(init_img.split()[-1])\n            .convert(\"L\")\n            .point(lambda x: 255 if x > 0 else 0, mode=\"1\")\n        )\n        init_img = init_img.convert(\"RGB\")\n        init_img = resize_image(resize_mode, init_img, width, height)\n        init_img = init_img.convert(\"RGB\")\n        init_mask = init_info_mask[\"mask\"]\n        init_mask = ImageChops.lighter(\n            init_img_transparency, init_mask.convert(\"L\")\n        ).convert(\"RGBA\")\n        init_mask = init_mask.convert(\"RGB\")\n        init_mask = resize_image(resize_mode, init_mask, width, height)\n        init_mask = init_mask.convert(\"RGB\")\n        keep_mask = mask_mode == 0\n        init_mask = init_mask if keep_mask else ImageOps.invert(init_mask)\n    else:\n        init_img = init_info\n        init_mask = None\n        keep_mask = False\n\n    assert 0.0 <= denoising_strength <= 1.0, \"can only work with strength in [0.0, 1.0]\"\n    t_enc = int(denoising_strength * ddim_steps)\n\n    def init():\n        image = init_img.convert(\"RGB\")\n        image = resize_image(resize_mode, image, width, height)\n        image = np.array(image).astype(np.float32) / 255.0\n        image = image[None].transpose(0, 3, 1, 2)\n        image = torch.from_numpy(image)\n\n        mask_channel = None\n        if image_editor_mode == \"Mask\":\n            alpha = init_mask.convert(\"RGBA\")\n            alpha = resize_image(resize_mode, alpha, width // 8, height // 8)\n            mask_channel = alpha.split()[1]\n\n        mask = None\n        if mask_channel is not None:\n            mask = np.array(mask_channel).astype(np.float32) / 255.0\n            mask = 1 - mask\n            mask = np.tile(mask, (4, 1, 1))\n            mask = mask[None].transpose(0, 1, 2, 3)\n            mask = torch.from_numpy(mask).to(device)\n        if opt.optimized:\n            modelFS.to(device)\n\n\n        if image_editor_mode == \"Uncrop\":\n            _image = image.numpy()[0]\n            _mask = np.ones((_image.shape[1], _image.shape[2]))\n\n            cmax = np.max(_image, axis=0)\n            rowmax = np.max(cmax, axis=0)\n            colmax = np.max(cmax, axis=1)\n            rowwhere = np.where(rowmax > 0)[0]\n            colwhere = np.where(colmax > 0)[0]\n            rowstart = rowwhere[0]\n            rowend = rowwhere[-1] + 1\n            colstart = colwhere[0]\n            colend = colwhere[-1] + 1\n            print(\"bounding box: \", rowstart, rowend, colstart, colend)\n\n            PAD_IMG = 16\n            boundingbox = np.zeros(shape=(height, width))\n            boundingbox[\n                colstart + PAD_IMG : colend - PAD_IMG,\n                rowstart + PAD_IMG : rowend - PAD_IMG,\n            ] = 1\n            boundingbox = blurArr(boundingbox, 4)\n\n            PAD_MASK = 24\n            boundingbox2 = np.zeros(shape=(height, width))\n            boundingbox2[\n                colstart + PAD_MASK : colend - PAD_MASK,\n                rowstart + PAD_MASK : rowend - PAD_MASK,\n            ] = 1\n            boundingbox2 = blurArr(boundingbox2, 4)\n\n            noise = np.array(\n                [perlinNoise(height, width, height / 64, width / 64) for i in range(3)]\n            )\n            _mask *= 1 - boundingbox2\n\n            _image = 2.0 * _image - 1.0\n\n            boundingbox = np.tile(boundingbox, (3, 1, 1))\n            _image = _image * boundingbox + noise * (1 - boundingbox)\n\n            _mask = (\n                np.array(\n                    resize_image(\n                        resize_mode,\n                        Image.fromarray(_mask * 255),\n                        width // 8,\n                        height // 8,\n                    )\n                )\n                / 255\n            )\n\n            init_image = torch.from_numpy(\n                np.expand_dims(_image, axis=0).astype(np.float32)\n            ).to(device)\n            mask = torch.from_numpy(_mask.astype(np.float32)).to(device)\n\n        else:\n            init_image = 2.0 * image - 1.0\n\n        init_image = init_image.to(device)\n        init_image = repeat(init_image, \"1 ... -> b ...\", b=batch_size)\n        init_latent = (\n            model if not opt.optimized else modelFS\n        ).get_first_stage_encoding(\n            (model if not opt.optimized else modelFS).encode_first_stage(init_image)\n        )\n\n        if opt.optimized:\n            mem = torch.cuda.memory_allocated() / 1e6\n            modelFS.to(\"cpu\")\n            while torch.cuda.memory_allocated() / 1e6 >= mem:\n                time.sleep(1)\n\n        return (\n            init_latent,\n            mask,\n        )\n\n    def sample(\n        init_data,\n        x,\n        conditioning,\n        unconditional_conditioning,\n        sampler_name,\n        img_callback: Callable = None,\n    ):\n        t_enc_steps = t_enc\n        obliterate = False\n        if ddim_steps == t_enc_steps:\n            t_enc_steps = t_enc_steps - 1\n            obliterate = True\n\n        if sampler_name != \"DDIM\":\n            x0, z_mask = init_data\n\n            sigmas = sampler.model_wrap.get_sigmas(ddim_steps)\n            noise = x * sigmas[ddim_steps - t_enc_steps - 1]\n\n            xi = x0 + noise\n\n            if z_mask is not None and obliterate:\n                random = torch.randn(z_mask.shape, device=xi.device)\n                xi = (z_mask * noise) + ((1 - z_mask) * xi)\n\n            sigma_sched = sigmas[ddim_steps - t_enc_steps - 1 :]\n            model_wrap_cfg = CFGMaskedDenoiser(sampler.model_wrap)\n            samples_ddim = K.sampling.__dict__[f\"sample_{sampler.get_sampler_name()}\"](\n                model_wrap_cfg,\n                xi,\n                sigma_sched,\n                extra_args={\n                    \"cond\": conditioning,\n                    \"uncond\": unconditional_conditioning,\n                    \"cond_scale\": cfg_scale,\n                    \"mask\": z_mask,\n                    \"x0\": x0,\n                    \"xi\": xi,\n                },\n                disable=False,\n                callback=partial(KDiffusionSampler.img_callback_wrapper, img_callback),\n            )\n        else:\n            x0, z_mask = init_data\n\n            sampler.make_schedule(\n                ddim_num_steps=ddim_steps, ddim_eta=0.0, verbose=False\n            )\n            z_enc = sampler.stochastic_encode(\n                x0, torch.tensor([t_enc_steps] * batch_size).to(device)\n            )\n\n            if z_mask is not None and obliterate:\n                random = torch.randn(z_mask.shape, device=z_enc.device)\n                z_enc = (z_mask * random) + ((1 - z_mask) * z_enc)\n\n            samples_ddim = sampler.decode(\n                z_enc,\n                conditioning,\n                t_enc_steps,\n                unconditional_guidance_scale=cfg_scale,\n                unconditional_conditioning=unconditional_conditioning,\n                z_mask=z_mask,\n                x0=x0,\n            )\n        return samples_ddim\n\n    correction_target = None\n    if loopback:\n        output_images, info = None, None\n        history = []\n        initial_seed = None\n\n        do_color_correction = True\n\n        for i in range(n_iter):\n            if do_color_correction and i == 0:\n                correction_target = cv2.cvtColor(\n                    np.asarray(init_img.copy()), cv2.COLOR_RGB2LAB\n                )\n\n            output_images, seed, info, stats = process_images(\n                outpath=outpath,\n                func_init=init,\n                func_sample=sample,\n                prompt=prompt,\n                seed=seed,\n                sampler_name=sampler_name,\n                skip_save=skip_save,\n                skip_grid=skip_grid,\n                batch_size=1,\n                n_iter=1,\n                steps=ddim_steps,\n                cfg_scale=cfg_scale,\n                width=width,\n                height=height,\n                prompt_matrix=prompt_matrix,\n                filter_nsfw=filter_nsfw,\n                use_GFPGAN=use_GFPGAN,\n                use_RealESRGAN=False,                realesrgan_model_name=realesrgan_model_name,\n                fp=fp,\n                do_not_save_grid=True,\n                normalize_prompt_weights=normalize_prompt_weights,\n                init_img=init_img,\n                init_mask=init_mask,\n                keep_mask=keep_mask,\n                mask_blur_strength=mask_blur_strength,\n                mask_restore=mask_restore,\n                denoising_strength=denoising_strength,\n                resize_mode=resize_mode,\n                uses_loopback=loopback,\n                uses_random_seed_loopback=random_seed_loopback,\n                sort_samples=sort_samples,\n                write_info_files=write_info_files,\n                write_sample_info_to_log_file=write_sample_info_to_log_file,\n                jpg_sample=jpg_sample,\n                job_info=job_info,\n                do_color_correction=do_color_correction,\n                correction_target=correction_target,\n            )\n\n            if initial_seed is None:\n                initial_seed = seed\n\n            init_img = output_images[0]\n\n            if not random_seed_loopback:\n                seed = seed + 1\n            else:\n                seed = seed_to_int(None)\n            denoising_strength = max(denoising_strength * 0.95, 0.1)\n            history.append(init_img)\n\n        if not skip_grid:\n            grid_count = get_next_sequence_number(outpath, \"grid-\")\n            grid = image_grid(history, batch_size, force_n_rows=1)\n            grid_file = f\"grid-{grid_count:05}-{seed}_{prompt.replace(' ', '_').translate({ord(x): '' for x in invalid_filename_chars})[:128]}.{grid_ext}\"\n            grid.save(\n                os.path.join(outpath, grid_file),\n                grid_format,\n                quality=grid_quality,\n                lossless=grid_lossless,\n                optimize=True,\n            )\n\n        output_images = history\n        seed = initial_seed\n\n    else:\n        if do_color_correction:\n            correction_target = cv2.cvtColor(\n                np.asarray(init_img.copy()), cv2.COLOR_RGB2LAB\n            )\n\n        output_images, seed, info, stats = process_images(\n            outpath=outpath,\n            func_init=init,\n            func_sample=sample,\n            prompt=prompt,\n            seed=seed,\n            sampler_name=sampler_name,\n            skip_save=skip_save,\n            skip_grid=skip_grid,\n            batch_size=batch_size,\n            n_iter=n_iter,\n            steps=ddim_steps,\n            cfg_scale=cfg_scale,\n            width=width,\n            height=height,\n            prompt_matrix=prompt_matrix,\n            filter_nsfw=filter_nsfw,\n            use_GFPGAN=use_GFPGAN,\n            use_RealESRGAN=use_RealESRGAN,\n            realesrgan_model_name=realesrgan_model_name,\n            fp=fp,\n            normalize_prompt_weights=normalize_prompt_weights,\n            init_img=init_img,\n            init_mask=init_mask,\n            keep_mask=keep_mask,\n            mask_blur_strength=mask_blur_strength,\n            denoising_strength=denoising_strength,\n            mask_restore=mask_restore,\n            resize_mode=resize_mode,\n            uses_loopback=loopback,\n            sort_samples=sort_samples,\n            write_info_files=write_info_files,\n            write_sample_info_to_log_file=write_sample_info_to_log_file,\n            jpg_sample=jpg_sample,\n            job_info=job_info,\n            do_color_correction=do_color_correction,\n            correction_target=correction_target,\n        )\n\n    del sampler\n\n    return output_images, seed, info, stats\n"
    },
    {
      "id": "Sygil-Dev_sygil-webui_928_16",
      "input_code": "def img2img(\n    prompt: str = \"\",\n    init_info: any = None,\n    init_info_mask: any = None,\n    mask_mode: int = 0,\n    mask_blur_strength: int = 3,\n    mask_restore: bool = False,\n    ddim_steps: int = 50,\n    sampler_name: str = \"DDIM\",\n    n_iter: int = 1,\n    cfg_scale: float = 7.5,\n    denoising_strength: float = 0.8,\n    seed: int = -1,\n    noise_mode: int = 0,\n    find_noise_steps: str = \"\",\n    height: int = 512,\n    width: int = 512,\n    resize_mode: int = 0,\n    fp=None,\n    variant_amount: float = 0.0,\n    variant_seed: int = None,\n    ddim_eta: float = 0.0,\n    write_info_files: bool = True,\n    separate_prompts: bool = False,\n    normalize_prompt_weights: bool = True,\n    save_individual_images: bool = True,\n    save_grid: bool = True,\n    group_by_prompt: bool = True,\n    save_as_jpg: bool = True,\n    use_GFPGAN: bool = True,\n    GFPGAN_model: str = \"GFPGANv1.4\",\n    use_RealESRGAN: bool = True,\n    RealESRGAN_model: str = \"RealESRGAN_x4plus_anime_6B\",\n    use_LDSR: bool = True,\n    LDSR_model: str = \"model\",\n    loopback: bool = False,\n    random_seed_loopback: bool = False,\n):\n    outpath = st.session_state[\"defaults\"].general.outdir_img2img\n    seed = seed_to_int(seed)\n\n    batch_size = 1\n\n    if sampler_name == \"PLMS\":\n        sampler = PLMSSampler(server_state[\"model\"])\n    elif sampler_name == \"DDIM\":\n        sampler = DDIMSampler(server_state[\"model\"])\n    elif sampler_name == \"k_dpm_2_a\":\n        sampler = KDiffusionSampler(server_state[\"model\"], \"dpm_2_ancestral\")\n    elif sampler_name == \"k_dpm_2\":\n        sampler = KDiffusionSampler(server_state[\"model\"], \"dpm_2\")\n    elif sampler_name == \"k_dpmpp_2m\":\n        sampler = KDiffusionSampler(server_state[\"model\"], \"dpmpp_2m\")\n    elif sampler_name == \"k_euler_a\":\n        sampler = KDiffusionSampler(server_state[\"model\"], \"euler_ancestral\")\n    elif sampler_name == \"k_euler\":\n        sampler = KDiffusionSampler(server_state[\"model\"], \"euler\")\n    elif sampler_name == \"k_heun\":\n        sampler = KDiffusionSampler(server_state[\"model\"], \"heun\")\n    elif sampler_name == \"k_lms\":\n        sampler = KDiffusionSampler(server_state[\"model\"], \"lms\")\n    else:\n        raise Exception(\"Unknown sampler: \" + sampler_name)\n\n    def process_init_mask(init_mask: Image):\n        if init_mask.mode == \"RGBA\":\n            init_mask = init_mask.convert(\"RGBA\")\n            background = Image.new(\"RGBA\", init_mask.size, (0, 0, 0))\n            init_mask = Image.alpha_composite(background, init_mask)\n            init_mask = init_mask.convert(\"RGB\")\n        return init_mask\n\n    init_img = init_info\n    init_mask = None\n    if mask_mode == 0:\n        if init_info_mask:\n            init_mask = process_init_mask(init_info_mask)\n    elif mask_mode == 1:\n        if init_info_mask:\n            init_mask = process_init_mask(init_info_mask)\n            init_mask = ImageOps.invert(init_mask)\n    elif mask_mode == 2:\n        init_img_transparency = init_img.split()[-1].convert(\n            \"L\"\n        )\n        init_mask = init_img_transparency\n        init_mask = init_mask.convert(\"RGB\")\n        init_mask = resize_image(resize_mode, init_mask, width, height)\n        init_mask = init_mask.convert(\"RGB\")\n\n    assert 0.0 <= denoising_strength <= 1.0, \"can only work with strength in [0.0, 1.0]\"\n    t_enc = int(denoising_strength * ddim_steps)\n\n    if (\n        init_mask is not None\n        and (noise_mode == 2 or noise_mode == 3)\n        and init_img is not None\n    ):\n        noise_q = 0.99\n        color_variation = 0.0\n        mask_blend_factor = 1.0\n\n        np_init = (np.asarray(init_img.convert(\"RGB\")) / 255.0).astype(\n            np.float64\n        )\n        np_mask_rgb = 1.0 - (\n            np.asarray(ImageOps.invert(init_mask).convert(\"RGB\")) / 255.0\n        ).astype(np.float64)\n        np_mask_rgb -= np.min(np_mask_rgb)\n        np_mask_rgb /= np.max(np_mask_rgb)\n        np_mask_rgb = 1.0 - np_mask_rgb\n        np_mask_rgb_hardened = 1.0 - (np_mask_rgb < 0.99).astype(np.float64)\n        blurred = skimage.filters.gaussian(\n            np_mask_rgb_hardened[:], sigma=16.0, channel_axis=2, truncate=32.0\n        )\n        blurred2 = skimage.filters.gaussian(\n            np_mask_rgb_hardened[:], sigma=16.0, channel_axis=2, truncate=32.0\n        )\n        np_mask_rgb_dilated = np.clip((np_mask_rgb + blurred2) * 0.7071, 0.0, 1.0)\n        np_mask_rgb = np.clip((np_mask_rgb + blurred) * 0.7071, 0.0, 1.0)\n\n        noise_rgb = get_matched_noise(np_init, np_mask_rgb, noise_q, color_variation)\n        blend_mask_rgb = np.clip(np_mask_rgb_dilated, 0.0, 1.0) ** (mask_blend_factor)\n        noised = noise_rgb[:]\n        blend_mask_rgb **= 2.0\n        noised = np_init[:] * (1.0 - blend_mask_rgb) + noised * blend_mask_rgb\n\n        np_mask_grey = np.sum(np_mask_rgb, axis=2) / 3.0\n        ref_mask = np_mask_grey < 1e-3\n\n        all_mask = np.ones((height, width), dtype=bool)\n        noised[all_mask, :] = skimage.exposure.match_histograms(\n            noised[all_mask, :] ** 1.0, noised[ref_mask, :], channel_axis=1\n        )\n\n        init_img = Image.fromarray(\n            np.clip(noised * 255.0, 0.0, 255.0).astype(np.uint8), mode=\"RGB\"\n        )\n        st.session_state[\"editor_image\"].image(init_img)\n\n    def init():\n        image = init_img.convert(\"RGB\")\n        image = np.array(image).astype(np.float32) / 255.0\n        image = image[None].transpose(0, 3, 1, 2)\n        image = torch.from_numpy(image)\n\n        mask_channel = None\n        if init_mask:\n            alpha = resize_image(resize_mode, init_mask, width // 8, height // 8)\n            mask_channel = alpha.split()[-1]\n\n        mask = None\n        if mask_channel is not None:\n            mask = np.array(mask_channel).astype(np.float32) / 255.0\n            mask = 1 - mask\n            mask = np.tile(mask, (4, 1, 1))\n            mask = mask[None].transpose(0, 1, 2, 3)\n            mask = torch.from_numpy(mask).to(server_state[\"device\"])\n\n        if st.session_state[\"defaults\"].general.optimized:\n            server_state[\"modelFS\"].to(server_state[\"device\"])\n\n        init_image = 2.0 * image - 1.0\n        init_image = init_image.to(server_state[\"device\"])\n        init_latent = (\n            server_state[\"model\"]\n            if not st.session_state[\"defaults\"].general.optimized\n            else server_state[\"modelFS\"]\n        ).get_first_stage_encoding(\n            (\n                server_state[\"model\"]\n                if not st.session_state[\"defaults\"].general.optimized\n                else server_state[\"modelFS\"]\n            ).encode_first_stage(init_image)\n        )\n\n        if st.session_state[\"defaults\"].general.optimized:\n            mem = torch.cuda.memory_allocated() / 1e6\n            server_state[\"modelFS\"].to(\"cpu\")\n            while torch.cuda.memory_allocated() / 1e6 >= mem:\n                time.sleep(1)\n\n        return (\n            init_latent,\n            mask,\n        )\n\n    def sample(init_data, x, conditioning, unconditional_conditioning, sampler_name):\n        t_enc_steps = t_enc\n        obliterate = False\n        if ddim_steps == t_enc_steps:\n            t_enc_steps = t_enc_steps - 1\n            obliterate = True\n\n        if sampler_name != \"DDIM\":\n            x0, z_mask = init_data\n\n            sigmas = sampler.model_wrap.get_sigmas(ddim_steps)\n            noise = x * sigmas[ddim_steps - t_enc_steps - 1]\n\n            xi = x0 + noise\n\n            if z_mask is not None and obliterate:\n                random = torch.randn(z_mask.shape, device=xi.device)\n                xi = (z_mask * noise) + ((1 - z_mask) * xi)\n\n            sigma_sched = sigmas[ddim_steps - t_enc_steps - 1 :]\n            model_wrap_cfg = CFGMaskedDenoiser(sampler.model_wrap)\n            samples_ddim = K.sampling.__dict__[f\"sample_{sampler.get_sampler_name()}\"](\n                model_wrap_cfg,\n                xi,\n                sigma_sched,\n                extra_args={\n                    \"cond\": conditioning,\n                    \"uncond\": unconditional_conditioning,\n                    \"cond_scale\": cfg_scale,\n                    \"mask\": z_mask,\n                    \"x0\": x0,\n                    \"xi\": xi,\n                },\n                disable=False,\n                callback=generation_callback if not server_state[\"bridge\"] else None,\n            )\n        else:\n            x0, z_mask = init_data\n\n            sampler.make_schedule(\n                ddim_num_steps=ddim_steps, ddim_eta=0.0, verbose=False\n            )\n            z_enc = sampler.stochastic_encode(\n                x0, torch.tensor([t_enc_steps] * batch_size).to(server_state[\"device\"])\n            )\n\n            if z_mask is not None and obliterate:\n                random = torch.randn(z_mask.shape, device=z_enc.device)\n                z_enc = (z_mask * random) + ((1 - z_mask) * z_enc)\n\n            samples_ddim = sampler.decode(\n                z_enc,\n                conditioning,\n                t_enc_steps,\n                unconditional_guidance_scale=cfg_scale,\n                unconditional_conditioning=unconditional_conditioning,\n                z_mask=z_mask,\n                x0=x0,\n            )\n        return samples_ddim\n\n    if loopback:\n        output_images, info = None, None\n        history = []\n        initial_seed = None\n\n        do_color_correction = False\n        try:\n            from skimage import exposure\n\n            do_color_correction = True\n        except:\n            logger.error(\"Install scikit-image to perform color correction on loopback\")\n\n        for i in range(n_iter):\n            if do_color_correction and i == 0:\n                correction_target = cv2.cvtColor(\n                    np.asarray(init_img.copy()), cv2.COLOR_RGB2LAB\n                )\n\n            is_final_iteration = i == n_iter - 1\n\n            output_images, seed, info, stats = process_images(\n                outpath=outpath,\n                func_init=init,\n                func_sample=sample,\n                prompt=prompt,\n                seed=seed,\n                sampler_name=sampler_name,\n                save_grid=save_grid,\n                batch_size=1,\n                n_iter=1,\n                steps=ddim_steps,\n                cfg_scale=cfg_scale,\n                width=width,\n                height=height,\n                prompt_matrix=separate_prompts,\n                use_GFPGAN=use_GFPGAN,\n                GFPGAN_model=GFPGAN_model,\n                use_RealESRGAN=use_RealESRGAN\n                and is_final_iteration,                realesrgan_model_name=RealESRGAN_model,\n                use_LDSR=use_LDSR,\n                LDSR_model_name=LDSR_model,\n                normalize_prompt_weights=normalize_prompt_weights,\n                save_individual_images=save_individual_images,\n                init_img=init_img,\n                init_mask=init_mask,\n                mask_blur_strength=mask_blur_strength,\n                mask_restore=mask_restore,\n                denoising_strength=denoising_strength,\n                noise_mode=noise_mode,\n                find_noise_steps=find_noise_steps,\n                resize_mode=resize_mode,\n                uses_loopback=loopback,\n                uses_random_seed_loopback=random_seed_loopback,\n                sort_samples=group_by_prompt,\n                write_info_files=write_info_files,\n                jpg_sample=save_as_jpg,\n            )\n\n            if initial_seed is None:\n                initial_seed = seed\n\n            input_image = init_img\n            init_img = output_images[0]\n\n            if do_color_correction and correction_target is not None:\n                init_img = Image.fromarray(\n                    cv2.cvtColor(\n                        exposure.match_histograms(\n                            cv2.cvtColor(np.asarray(init_img), cv2.COLOR_RGB2LAB),\n                            correction_target,\n                            channel_axis=2,\n                        ),\n                        cv2.COLOR_LAB2RGB,\n                    ).astype(\"uint8\")\n                )\n                if mask_restore is True and init_mask is not None:\n                    color_mask = init_mask.filter(\n                        ImageFilter.GaussianBlur(mask_blur_strength)\n                    )\n                    color_mask = color_mask.convert(\"L\")\n                    source_image = input_image.convert(\"RGB\")\n                    target_image = init_img.convert(\"RGB\")\n\n                    init_img = Image.composite(source_image, target_image, color_mask)\n\n            if not random_seed_loopback:\n                seed = seed + 1\n            else:\n                seed = seed_to_int(None)\n\n            denoising_strength = max(denoising_strength * 0.95, 0.1)\n            history.append(init_img)\n\n        output_images = history\n        seed = initial_seed\n\n    else:\n        output_images, seed, info, stats = process_images(\n            outpath=outpath,\n            func_init=init,\n            func_sample=sample,\n            prompt=prompt,\n            seed=seed,\n            sampler_name=sampler_name,\n            save_grid=save_grid,\n            batch_size=batch_size,\n            n_iter=n_iter,\n            steps=ddim_steps,\n            cfg_scale=cfg_scale,\n            width=width,\n            height=height,\n            prompt_matrix=separate_prompts,\n            use_GFPGAN=use_GFPGAN,\n            GFPGAN_model=GFPGAN_model,\n            use_RealESRGAN=use_RealESRGAN,\n            realesrgan_model_name=RealESRGAN_model,\n            use_LDSR=use_LDSR,\n            LDSR_model_name=LDSR_model,\n            normalize_prompt_weights=normalize_prompt_weights,\n            save_individual_images=save_individual_images,\n            init_img=init_img,\n            init_mask=init_mask,\n            mask_blur_strength=mask_blur_strength,\n            denoising_strength=denoising_strength,\n            noise_mode=noise_mode,\n            find_noise_steps=find_noise_steps,\n            mask_restore=mask_restore,\n            resize_mode=resize_mode,\n            uses_loopback=loopback,\n            sort_samples=group_by_prompt,\n            write_info_files=write_info_files,\n            jpg_sample=save_as_jpg,\n        )\n\n    del sampler\n\n    return output_images, seed, info, stats\n"
    },
    {
      "id": "Sygil-Dev_sygil-webui_928_11",
      "input_code": "def find_noise_for_image(\n    model,\n    device,\n    init_image,\n    prompt,\n    steps=200,\n    cond_scale=2.0,\n    verbose=False,\n    normalize=False,\n    generation_callback=None,\n):\n    image = np.array(init_image).astype(np.float32) / 255.0\n    image = image[None].transpose(0, 3, 1, 2)\n    image = torch.from_numpy(image)\n    image = 2.0 * image - 1.0\n    image = image.to(device)\n    x = model.get_first_stage_encoding(model.encode_first_stage(image))\n\n    uncond = model.get_learned_conditioning([\"\"])\n    cond = model.get_learned_conditioning([prompt])\n\n    s_in = x.new_ones([x.shape[0]])\n    dnw = K.external.CompVisDenoiser(model)\n    sigmas = dnw.get_sigmas(steps).flip(0)\n\n    if verbose:\n        logger.info(sigmas)\n\n    for i in trange(1, len(sigmas)):\n        x_in = torch.cat([x] * 2)\n        sigma_in = torch.cat([sigmas[i - 1] * s_in] * 2)\n        cond_in = torch.cat([uncond, cond])\n\n        c_out, c_in = [\n            K.utils.append_dims(k, x_in.ndim) for k in dnw.get_scalings(sigma_in)\n        ]\n\n        if i == 1:\n            t = dnw.sigma_to_t(torch.cat([sigmas[i] * s_in] * 2))\n        else:\n            t = dnw.sigma_to_t(sigma_in)\n\n        eps = model.apply_model(x_in * c_in, t, cond=cond_in)\n        denoised_uncond, denoised_cond = (x_in + eps * c_out).chunk(2)\n\n        denoised = denoised_uncond + (denoised_cond - denoised_uncond) * cond_scale\n\n        if i == 1:\n            d = (x - denoised) / (2 * sigmas[i])\n        else:\n            d = (x - denoised) / sigmas[i - 1]\n\n        if generation_callback is not None:\n            generation_callback(x, i)\n\n        dt = sigmas[i] - sigmas[i - 1]\n        x = x + d * dt\n\n    return x / sigmas[-1]\n"
    },
    {
      "id": "Sygil-Dev_sygil-webui_928_15",
      "input_code": "def imgproc(\n    image,\n    image_batch,\n    imgproc_prompt,\n    imgproc_toggles,\n    imgproc_upscale_toggles,\n    imgproc_realesrgan_model_name,\n    imgproc_sampling,\n    imgproc_steps,\n    imgproc_height,\n    imgproc_width,\n    imgproc_cfg,\n    imgproc_denoising,\n    imgproc_seed,\n    imgproc_gfpgan_strength,\n    imgproc_ldsr_steps,\n    imgproc_ldsr_pre_downSample,\n    imgproc_ldsr_post_downSample,\n):\n    outpath = opt.outdir_imglab or opt.outdir or \"outputs/imglab-samples\"\n    output = []\n    images = []\n\n    def processGFPGAN(image, strength):\n        image = image.convert(\"RGB\")\n        metadata = ImageMetadata.get_from_image(image)\n        cropped_faces, restored_faces, restored_img = GFPGAN.enhance(\n            np.array(image, dtype=np.uint8),\n            has_aligned=False,\n            only_center_face=False,\n            paste_back=True,\n        )\n        result = Image.fromarray(restored_img)\n        if metadata:\n            metadata.GFPGAN = True\n            ImageMetadata.set_on_image(image, metadata)\n\n        if strength < 1.0:\n            result = Image.blend(image, result, strength)\n\n        return result\n\n    def processRealESRGAN(image):\n        if \"x2\" in imgproc_realesrgan_model_name:\n            modelMode = imgproc_realesrgan_model_name.replace(\"x2\", \"x4\")\n        else:\n            modelMode = imgproc_realesrgan_model_name\n        image = image.convert(\"RGB\")\n        metadata = ImageMetadata.get_from_image(image)\n        RealESRGAN = load_RealESRGAN(modelMode)\n        result, res = RealESRGAN.enhance(np.array(image, dtype=np.uint8))\n        result = Image.fromarray(result)\n        ImageMetadata.set_on_image(result, metadata)\n        if \"x2\" in imgproc_realesrgan_model_name:\n            result = result.resize((result.width // 2, result.height // 2), LANCZOS)\n\n        return result\n\n    def processGoBig(image):\n        metadata = ImageMetadata.get_from_image(image)\n        result = processRealESRGAN(\n            image,\n        )\n        if \"x4\" in imgproc_realesrgan_model_name:\n            result = result.resize((result.width // 2, result.height // 2), LANCZOS)\n\n        n_iter = 1\n        batch_size = 1\n        seed = seed_to_int(imgproc_seed)\n        ddim_steps = int(imgproc_steps)\n        resize_mode = 0\n        width = int(imgproc_width)\n        height = int(imgproc_height)\n        cfg_scale = float(imgproc_cfg)\n        denoising_strength = float(imgproc_denoising)\n        skip_save = True\n        skip_grid = True\n        prompt = imgproc_prompt\n        t_enc = int(denoising_strength * ddim_steps)\n        sampler_name = imgproc_sampling\n\n        if sampler_name == \"DDIM\":\n            sampler = DDIMSampler(model)\n        elif sampler_name == \"k_dpm_2_a\":\n            sampler = KDiffusionSampler(model, \"dpm_2_ancestral\")\n        elif sampler_name == \"k_dpm_2\":\n            sampler = KDiffusionSampler(model, \"dpm_2\")\n        elif sampler_name == \"k_euler_a\":\n            sampler = KDiffusionSampler(model, \"euler_ancestral\")\n        elif sampler_name == \"k_euler\":\n            sampler = KDiffusionSampler(model, \"euler\")\n        elif sampler_name == \"k_heun\":\n            sampler = KDiffusionSampler(model, \"heun\")\n        elif sampler_name == \"k_lms\":\n            sampler = KDiffusionSampler(model, \"lms\")\n        else:\n            raise Exception(\"Unknown sampler: \" + sampler_name)\n            pass\n        init_img = result\n        mask_restore = False\n        assert (\n            0.0 <= denoising_strength <= 1.0\n        ), \"can only work with strength in [0.0, 1.0]\"\n\n        def init():\n            image = init_img.convert(\"RGB\")\n            image = resize_image(resize_mode, image, width, height)\n            image = np.array(image).astype(np.float32) / 255.0\n            image = image[None].transpose(0, 3, 1, 2)\n            image = torch.from_numpy(image)\n\n            if opt.optimized:\n                modelFS.to(device)\n\n            init_image = 2.0 * image - 1.0\n            init_image = init_image.to(device)\n            init_image = repeat(init_image, \"1 ... -> b ...\", b=batch_size)\n            init_latent = (\n                model if not opt.optimized else modelFS\n            ).get_first_stage_encoding(\n                (model if not opt.optimized else modelFS).encode_first_stage(init_image)\n            )\n\n            if opt.optimized:\n                mem = torch.cuda.memory_allocated() / 1e6\n                modelFS.to(\"cpu\")\n                while torch.cuda.memory_allocated() / 1e6 >= mem:\n                    time.sleep(1)\n\n            return (init_latent,)\n\n        def sample(\n            init_data,\n            x,\n            conditioning,\n            unconditional_conditioning,\n            sampler_name,\n            img_callback: Callable = None,\n        ):\n            if sampler_name != \"DDIM\":\n                (x0,) = init_data\n\n                sigmas = sampler.model_wrap.get_sigmas(ddim_steps)\n                noise = x * sigmas[ddim_steps - t_enc - 1]\n\n                xi = x0 + noise\n                sigma_sched = sigmas[ddim_steps - t_enc - 1 :]\n                model_wrap_cfg = CFGDenoiser(sampler.model_wrap)\n                samples_ddim = K.sampling.__dict__[\n                    f\"sample_{sampler.get_sampler_name()}\"\n                ](\n                    model_wrap_cfg,\n                    xi,\n                    sigma_sched,\n                    extra_args={\n                        \"cond\": conditioning,\n                        \"uncond\": unconditional_conditioning,\n                        \"cond_scale\": cfg_scale,\n                    },\n                    disable=False,\n                    callback=partial(\n                        KDiffusionSampler.img_callback_wrapper, img_callback\n                    ),\n                )\n            else:\n                (x0,) = init_data\n                sampler.make_schedule(\n                    ddim_num_steps=ddim_steps, ddim_eta=0.0, verbose=False\n                )\n                z_enc = sampler.stochastic_encode(\n                    x0, torch.tensor([t_enc] * batch_size).to(device)\n                )\n                samples_ddim = sampler.decode(\n                    z_enc,\n                    conditioning,\n                    t_enc,\n                    unconditional_guidance_scale=cfg_scale,\n                    unconditional_conditioning=unconditional_conditioning,\n                )\n            return samples_ddim\n\n        def split_grid(image, tile_w=512, tile_h=512, overlap=64):\n            Grid = namedtuple(\n                \"Grid\", [\"tiles\", \"tile_w\", \"tile_h\", \"image_w\", \"image_h\", \"overlap\"]\n            )\n            w = image.width\n            h = image.height\n\n            now = tile_w - overlap\n            noh = tile_h - overlap\n\n            cols = math.ceil((w - overlap) / now)\n            rows = math.ceil((h - overlap) / noh)\n\n            grid = Grid([], tile_w, tile_h, w, h, overlap)\n            for row in range(rows):\n                row_images = []\n\n                y = row * noh\n\n                if y + tile_h >= h:\n                    y = h - tile_h\n\n                for col in range(cols):\n                    x = col * now\n\n                    if x + tile_w >= w:\n                        x = w - tile_w\n\n                    tile = image.crop((x, y, x + tile_w, y + tile_h))\n\n                    row_images.append([x, tile_w, tile])\n\n                grid.tiles.append([y, tile_h, row_images])\n\n            return grid\n\n        def combine_grid(grid):\n            def make_mask_image(r):\n                r = r * 255 / grid.overlap\n                r = r.astype(np.uint8)\n                return Image.fromarray(r, \"L\")\n\n            mask_w = make_mask_image(\n                np.arange(grid.overlap, dtype=np.float)\n                .reshape((1, grid.overlap))\n                .repeat(grid.tile_h, axis=0)\n            )\n            mask_h = make_mask_image(\n                np.arange(grid.overlap, dtype=np.float)\n                .reshape((grid.overlap, 1))\n                .repeat(grid.image_w, axis=1)\n            )\n\n            combined_image = Image.new(\"RGB\", (grid.image_w, grid.image_h))\n            for y, h, row in grid.tiles:\n                combined_row = Image.new(\"RGB\", (grid.image_w, h))\n                for x, w, tile in row:\n                    if x == 0:\n                        combined_row.paste(tile, (0, 0))\n                        continue\n\n                    combined_row.paste(\n                        tile.crop((0, 0, grid.overlap, h)), (x, 0), mask=mask_w\n                    )\n                    combined_row.paste(\n                        tile.crop((grid.overlap, 0, w, h)), (x + grid.overlap, 0)\n                    )\n\n                if y == 0:\n                    combined_image.paste(combined_row, (0, 0))\n                    continue\n\n                combined_image.paste(\n                    combined_row.crop((0, 0, combined_row.width, grid.overlap)),\n                    (0, y),\n                    mask=mask_h,\n                )\n                combined_image.paste(\n                    combined_row.crop((0, grid.overlap, combined_row.width, h)),\n                    (0, y + grid.overlap),\n                )\n\n            return combined_image\n\n        grid = split_grid(result, tile_w=width, tile_h=height, overlap=64)\n        work = []\n        work_results = []\n\n        for y, h, row in grid.tiles:\n            for tiledata in row:\n                work.append(tiledata[2])\n        batch_count = math.ceil(len(work) / batch_size)\n        print(\n            f\"GoBig upscaling will process a total of {len(work)} images tiled as {len(grid.tiles[0][2])}x{len(grid.tiles)} in a total of {batch_count} batches.\"\n        )\n        for i in range(batch_count):\n            init_img = work[i * batch_size : (i + 1) * batch_size][0]\n            output_images, seed, info, stats = process_images(\n                outpath=outpath,\n                func_init=init,\n                func_sample=sample,\n                prompt=prompt,\n                seed=seed,\n                sampler_name=sampler_name,\n                skip_save=skip_save,\n                skip_grid=skip_grid,\n                batch_size=batch_size,\n                n_iter=n_iter,\n                steps=ddim_steps,\n                cfg_scale=cfg_scale,\n                width=width,\n                height=height,\n                prompt_matrix=None,\n                filter_nsfw=False,\n                use_GFPGAN=None,\n                use_RealESRGAN=None,\n                realesrgan_model_name=None,\n                fp=None,\n                normalize_prompt_weights=False,\n                init_img=init_img,\n                init_mask=None,\n                keep_mask=False,\n                mask_blur_strength=None,\n                denoising_strength=denoising_strength,\n                mask_restore=mask_restore,\n                resize_mode=resize_mode,\n                uses_loopback=False,\n                sort_samples=True,\n                write_info_files=True,\n                write_sample_info_to_log_file=False,\n                jpg_sample=False,\n                imgProcessorTask=True,\n            )\n\n            work_results.append(output_images[0])\n        image_index = 0\n        for y, h, row in grid.tiles:\n            for tiledata in row:\n                tiledata[2] = work_results[image_index]\n                image_index += 1\n\n        combined_image = combine_grid(grid)\n        len(os.listdir(outpath)) - 1\n        del sampler\n\n        torch.cuda.empty_cache()\n        ImageMetadata.set_on_image(combined_image, metadata)\n        return combined_image\n\n    def processLDSR(image):\n        metadata = ImageMetadata.get_from_image(image)\n        result = LDSR.superResolution(\n            image,\n            int(imgproc_ldsr_steps),\n            str(imgproc_ldsr_pre_downSample),\n            str(imgproc_ldsr_post_downSample),\n        )\n        ImageMetadata.set_on_image(result, metadata)\n        return result\n\n    if image_batch is not None:\n        if image is not None:\n            print(\n                \"Batch detected and single image detected, please only use one of the two. Aborting.\"\n            )\n            return None\n        for img in image_batch:\n            image = Image.fromarray(np.array(Image.open(img)))\n            images.append(image)\n\n    elif image is not None:\n        if image_batch is not None:\n            print(\n                \"Batch detected and single image detected, please only use one of the two. Aborting.\"\n            )\n            return None\n        else:\n            images.append(image)\n\n    if len(images) > 0:\n        print(\"Processing images...\")\n        if 0 in imgproc_toggles:\n            ModelLoader([\"RealESGAN\", \"LDSR\"], False, True)\n            ModelLoader([\"GFPGAN\"], True, False)\n        if 1 in imgproc_toggles:\n            if imgproc_upscale_toggles == 0:\n                ModelLoader([\"GFPGAN\", \"LDSR\"], False, True)\n                ModelLoader(\n                    [\"RealESGAN\"], True, False, imgproc_realesrgan_model_name\n                )\n            elif imgproc_upscale_toggles == 1:\n                ModelLoader([\"GFPGAN\", \"LDSR\"], False, True)\n                ModelLoader([\"RealESGAN\", \"model\"], True, False)\n            elif imgproc_upscale_toggles == 2:\n                ModelLoader(\n                    [\"model\", \"GFPGAN\", \"RealESGAN\"], False, True\n                )\n                ModelLoader([\"LDSR\"], True, False)\n            elif imgproc_upscale_toggles == 3:\n                ModelLoader([\"GFPGAN\", \"LDSR\"], False, True)\n                ModelLoader(\n                    [\"RealESGAN\", \"model\"], True, False, imgproc_realesrgan_model_name\n                )\n        for image in images:\n            metadata = ImageMetadata.get_from_image(image)\n            if 0 in imgproc_toggles:\n                ModelLoader([\"GFPGAN\"], True, False)\n                image = processGFPGAN(image, imgproc_gfpgan_strength)\n                if metadata:\n                    metadata.GFPGAN = True\n                ImageMetadata.set_on_image(image, metadata)\n                outpathDir = os.path.join(outpath, \"GFPGAN\")\n                os.makedirs(outpathDir, exist_ok=True)\n                batchNumber = get_next_sequence_number(outpathDir)\n                outFilename = str(batchNumber) + \"-\" + \"result\"\n\n                if 1 not in imgproc_toggles:\n                    output.append(image)\n                    save_sample(\n                        image,\n                        outpathDir,\n                        outFilename,\n                        False,\n                        None,\n                        None,\n                        None,\n                        None,\n                        None,\n                        False,\n                        None,\n                        None,\n                        None,\n                        None,\n                        None,\n                        None,\n                        None,\n                        None,\n                        None,\n                        False,\n                    )\n            if 1 in imgproc_toggles:\n                if imgproc_upscale_toggles == 0:\n                    image = processRealESRGAN(image)\n                    ImageMetadata.set_on_image(image, metadata)\n                    outpathDir = os.path.join(outpath, \"RealESRGAN\")\n                    os.makedirs(outpathDir, exist_ok=True)\n                    batchNumber = get_next_sequence_number(outpathDir)\n                    outFilename = str(batchNumber) + \"-\" + \"result\"\n                    output.append(image)\n                    save_sample(\n                        image,\n                        outpathDir,\n                        outFilename,\n                        False,\n                        None,\n                        None,\n                        None,\n                        None,\n                        None,\n                        False,\n                        None,\n                        None,\n                        None,\n                        None,\n                        None,\n                        None,\n                        None,\n                        None,\n                        None,\n                        False,\n                    )\n\n                elif imgproc_upscale_toggles == 1:\n                    image = processGoBig(image)\n                    ImageMetadata.set_on_image(image, metadata)\n                    outpathDir = os.path.join(outpath, \"GoBig\")\n                    os.makedirs(outpathDir, exist_ok=True)\n                    batchNumber = get_next_sequence_number(outpathDir)\n                    outFilename = str(batchNumber) + \"-\" + \"result\"\n                    output.append(image)\n                    save_sample(\n                        image,\n                        outpathDir,\n                        outFilename,\n                        False,\n                        None,\n                        None,\n                        None,\n                        None,\n                        None,\n                        False,\n                        None,\n                        None,\n                        None,\n                        None,\n                        None,\n                        None,\n                        None,\n                        None,\n                        None,\n                        False,\n                    )\n\n                elif imgproc_upscale_toggles == 2:\n                    image = processLDSR(image)\n                    ImageMetadata.set_on_image(image, metadata)\n                    outpathDir = os.path.join(outpath, \"LDSR\")\n                    os.makedirs(outpathDir, exist_ok=True)\n                    batchNumber = get_next_sequence_number(outpathDir)\n                    outFilename = str(batchNumber) + \"-\" + \"result\"\n                    output.append(image)\n                    save_sample(\n                        image,\n                        outpathDir,\n                        outFilename,\n                        False,\n                        None,\n                        None,\n                        None,\n                        None,\n                        None,\n                        False,\n                        None,\n                        None,\n                        None,\n                        None,\n                        None,\n                        None,\n                        None,\n                        None,\n                        None,\n                        False,\n                    )\n\n                elif imgproc_upscale_toggles == 3:\n                    image = processGoBig(image)\n                    ModelLoader(\n                        [\"model\", \"GFPGAN\", \"RealESGAN\"], False, True\n                    )\n                    ModelLoader([\"LDSR\"], True, False)\n                    image = processLDSR(image)\n                    ImageMetadata.set_on_image(image, metadata)\n                    outpathDir = os.path.join(outpath, \"GoLatent\")\n                    os.makedirs(outpathDir, exist_ok=True)\n                    batchNumber = get_next_sequence_number(outpathDir)\n                    outFilename = str(batchNumber) + \"-\" + \"result\"\n                    output.append(image)\n\n                    save_sample(\n                        image,\n                        outpathDir,\n                        outFilename,\n                        None,\n                        None,\n                        None,\n                        None,\n                        None,\n                        None,\n                        False,\n                        None,\n                        None,\n                        None,\n                        None,\n                        None,\n                        None,\n                        None,\n                        None,\n                        None,\n                        False,\n                    )\n\n    print(\"Done.\")\n    return output\n"
    },
    {
      "id": "Sygil-Dev_sygil-webui_928_12",
      "input_code": "    def sample(\n        self,\n        S,\n        conditioning,\n        batch_size,\n        shape,\n        verbose,\n        unconditional_guidance_scale,\n        unconditional_conditioning,\n        eta,\n        x_T,\n        img_callback=None,\n        log_every_t=None,\n    ):\n        sigmas = self.model_wrap.get_sigmas(S)\n        x = x_T * sigmas[0]\n        model_wrap_cfg = CFGDenoiser(self.model_wrap)\n        samples_ddim = None\n        samples_ddim = K.sampling.__dict__[f\"sample_{self.schedule}\"](\n            model_wrap_cfg,\n            x,\n            sigmas,\n            extra_args={\n                \"cond\": conditioning,\n                \"uncond\": unconditional_conditioning,\n                \"cond_scale\": unconditional_guidance_scale,\n            },\n            disable=False,\n            callback=generation_callback,\n        )\n        return samples_ddim, None\n"
    },
    {
      "id": "Sygil-Dev_sygil-webui_928_6",
      "input_code": "    def dpm_2_ancestral_sampling(\n        self,\n        ac,\n        x,\n        S,\n        cond,\n        unconditional_conditioning=None,\n        unconditional_guidance_scale=1,\n        extra_args=None,\n        callback=None,\n        disable=None,\n    ):\n        \n        extra_args = {} if extra_args is None else extra_args\n\n        cvd = CompVisDenoiser(ac)\n        sigmas = cvd.get_sigmas(S)\n        x = x * sigmas[0]\n\n        s_in = x.new_ones([x.shape[0]]).half()\n        for i in trange(len(sigmas) - 1, disable=disable):\n            s_i = sigmas[i] * s_in\n            x_in = torch.cat([x] * 2)\n            t_in = torch.cat([s_i] * 2)\n            cond_in = torch.cat([unconditional_conditioning, cond])\n            c_out, c_in = [\n                append_dims(tmp, x_in.ndim) for tmp in cvd.get_scalings(t_in)\n            ]\n            eps = self.apply_model(x_in * c_in, cvd.sigma_to_t(t_in), cond_in)\n            e_t_uncond, e_t = (x_in + eps * c_out).chunk(2)\n            denoised = e_t_uncond + unconditional_guidance_scale * (e_t - e_t_uncond)\n\n            sigma_down, sigma_up = get_ancestral_step(sigmas[i], sigmas[i + 1])\n            if callback is not None:\n                callback(\n                    {\n                        \"x\": x,\n                        \"i\": i,\n                        \"sigma\": sigmas[i],\n                        \"sigma_hat\": sigmas[i],\n                        \"denoised\": denoised,\n                    }\n                )\n            d = to_d(x, sigmas[i], denoised)\n            sigma_mid = ((sigmas[i] ** (1 / 3) + sigma_down ** (1 / 3)) / 2) ** 3\n            dt_1 = sigma_mid - sigmas[i]\n            dt_2 = sigma_down - sigmas[i]\n            x_2 = x + d * dt_1\n\n            s_i = sigma_mid * s_in\n            x_in = torch.cat([x_2] * 2)\n            t_in = torch.cat([s_i] * 2)\n            cond_in = torch.cat([unconditional_conditioning, cond])\n            c_out, c_in = [\n                append_dims(tmp, x_in.ndim) for tmp in cvd.get_scalings(t_in)\n            ]\n            eps = self.apply_model(x_in * c_in, cvd.sigma_to_t(t_in), cond_in)\n            e_t_uncond, e_t = (x_in + eps * c_out).chunk(2)\n            denoised_2 = e_t_uncond + unconditional_guidance_scale * (e_t - e_t_uncond)\n\n            d_2 = to_d(x_2, sigma_mid, denoised_2)\n            x = x + d_2 * dt_2\n            x = x + torch.randn_like(x) * sigma_up\n        return x\n"
    },
    {
      "id": "Sygil-Dev_sygil-webui_928_5",
      "input_code": "    def dpm_2_sampling(\n        self,\n        ac,\n        x,\n        S,\n        cond,\n        unconditional_conditioning=None,\n        unconditional_guidance_scale=1,\n        extra_args=None,\n        callback=None,\n        disable=None,\n        s_churn=0.0,\n        s_tmin=0.0,\n        s_tmax=float(\"inf\"),\n        s_noise=1.0,\n    ):\n        \n        extra_args = {} if extra_args is None else extra_args\n\n        cvd = CompVisDenoiser(ac)\n        sigmas = cvd.get_sigmas(S)\n        x = x * sigmas[0]\n\n        s_in = x.new_ones([x.shape[0]]).half()\n        for i in trange(len(sigmas) - 1, disable=disable):\n            gamma = (\n                min(s_churn / (len(sigmas) - 1), 2**0.5 - 1)\n                if s_tmin <= sigmas[i] <= s_tmax\n                else 0.0\n            )\n            eps = torch.randn_like(x) * s_noise\n            sigma_hat = sigmas[i] * (gamma + 1)\n            if gamma > 0:\n                x = x + eps * (sigma_hat**2 - sigmas[i] ** 2) ** 0.5\n\n            s_i = sigma_hat * s_in\n            x_in = torch.cat([x] * 2)\n            t_in = torch.cat([s_i] * 2)\n            cond_in = torch.cat([unconditional_conditioning, cond])\n            c_out, c_in = [\n                append_dims(tmp, x_in.ndim) for tmp in cvd.get_scalings(t_in)\n            ]\n            eps = self.apply_model(x_in * c_in, cvd.sigma_to_t(t_in), cond_in)\n            e_t_uncond, e_t = (x_in + eps * c_out).chunk(2)\n            denoised = e_t_uncond + unconditional_guidance_scale * (e_t - e_t_uncond)\n\n            d = to_d(x, sigma_hat, denoised)\n            sigma_mid = ((sigma_hat ** (1 / 3) + sigmas[i + 1] ** (1 / 3)) / 2) ** 3\n            dt_1 = sigma_mid - sigma_hat\n            dt_2 = sigmas[i + 1] - sigma_hat\n            x_2 = x + d * dt_1\n\n            s_i = sigma_mid * s_in\n            x_in = torch.cat([x_2] * 2)\n            t_in = torch.cat([s_i] * 2)\n            cond_in = torch.cat([unconditional_conditioning, cond])\n            c_out, c_in = [\n                append_dims(tmp, x_in.ndim) for tmp in cvd.get_scalings(t_in)\n            ]\n            eps = self.apply_model(x_in * c_in, cvd.sigma_to_t(t_in), cond_in)\n            e_t_uncond, e_t = (x_in + eps * c_out).chunk(2)\n            denoised_2 = e_t_uncond + unconditional_guidance_scale * (e_t - e_t_uncond)\n\n            d_2 = to_d(x_2, sigma_mid, denoised_2)\n            x = x + d_2 * dt_2\n        return x\n"
    },
    {
      "id": "Sygil-Dev_sygil-webui_928_9",
      "input_code": "    def sample(\n        self,\n        S,\n        conditioning,\n        batch_size,\n        shape,\n        verbose,\n        unconditional_guidance_scale,\n        unconditional_conditioning,\n        eta,\n        x_T,\n        img_callback=None,\n        log_every_t=None,\n    ):\n        sigmas = self.model_wrap.get_sigmas(S)\n        x = x_T * sigmas[0]\n        model_wrap_cfg = CFGDenoiser(self.model_wrap)\n        samples_ddim = None\n        samples_ddim = K.sampling.__dict__[f\"sample_{self.schedule}\"](\n            model_wrap_cfg,\n            x,\n            sigmas,\n            extra_args={\n                \"cond\": conditioning,\n                \"uncond\": unconditional_conditioning,\n                \"cond_scale\": unconditional_guidance_scale,\n            },\n            disable=False,\n            callback=generation_callback,\n        )\n        return samples_ddim, None\n"
    },
    {
      "id": "Sygil-Dev_sygil-webui_928_8",
      "input_code": "def find_noise_for_image(\n    model,\n    device,\n    init_image,\n    prompt,\n    steps=200,\n    cond_scale=2.0,\n    verbose=False,\n    normalize=False,\n    generation_callback=None,\n):\n    image = np.array(init_image).astype(np.float32) / 255.0\n    image = image[None].transpose(0, 3, 1, 2)\n    image = torch.from_numpy(image)\n    image = 2.0 * image - 1.0\n    image = image.to(device)\n    x = model.get_first_stage_encoding(model.encode_first_stage(image))\n\n    uncond = model.get_learned_conditioning([\"\"])\n    cond = model.get_learned_conditioning([prompt])\n\n    s_in = x.new_ones([x.shape[0]])\n    dnw = K.external.CompVisDenoiser(model)\n    sigmas = dnw.get_sigmas(steps).flip(0)\n\n    if verbose:\n        logger.info(sigmas)\n\n    for i in trange(1, len(sigmas)):\n        x_in = torch.cat([x] * 2)\n        sigma_in = torch.cat([sigmas[i - 1] * s_in] * 2)\n        cond_in = torch.cat([uncond, cond])\n\n        c_out, c_in = [\n            K.utils.append_dims(k, x_in.ndim) for k in dnw.get_scalings(sigma_in)\n        ]\n\n        if i == 1:\n            t = dnw.sigma_to_t(torch.cat([sigmas[i] * s_in] * 2))\n        else:\n            t = dnw.sigma_to_t(sigma_in)\n\n        eps = model.apply_model(x_in * c_in, t, cond=cond_in)\n        denoised_uncond, denoised_cond = (x_in + eps * c_out).chunk(2)\n\n        denoised = denoised_uncond + (denoised_cond - denoised_uncond) * cond_scale\n\n        if i == 1:\n            d = (x - denoised) / (2 * sigmas[i])\n        else:\n            d = (x - denoised) / sigmas[i - 1]\n\n        if generation_callback is not None:\n            generation_callback(x, i)\n\n        dt = sigmas[i] - sigmas[i - 1]\n        x = x + d * dt\n\n    return x / sigmas[-1]\n"
    },
    {
      "id": "Sygil-Dev_sygil-webui_928_1",
      "input_code": "    def sample(\n        self,\n        S,\n        conditioning,\n        unconditional_guidance_scale,\n        unconditional_conditioning,\n        x_T,\n    ):\n        sigmas = self.model_wrap.get_sigmas(S)\n        x = x_T * sigmas[0]\n        model_wrap_cfg = CFGDenoiser(self.model_wrap)\n        samples_ddim = None\n        samples_ddim = K.sampling.__dict__[f\"sample_{self.schedule}\"](\n            model_wrap_cfg,\n            x,\n            sigmas,\n            extra_args={\n                \"cond\": conditioning,\n                \"uncond\": unconditional_conditioning,\n                \"cond_scale\": unconditional_guidance_scale,\n            },\n            disable=False,\n            callback=self.generation_callback,\n        )\n        return samples_ddim, None\n"
    },
    {
      "id": "Sygil-Dev_sygil-webui_928_10",
      "input_code": "def img2img(\n    prompt: str = \"\",\n    init_info: any = None,\n    init_info_mask: any = None,\n    mask_mode: int = 0,\n    mask_blur_strength: int = 3,\n    mask_restore: bool = False,\n    ddim_steps: int = 50,\n    sampler_name: str = \"DDIM\",\n    n_iter: int = 1,\n    cfg_scale: float = 7.5,\n    denoising_strength: float = 0.8,\n    seed: int = -1,\n    noise_mode: int = 0,\n    find_noise_steps: str = \"\",\n    height: int = 512,\n    width: int = 512,\n    resize_mode: int = 0,\n    fp=None,\n    variant_amount: float = 0.0,\n    variant_seed: int = None,\n    ddim_eta: float = 0.0,\n    write_info_files: bool = True,\n    separate_prompts: bool = False,\n    normalize_prompt_weights: bool = True,\n    save_individual_images: bool = True,\n    save_grid: bool = True,\n    group_by_prompt: bool = True,\n    save_as_jpg: bool = True,\n    use_GFPGAN: bool = True,\n    GFPGAN_model: str = \"GFPGANv1.4\",\n    use_RealESRGAN: bool = True,\n    RealESRGAN_model: str = \"RealESRGAN_x4plus_anime_6B\",\n    use_LDSR: bool = True,\n    LDSR_model: str = \"model\",\n    loopback: bool = False,\n    random_seed_loopback: bool = False,\n):\n    outpath = st.session_state[\"defaults\"].general.outdir_img2img\n    seed = seed_to_int(seed)\n\n    batch_size = 1\n\n    if sampler_name == \"PLMS\":\n        sampler = PLMSSampler(server_state[\"model\"])\n    elif sampler_name == \"DDIM\":\n        sampler = DDIMSampler(server_state[\"model\"])\n    elif sampler_name == \"k_dpm_2_a\":\n        sampler = KDiffusionSampler(server_state[\"model\"], \"dpm_2_ancestral\")\n    elif sampler_name == \"k_dpm_2\":\n        sampler = KDiffusionSampler(server_state[\"model\"], \"dpm_2\")\n    elif sampler_name == \"k_dpmpp_2m\":\n        sampler = KDiffusionSampler(server_state[\"model\"], \"dpmpp_2m\")\n    elif sampler_name == \"k_euler_a\":\n        sampler = KDiffusionSampler(server_state[\"model\"], \"euler_ancestral\")\n    elif sampler_name == \"k_euler\":\n        sampler = KDiffusionSampler(server_state[\"model\"], \"euler\")\n    elif sampler_name == \"k_heun\":\n        sampler = KDiffusionSampler(server_state[\"model\"], \"heun\")\n    elif sampler_name == \"k_lms\":\n        sampler = KDiffusionSampler(server_state[\"model\"], \"lms\")\n    else:\n        raise Exception(\"Unknown sampler: \" + sampler_name)\n\n    def process_init_mask(init_mask: Image):\n        if init_mask.mode == \"RGBA\":\n            init_mask = init_mask.convert(\"RGBA\")\n            background = Image.new(\"RGBA\", init_mask.size, (0, 0, 0))\n            init_mask = Image.alpha_composite(background, init_mask)\n            init_mask = init_mask.convert(\"RGB\")\n        return init_mask\n\n    init_img = init_info\n    init_mask = None\n    if mask_mode == 0:\n        if init_info_mask:\n            init_mask = process_init_mask(init_info_mask)\n    elif mask_mode == 1:\n        if init_info_mask:\n            init_mask = process_init_mask(init_info_mask)\n            init_mask = ImageOps.invert(init_mask)\n    elif mask_mode == 2:\n        init_img_transparency = init_img.split()[-1].convert(\n            \"L\"\n        )\n        init_mask = init_img_transparency\n        init_mask = init_mask.convert(\"RGB\")\n        init_mask = resize_image(resize_mode, init_mask, width, height)\n        init_mask = init_mask.convert(\"RGB\")\n\n    assert 0.0 <= denoising_strength <= 1.0, \"can only work with strength in [0.0, 1.0]\"\n    t_enc = int(denoising_strength * ddim_steps)\n\n    if (\n        init_mask is not None\n        and (noise_mode == 2 or noise_mode == 3)\n        and init_img is not None\n    ):\n        noise_q = 0.99\n        color_variation = 0.0\n        mask_blend_factor = 1.0\n\n        np_init = (np.asarray(init_img.convert(\"RGB\")) / 255.0).astype(\n            np.float64\n        )\n        np_mask_rgb = 1.0 - (\n            np.asarray(ImageOps.invert(init_mask).convert(\"RGB\")) / 255.0\n        ).astype(np.float64)\n        np_mask_rgb -= np.min(np_mask_rgb)\n        np_mask_rgb /= np.max(np_mask_rgb)\n        np_mask_rgb = 1.0 - np_mask_rgb\n        np_mask_rgb_hardened = 1.0 - (np_mask_rgb < 0.99).astype(np.float64)\n        blurred = skimage.filters.gaussian(\n            np_mask_rgb_hardened[:], sigma=16.0, channel_axis=2, truncate=32.0\n        )\n        blurred2 = skimage.filters.gaussian(\n            np_mask_rgb_hardened[:], sigma=16.0, channel_axis=2, truncate=32.0\n        )\n        np_mask_rgb_dilated = np.clip((np_mask_rgb + blurred2) * 0.7071, 0.0, 1.0)\n        np_mask_rgb = np.clip((np_mask_rgb + blurred) * 0.7071, 0.0, 1.0)\n\n        noise_rgb = get_matched_noise(np_init, np_mask_rgb, noise_q, color_variation)\n        blend_mask_rgb = np.clip(np_mask_rgb_dilated, 0.0, 1.0) ** (mask_blend_factor)\n        noised = noise_rgb[:]\n        blend_mask_rgb **= 2.0\n        noised = np_init[:] * (1.0 - blend_mask_rgb) + noised * blend_mask_rgb\n\n        np_mask_grey = np.sum(np_mask_rgb, axis=2) / 3.0\n        ref_mask = np_mask_grey < 1e-3\n\n        all_mask = np.ones((height, width), dtype=bool)\n        noised[all_mask, :] = skimage.exposure.match_histograms(\n            noised[all_mask, :] ** 1.0, noised[ref_mask, :], channel_axis=1\n        )\n\n        init_img = Image.fromarray(\n            np.clip(noised * 255.0, 0.0, 255.0).astype(np.uint8), mode=\"RGB\"\n        )\n        st.session_state[\"editor_image\"].image(init_img)\n\n    def init():\n        image = init_img.convert(\"RGB\")\n        image = np.array(image).astype(np.float32) / 255.0\n        image = image[None].transpose(0, 3, 1, 2)\n        image = torch.from_numpy(image)\n\n        mask_channel = None\n        if init_mask:\n            alpha = resize_image(resize_mode, init_mask, width // 8, height // 8)\n            mask_channel = alpha.split()[-1]\n\n        mask = None\n        if mask_channel is not None:\n            mask = np.array(mask_channel).astype(np.float32) / 255.0\n            mask = 1 - mask\n            mask = np.tile(mask, (4, 1, 1))\n            mask = mask[None].transpose(0, 1, 2, 3)\n            mask = torch.from_numpy(mask).to(server_state[\"device\"])\n\n        if st.session_state[\"defaults\"].general.optimized:\n            server_state[\"modelFS\"].to(server_state[\"device\"])\n\n        init_image = 2.0 * image - 1.0\n        init_image = init_image.to(server_state[\"device\"])\n        init_latent = (\n            server_state[\"model\"]\n            if not st.session_state[\"defaults\"].general.optimized\n            else server_state[\"modelFS\"]\n        ).get_first_stage_encoding(\n            (\n                server_state[\"model\"]\n                if not st.session_state[\"defaults\"].general.optimized\n                else server_state[\"modelFS\"]\n            ).encode_first_stage(init_image)\n        )\n\n        if st.session_state[\"defaults\"].general.optimized:\n            mem = torch.cuda.memory_allocated() / 1e6\n            server_state[\"modelFS\"].to(\"cpu\")\n            while torch.cuda.memory_allocated() / 1e6 >= mem:\n                time.sleep(1)\n\n        return (\n            init_latent,\n            mask,\n        )\n\n    def sample(init_data, x, conditioning, unconditional_conditioning, sampler_name):\n        t_enc_steps = t_enc\n        obliterate = False\n        if ddim_steps == t_enc_steps:\n            t_enc_steps = t_enc_steps - 1\n            obliterate = True\n\n        if sampler_name != \"DDIM\":\n            x0, z_mask = init_data\n\n            sigmas = sampler.model_wrap.get_sigmas(ddim_steps)\n            noise = x * sigmas[ddim_steps - t_enc_steps - 1]\n\n            xi = x0 + noise\n\n            if z_mask is not None and obliterate:\n                random = torch.randn(z_mask.shape, device=xi.device)\n                xi = (z_mask * noise) + ((1 - z_mask) * xi)\n\n            sigma_sched = sigmas[ddim_steps - t_enc_steps - 1 :]\n            model_wrap_cfg = CFGMaskedDenoiser(sampler.model_wrap)\n            samples_ddim = K.sampling.__dict__[f\"sample_{sampler.get_sampler_name()}\"](\n                model_wrap_cfg,\n                xi,\n                sigma_sched,\n                extra_args={\n                    \"cond\": conditioning,\n                    \"uncond\": unconditional_conditioning,\n                    \"cond_scale\": cfg_scale,\n                    \"mask\": z_mask,\n                    \"x0\": x0,\n                    \"xi\": xi,\n                },\n                disable=False,\n                callback=generation_callback if not server_state[\"bridge\"] else None,\n            )\n        else:\n            x0, z_mask = init_data\n\n            sampler.make_schedule(\n                ddim_num_steps=ddim_steps, ddim_eta=0.0, verbose=False\n            )\n            z_enc = sampler.stochastic_encode(\n                x0, torch.tensor([t_enc_steps] * batch_size).to(server_state[\"device\"])\n            )\n\n            if z_mask is not None and obliterate:\n                random = torch.randn(z_mask.shape, device=z_enc.device)\n                z_enc = (z_mask * random) + ((1 - z_mask) * z_enc)\n\n            samples_ddim = sampler.decode(\n                z_enc,\n                conditioning,\n                t_enc_steps,\n                unconditional_guidance_scale=cfg_scale,\n                unconditional_conditioning=unconditional_conditioning,\n                z_mask=z_mask,\n                x0=x0,\n            )\n        return samples_ddim\n\n    if loopback:\n        output_images, info = None, None\n        history = []\n        initial_seed = None\n\n        do_color_correction = False\n        try:\n            from skimage import exposure\n\n            do_color_correction = True\n        except:\n            logger.error(\"Install scikit-image to perform color correction on loopback\")\n\n        for i in range(n_iter):\n            if do_color_correction and i == 0:\n                correction_target = cv2.cvtColor(\n                    np.asarray(init_img.copy()), cv2.COLOR_RGB2LAB\n                )\n\n            is_final_iteration = i == n_iter - 1\n\n            output_images, seed, info, stats = process_images(\n                outpath=outpath,\n                func_init=init,\n                func_sample=sample,\n                prompt=prompt,\n                seed=seed,\n                sampler_name=sampler_name,\n                save_grid=save_grid,\n                batch_size=1,\n                n_iter=1,\n                steps=ddim_steps,\n                cfg_scale=cfg_scale,\n                width=width,\n                height=height,\n                prompt_matrix=separate_prompts,\n                use_GFPGAN=use_GFPGAN,\n                GFPGAN_model=GFPGAN_model,\n                use_RealESRGAN=use_RealESRGAN\n                and is_final_iteration,                realesrgan_model_name=RealESRGAN_model,\n                use_LDSR=use_LDSR,\n                LDSR_model_name=LDSR_model,\n                normalize_prompt_weights=normalize_prompt_weights,\n                save_individual_images=save_individual_images,\n                init_img=init_img,\n                init_mask=init_mask,\n                mask_blur_strength=mask_blur_strength,\n                mask_restore=mask_restore,\n                denoising_strength=denoising_strength,\n                noise_mode=noise_mode,\n                find_noise_steps=find_noise_steps,\n                resize_mode=resize_mode,\n                uses_loopback=loopback,\n                uses_random_seed_loopback=random_seed_loopback,\n                sort_samples=group_by_prompt,\n                write_info_files=write_info_files,\n                jpg_sample=save_as_jpg,\n            )\n\n            if initial_seed is None:\n                initial_seed = seed\n\n            input_image = init_img\n            init_img = output_images[0]\n\n            if do_color_correction and correction_target is not None:\n                init_img = Image.fromarray(\n                    cv2.cvtColor(\n                        exposure.match_histograms(\n                            cv2.cvtColor(np.asarray(init_img), cv2.COLOR_RGB2LAB),\n                            correction_target,\n                            channel_axis=2,\n                        ),\n                        cv2.COLOR_LAB2RGB,\n                    ).astype(\"uint8\")\n                )\n                if mask_restore is True and init_mask is not None:\n                    color_mask = init_mask.filter(\n                        ImageFilter.GaussianBlur(mask_blur_strength)\n                    )\n                    color_mask = color_mask.convert(\"L\")\n                    source_image = input_image.convert(\"RGB\")\n                    target_image = init_img.convert(\"RGB\")\n\n                    init_img = Image.composite(source_image, target_image, color_mask)\n\n            if not random_seed_loopback:\n                seed = seed + 1\n            else:\n                seed = seed_to_int(None)\n\n            denoising_strength = max(denoising_strength * 0.95, 0.1)\n            history.append(init_img)\n\n        output_images = history\n        seed = initial_seed\n\n    else:\n        output_images, seed, info, stats = process_images(\n            outpath=outpath,\n            func_init=init,\n            func_sample=sample,\n            prompt=prompt,\n            seed=seed,\n            sampler_name=sampler_name,\n            save_grid=save_grid,\n            batch_size=batch_size,\n            n_iter=n_iter,\n            steps=ddim_steps,\n            cfg_scale=cfg_scale,\n            width=width,\n            height=height,\n            prompt_matrix=separate_prompts,\n            use_GFPGAN=use_GFPGAN,\n            GFPGAN_model=GFPGAN_model,\n            use_RealESRGAN=use_RealESRGAN,\n            realesrgan_model_name=RealESRGAN_model,\n            use_LDSR=use_LDSR,\n            LDSR_model_name=LDSR_model,\n            normalize_prompt_weights=normalize_prompt_weights,\n            save_individual_images=save_individual_images,\n            init_img=init_img,\n            init_mask=init_mask,\n            mask_blur_strength=mask_blur_strength,\n            denoising_strength=denoising_strength,\n            noise_mode=noise_mode,\n            find_noise_steps=find_noise_steps,\n            mask_restore=mask_restore,\n            resize_mode=resize_mode,\n            uses_loopback=loopback,\n            sort_samples=group_by_prompt,\n            write_info_files=write_info_files,\n            jpg_sample=save_as_jpg,\n        )\n\n    del sampler\n\n    return output_images, seed, info, stats\n"
    }
  ],
  "generated_inputs": [
    "<needimport>\nimport torch\n\n<testcase_1>\nself.sigmas = torch.tensor([0.1, 0.2, 0.3, 0.4, 0.5])\nself.t_to_sigma = lambda t: t * 0.1\nn = 5\n\n<testcase_2>\nself.sigmas = torch.tensor([0.05, 0.1, 0.15, 0.2, 0.25, 0.3])\nself.t_to_sigma = lambda t: t * 0.05\nn = 10\n\n<testcase_3>\nself.sigmas = torch.tensor([0.01, 0.02, 0.03, 0.04, 0.05, 0.06, 0.07])\nself.t_to_sigma = lambda t: t * 0.01\nn = None\n\n<testcase_4>\nself.sigmas = torch.tensor([0.005, 0.01, 0.015, 0.02, 0.025, 0.03, 0.035, 0.04])\nself.t_to_sigma = lambda t: t * 0.005\nn = 8\n\n<testcase_5>\nself.sigmas = torch.tensor([0.001, 0.002, 0.003, 0.004, 0.005, 0.006, 0.007, 0.008, 0.009, 0.01])\nself.t_to_sigma = lambda t: t * 0.001\nn = 20"
  ],
  "gpt_output": "<needimport>\nimport torch\n\n<testcase_1>\nself.sigmas = torch.tensor([0.1, 0.2, 0.3, 0.4, 0.5])\nself.t_to_sigma = lambda t: t * 0.1\nn = 5\n\n<testcase_2>\nself.sigmas = torch.tensor([0.05, 0.1, 0.15, 0.2, 0.25, 0.3])\nself.t_to_sigma = lambda t: t * 0.05\nn = 10\n\n<testcase_3>\nself.sigmas = torch.tensor([0.01, 0.02, 0.03, 0.04, 0.05, 0.06, 0.07])\nself.t_to_sigma = lambda t: t * 0.01\nn = None\n\n<testcase_4>\nself.sigmas = torch.tensor([0.005, 0.01, 0.015, 0.02, 0.025, 0.03, 0.035, 0.04])\nself.t_to_sigma = lambda t: t * 0.005\nn = 8\n\n<testcase_5>\nself.sigmas = torch.tensor([0.001, 0.002, 0.003, 0.004, 0.005, 0.006, 0.007, 0.008, 0.009, 0.01])\nself.t_to_sigma = lambda t: t * 0.001\nn = 20",
  "needimport": "import torch\n\n",
  "test_results": {
    "ans1": [
      0.0,
      0.4000000059604645,
      0.30000001192092896,
      0.20000000298023224,
      0.10000000149011612,
      0.0
    ],
    "ans2": [
      0.0,
      0.25,
      0.222222238779068,
      0.1944444477558136,
      0.1666666716337204,
      0.138888880610466,
      0.111111119389534,
      0.0833333358168602,
      0.055555559694767,
      0.0277777798473835,
      0.0
    ],
    "ans3": [
      0.0,
      0.07000000029802322,
      0.05999999865889549,
      0.05000000074505806,
      0.03999999910593033,
      0.029999999329447746,
      0.019999999552965164,
      0.009999999776482582
    ],
    "ans4": [
      0.0,
      0.03500000014901161,
      0.029999999329447746,
      0.02499999850988388,
      0.019999999552965164,
      0.014999999664723873,
      0.009999999776482582,
      0.004999999888241291,
      0.0
    ],
    "ans5": [
      0.0,
      0.009000000543892384,
      0.008526315912604332,
      0.008052632212638855,
      0.007578947581350803,
      0.007105263415724039,
      0.006631579250097275,
      0.0061578950844705105,
      0.005684210918843746,
      0.005210526753216982,
      0.004736842587590218,
      0.004263157956302166,
      0.0037894740235060453,
      0.0033157896250486374,
      0.002842105459421873,
      0.002368421293795109,
      0.0018947370117530227,
      0.0014210527297109365,
      0.0009473685058765113,
      0.00047368425293825567,
      0.0
    ]
  }
}