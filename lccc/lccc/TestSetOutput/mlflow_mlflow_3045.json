{
  "input_header": "def _find_mlproject(directory):",
  "input_docstring": null,
  "output_code": "def _find_mlproject(directory):\n    filenames = os.listdir(directory)\n    for filename in filenames:\n        if filename.lower() == MLPROJECT_FILE_NAME:\n            return os.path.join(directory, filename)\n    return None",
  "input_contexts": [
    {
      "id": "mlflow_mlflow_3045_1",
      "input_code": "def load_project(directory):\n    mlproject_path = _find_mlproject(directory)\n\n    yaml_obj = {}\n    if mlproject_path is not None:\n        with open(mlproject_path) as mlproject_file:\n            yaml_obj = yaml.safe_load(mlproject_file)\n\n    env_fields = set(yaml_obj.keys()).intersection(env_type.ALL)\n    if len(env_fields) > 1:\n        raise ExecutionException(\n            f\"Project cannot contain multiple environment fields: {env_fields}\"\n        )\n\n    project_name = yaml_obj.get(\"name\")\n\n    entry_points = {}\n    for name, entry_point_yaml in yaml_obj.get(\"entry_points\", {}).items():\n        parameters = entry_point_yaml.get(\"parameters\", {})\n        command = entry_point_yaml.get(\"command\")\n        entry_points[name] = EntryPoint(name, parameters, command)\n\n    databricks_spark_job_yaml = yaml_obj.get(\"databricks_spark_job\")\n    if databricks_spark_job_yaml is not None:\n        python_file = databricks_spark_job_yaml.get(\"python_file\")\n\n        if python_file is None and not entry_points:\n            raise MlflowException(\n                \"Databricks Spark job requires either 'databricks_spark_job.python_file' \"\n                \"setting or 'entry_points' setting.\"\n            )\n        if python_file is not None and entry_points:\n            raise MlflowException(\n                \"Databricks Spark job does not allow setting both \"\n                \"'databricks_spark_job.python_file' and 'entry_points'.\"\n            )\n\n        for entry_point in entry_points.values():\n            for param in entry_point.parameters.values():\n                if param.type == \"path\":\n                    raise MlflowException(\n                        \"Databricks Spark job does not support entry point parameter of 'path' \"\n                        f\"type. '{param.name}' value type is invalid.\"\n                    )\n\n        if env_type.DOCKER in yaml_obj:\n            raise MlflowException(\n                \"Databricks Spark job does not support setting docker environment.\"\n            )\n\n        if env_type.PYTHON in yaml_obj:\n            raise MlflowException(\n                \"Databricks Spark job does not support setting python environment.\"\n            )\n\n        if env_type.CONDA in yaml_obj:\n            raise MlflowException(\n                \"Databricks Spark job does not support setting conda environment.\"\n            )\n\n        databricks_spark_job_spec = DatabricksSparkJobSpec(\n            python_file=databricks_spark_job_yaml.get(\"python_file\"),\n            parameters=databricks_spark_job_yaml.get(\"parameters\", []),\n            python_libraries=databricks_spark_job_yaml.get(\"python_libraries\", []),\n        )\n        return Project(\n            databricks_spark_job_spec=databricks_spark_job_spec,\n            name=project_name,\n            entry_points=entry_points,\n        )\n\n    docker_env = yaml_obj.get(env_type.DOCKER)\n    if docker_env:\n        if not docker_env.get(\"image\"):\n            raise ExecutionException(\n                \"Project configuration (MLproject file) was invalid: Docker \"\n                \"environment specified but no image attribute found.\"\n            )\n        if docker_env.get(\"volumes\"):\n            if not (\n                isinstance(docker_env[\"volumes\"], list)\n                and all(isinstance(i, str) for i in docker_env[\"volumes\"])\n            ):\n                raise ExecutionException(\n                    \"Project configuration (MLproject file) was invalid: \"\n                    \"Docker volumes must be a list of strings, \"\n                    \"\"\"e.g.: '[\"/path1/:/path1\", \"/path2/:/path2\"])\"\"\"\n                )\n        if docker_env.get(\"environment\"):\n            if not (\n                isinstance(docker_env[\"environment\"], list)\n                and all(isinstance(i, (list, str)) for i in docker_env[\"environment\"])\n            ):\n                raise ExecutionException(\n                    \"Project configuration (MLproject file) was invalid: \"\n                    \"environment must be a list containing either strings (to copy environment \"\n                    \"variables from host system) or lists of string pairs (to define new \"\n                    \"environment variables).\"\n                    \"\"\"E.g.: '[[\"NEW_VAR\", \"new_value\"], \"VAR_TO_COPY_FROM_HOST\"])\"\"\"\n                )\n        return Project(\n            env_type=env_type.DOCKER,\n            env_config_path=None,\n            entry_points=entry_points,\n            docker_env=docker_env,\n            name=project_name,\n        )\n\n    python_env = yaml_obj.get(env_type.PYTHON)\n    if python_env:\n        python_env_path = os.path.join(directory, python_env)\n        if not os.path.exists(python_env_path):\n            raise ExecutionException(\n                f\"Project specified python_env file {python_env_path}, but no such \"\n                \"file was found.\"\n            )\n        return Project(\n            env_type=env_type.PYTHON,\n            env_config_path=python_env_path,\n            entry_points=entry_points,\n            docker_env=None,\n            name=project_name,\n        )\n\n    conda_path = yaml_obj.get(env_type.CONDA)\n    if conda_path:\n        conda_env_path = os.path.join(directory, conda_path)\n        if not os.path.exists(conda_env_path):\n            raise ExecutionException(\n                f\"Project specified conda environment file {conda_env_path}, but no such \"\n                \"file was found.\"\n            )\n        return Project(\n            env_type=env_type.CONDA,\n            env_config_path=conda_env_path,\n            entry_points=entry_points,\n            docker_env=None,\n            name=project_name,\n        )\n\n    default_python_env_path = os.path.join(directory, _PYTHON_ENV_FILE_NAME)\n    if os.path.exists(default_python_env_path):\n        return Project(\n            env_type=env_type.PYTHON,\n            env_config_path=default_python_env_path,\n            entry_points=entry_points,\n            docker_env=None,\n            name=project_name,\n        )\n\n    default_conda_path = os.path.join(directory, DEFAULT_CONDA_FILE_NAME)\n    if os.path.exists(default_conda_path):\n        return Project(\n            env_type=env_type.CONDA,\n            env_config_path=default_conda_path,\n            entry_points=entry_points,\n            docker_env=None,\n            name=project_name,\n        )\n\n    return Project(\n        env_type=env_type.PYTHON,\n        env_config_path=None,\n        entry_points=entry_points,\n        docker_env=None,\n        name=project_name,\n    )\n"
    }
  ],
  "generated_inputs": [
    "<needimport>\nimport os\n\n<testcase_1>\ndirectory = \"/path/to/project1\"\n\n<testcase_2>\ndirectory = \"/path/to/project2\"\n\n<testcase_3>\ndirectory = \"/path/to/nonexistent_directory\"\n\n<testcase_4>\ndirectory = \"/path/to/project_with_no_mlproject\"\n\n<testcase_5>\ndirectory = \"/path/to/project_with_mixed_case_mlproject\""
  ],
  "gpt_output": "<needimport>\nimport os\n\n<testcase_1>\ndirectory = \"/path/to/project1\"\n\n<testcase_2>\ndirectory = \"/path/to/project2\"\n\n<testcase_3>\ndirectory = \"/path/to/nonexistent_directory\"\n\n<testcase_4>\ndirectory = \"/path/to/project_with_no_mlproject\"\n\n<testcase_5>\ndirectory = \"/path/to/project_with_mixed_case_mlproject\"",
  "needimport": "import os\n\n",
  "test_results": {
    "ans1": true,
    "ans2": false,
    "ans3": true,
    "ans4": true,
    "ans5": true
  }
}