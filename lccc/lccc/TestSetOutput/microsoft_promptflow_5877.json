{
  "input_header": "def completion( self, prompt: PromptTemplate, deployment_name: str, suffix: str = None, max_tokens: int = 16, temperature: float = 1.0, top_p: float = 1.0, n: int = 1, stream: bool = False, logprobs: int = None, echo: bool = False, stop: list = None, presence_penalty: float = None, frequency_penalty: float = None, best_of: int = 1, logit_bias: dict = {}, user: str = \"\", **kwargs, ):",
  "input_docstring": null,
  "output_code": "    def completion(\n        self,\n        prompt: PromptTemplate,\n        deployment_name: str,\n        suffix: str = None,\n        max_tokens: int = 16,\n        temperature: float = 1.0,\n        top_p: float = 1.0,\n        n: int = 1,\n        stream: bool = False,\n        logprobs: int = None,\n        echo: bool = False,\n        stop: list = None,\n        presence_penalty: float = None,\n        frequency_penalty: float = None,\n        best_of: int = 1,\n        logit_bias: dict = {},\n        user: str = \"\",\n        **kwargs,\n    ):\n        prompt = render_jinja_template(prompt, trim_blocks=True, keep_trailing_newline=True, **kwargs)\n        echo = to_bool(echo)\n        stream = to_bool(stream)\n        params = {}\n        if presence_penalty is not None:\n            params[\"presence_penalty\"] = presence_penalty\n        if frequency_penalty is not None:\n            params[\"frequency_penalty\"] = frequency_penalty\n\n        response = self._client.completions.create(\n            prompt=prompt,\n            model=deployment_name,\n            suffix=suffix if suffix else None,\n            max_tokens=int(max_tokens) if max_tokens is not None else None,\n            temperature=float(temperature),\n            top_p=float(top_p),\n            n=int(n),\n            stream=stream,\n            logprobs=int(logprobs) if logprobs else None,\n            echo=echo,\n            stop=stop if stop else None,\n            best_of=int(best_of),\n            logit_bias=logit_bias if logit_bias else {},\n            user=user,\n            extra_headers={\"ms-azure-ai-promptflow-called-from\": \"aoai-tool\"},\n            **params\n        )\n\n        if stream:\n            def generator():\n                for chunk in response:\n                    if chunk.choices:\n                        yield chunk.choices[0].text if hasattr(chunk.choices[0], 'text') and \\\n                               chunk.choices[0].text is not None else \"\"\n\n            return generator()\n        else:\n            return response.choices[0].text",
  "input_contexts": [
    {
      "id": "microsoft_promptflow_5877_4",
      "input_code": "    def test_aoai_parameters(self, params, expected):\n        for k, v in params.items():\n            if k not in expected:\n                expected[k] = v\n        deployment_name = \"dummy\"\n        conn_dict = {\"api_key\": \"dummy\", \"api_base\": \"base\", \"api_version\": \"dummy_ver\", \"api_type\": \"azure\"}\n        conn = AzureOpenAIConnection(**conn_dict)\n\n        def mock_completion(self, **kwargs):\n            assert kwargs[\"model\"] == deployment_name\n            for k, v in expected.items():\n                assert kwargs[k] == v, f\"Expect {k} to be {v}, but got {kwargs[k]}\"\n            text = kwargs[\"prompt\"]\n            return AttrDict({\"choices\": [AttrDict({\"text\": text})]})\n\n        with patch(\"openai.resources.Completions.create\", new=mock_completion):\n            prompt = \"dummy_prompt\"\n            result = completion(connection=conn, prompt=prompt, deployment_name=deployment_name, **params)\n            assert result == prompt\n"
    },
    {
      "id": "microsoft_promptflow_5877_5",
      "input_code": "    def test_rate_limit_error_insufficient_quota(self, azure_open_ai_connection, mocker: MockerFixture):\n        dummyEx = RateLimitError(\"Something went wrong\", response=httpx.Response(\n            429, request=httpx.Request('GET', 'https://www.example.com')), body={\"type\": \"insufficient_quota\"})\n        mock_method = mocker.patch(\"openai.resources.Completions.create\", side_effect=dummyEx)\n        error_codes = \"UserError/OpenAIError/RateLimitError\"\n        with pytest.raises(WrappedOpenAIError) as exc_info:\n            completion(connection=azure_open_ai_connection, prompt=\"hello\", deployment_name=\"text-ada-001\")\n        assert to_openai_error_message(dummyEx) == exc_info.value.message\n        assert mock_method.call_count == 1\n        assert exc_info.value.error_codes == error_codes.split(\"/\")\n"
    },
    {
      "id": "microsoft_promptflow_5877_8",
      "input_code": "def test_aoai_tool_header():\n    def mock_complete(*args, **kwargs):\n        Response = namedtuple(\"Response\", [\"choices\"])\n        Choice = namedtuple(\"Choice\", [\"text\"])\n        choice = Choice(text=kwargs.get(\"extra_headers\", {}))\n        response = Response(choices=[choice])\n        return response\n\n    def mock_chat(*args, **kwargs):\n        Completion = namedtuple(\"Completion\", [\"choices\"])\n        Choice = namedtuple(\"Choice\", [\"message\"])\n        Message = namedtuple(\"Message\", [\"content\"])\n        message = Message(content=kwargs.get(\"extra_headers\", {}))\n        choice = Choice(message=message)\n        completion = Completion(choices=[choice])\n        return completion\n\n    def mock_embedding(*args, **kwargs):\n        Response = namedtuple(\"Response\", [\"data\"])\n        Embedding = namedtuple(\"Embedding\", [\"embedding\"])\n        response = Response(data=[Embedding(embedding=kwargs.get(\"extra_headers\", {}))])\n        return response\n\n    with patch(\"openai.resources.Completions.create\", new=mock_complete), patch(\n        \"openai.resources.chat.Completions.create\", new=mock_chat\n    ), patch(\"openai.resources.Embeddings.create\", new=mock_embedding):\n        inject_openai_api()\n        aoai_tool_header = {\"ms-azure-ai-promptflow-called-from\": \"aoai-tool\"}\n\n        return_headers = AzureOpenAI(AzureOpenAIConnection(api_key=\"test\", api_base=\"test\")).completion(\n            prompt=\"test\", deployment_name=\"test\"\n        )\n        assert aoai_tool_header.items() <= return_headers.items()\n\n        return_headers = AzureOpenAI(AzureOpenAIConnection(api_key=\"test\", api_base=\"test\")).chat(\n            prompt=\"user:\\ntest\", deployment_name=\"test\"\n        )\n        assert aoai_tool_header.items() <= return_headers.items()\n\n        return_headers = embedding(\n            AzureOpenAIConnection(api_key=\"test\", api_base=\"test\"), input=\"test\", deployment_name=\"test\"\n        )\n        assert aoai_tool_header.items() <= return_headers.items()\n"
    },
    {
      "id": "microsoft_promptflow_5877_7",
      "input_code": "    def test_completion_with_chat_model(self, azure_open_ai_connection):\n        with pytest.raises(UserErrorException) as exc_info:\n            completion(connection=azure_open_ai_connection, prompt=\"hello\", deployment_name=\"gpt-35-turbo\")\n        msg = \"Completion API is a legacy api and is going to be deprecated soon. \" \\\n              \"Please change to use Chat API for current model.\"\n        assert msg in exc_info.value.message\n"
    },
    {
      "id": "microsoft_promptflow_5877_1",
      "input_code": "def llm(\n    connection,\n    prompt: PromptTemplate,\n    api: str = \"chat\",\n    deployment_name: str = \"\",\n    model: str = \"\",\n    temperature: float = 1.0,\n    top_p: float = 1.0,\n    stream: bool = False,\n    stop: list = None,\n    max_tokens: int = None,\n    presence_penalty: float = None,\n    frequency_penalty: float = None,\n    logit_bias: dict = {},\n    tool_choice: object = None,\n    tools: list = None,\n    response_format: object = None,\n    seed: int = None,\n    suffix: str = None,\n    logprobs: int = None,\n    echo: bool = False,\n    best_of: int = 1,\n    **kwargs,\n):\n    if isinstance(connection, AzureOpenAIConnection):\n        if api == \"completion\":\n            return AzureOpenAI(connection).completion(\n                prompt=prompt,\n                deployment_name=deployment_name,\n                temperature=temperature,\n                top_p=top_p,\n                stream=stream,\n                stop=stop,\n                max_tokens=max_tokens,\n                presence_penalty=presence_penalty,\n                frequency_penalty=frequency_penalty,\n                logit_bias=logit_bias,\n                suffix=suffix,\n                logprobs=logprobs,\n                echo=echo,\n                best_of=best_of,\n                **kwargs\n            )\n        else:\n            return AzureOpenAI(connection).chat(\n                prompt=prompt,\n                deployment_name=deployment_name,\n                temperature=temperature,\n                top_p=top_p,\n                stream=stream,\n                stop=stop,\n                max_tokens=max_tokens,\n                presence_penalty=presence_penalty,\n                frequency_penalty=frequency_penalty,\n                logit_bias=logit_bias,\n                tool_choice=tool_choice,\n                tools=tools,\n                response_format=response_format,\n                seed=seed,\n                **kwargs\n            )\n    elif isinstance(connection, (OpenAIConnection, ServerlessConnection)):\n        if api == \"completion\":\n            return OpenAI(connection).completion(\n                prompt=prompt,\n                model=model,\n                temperature=temperature,\n                top_p=top_p,\n                stream=stream,\n                stop=stop,\n                max_tokens=max_tokens,\n                presence_penalty=presence_penalty,\n                frequency_penalty=frequency_penalty,\n                logit_bias=logit_bias,\n                suffix=suffix,\n                logprobs=logprobs,\n                echo=echo,\n                best_of=best_of,\n                **kwargs\n            )\n        else:\n            return OpenAI(connection).chat(\n                prompt=prompt,\n                model=model,\n                temperature=temperature,\n                top_p=top_p,\n                stream=stream,\n                stop=stop,\n                max_tokens=max_tokens,\n                presence_penalty=presence_penalty,\n                frequency_penalty=frequency_penalty,\n                logit_bias=logit_bias,\n                tool_choice=tool_choice,\n                tools=tools,\n                response_format=response_format,\n                seed=seed,\n                **kwargs\n            )\n    else:\n        error_message = f\"Not Support connection type '{type(connection).__name__}' for llm. \" \\\n                        \"Connection type should be in [AzureOpenAIConnection, OpenAIConnection\" \\\n                        \", ServerlessConnection].\"\n        raise InvalidConnectionType(message=error_message)\n"
    },
    {
      "id": "microsoft_promptflow_5877_3",
      "input_code": "    def test_aoai_stream_completion(self, aoai_provider):\n        prompt_template = \"please complete this sentence: world war II \"\n        aoai_provider.completion(\n            prompt=prompt_template, deployment_name=\"gpt-35-turbo-instruct\", stop=[], logit_bias={}, stream=True\n        )\n"
    },
    {
      "id": "microsoft_promptflow_5877_2",
      "input_code": "    def test_aoai_completion(self, aoai_provider):\n        prompt_template = \"please complete this sentence: world war II \"\n        aoai_provider.completion(\n            prompt=prompt_template, deployment_name=\"gpt-35-turbo-instruct\", stop=[], logit_bias={}\n        )\n"
    },
    {
      "id": "microsoft_promptflow_5877_6",
      "input_code": "    def test_non_retriable_openai_error_handle(\n            self, azure_open_ai_connection, mocker: MockerFixture, dummyExceptionList\n    ):\n        for dummyEx in dummyExceptionList:\n            mock_method = mocker.patch(\"openai.resources.Completions.create\", side_effect=dummyEx)\n            with pytest.raises(UserErrorException) as exc_info:\n                completion(connection=azure_open_ai_connection, prompt=\"hello\", deployment_name=\"text-ada-001\")\n            assert to_openai_error_message(dummyEx) == exc_info.value.message\n            error_codes = \"UserError/OpenAIError/\" + type(dummyEx).__name__\n            assert exc_info.value.error_codes == error_codes.split(\"/\")\n            assert mock_method.call_count == 1\n"
    }
  ],
  "generated_inputs": [
    "<needimport>\nimport openai\nfrom unittest.mock import patch\nfrom collections import namedtuple\n\n<testcase_1>\nprompt = \"What is the capital of France?\"\ndeployment_name = \"text-davinci-003\"\nsuffix = None\nmax_tokens = 50\ntemperature = 0.7\ntop_p = 1.0\nn = 1\nstream = False\nlogprobs = None\necho = False\nstop = [\"\\n\"]\npresence_penalty = 0.0\nfrequency_penalty = 0.0\nbest_of = 1\nlogit_bias = {}\nuser = \"test_user\"\n\n<testcase_2>\nprompt = \"Translate the following English text to French: 'Hello, how are you?'\"\ndeployment_name = \"text-davinci-003\"\nsuffix = None\nmax_tokens = 60\ntemperature = 0.5\ntop_p = 0.9\nn = 1\nstream = False\nlogprobs = None\necho = False\nstop = [\"\\n\"]\npresence_penalty = 0.0\nfrequency_penalty = 0.0\nbest_of = 1\nlogit_bias = {}\nuser = \"translator_bot\"\n\n<testcase_3>\nprompt = \"Write a short story about a dragon and a knight.\"\ndeployment_name = \"text-davinci-003\"\nsuffix = None\nmax_tokens = 150\ntemperature = 0.9\ntop_p = 1.0\nn = 1\nstream = False\nlogprobs = None\necho = False\nstop = [\"\\n\"]\npresence_penalty = 0.0\nfrequency_penalty = 0.0\nbest_of = 1\nlogit_bias = {}\nuser = \"story_writer\"\n\n<testcase_4>\nprompt = \"Summarize the following article: 'The quick brown fox jumps over the lazy dog.'\"\ndeployment_name = \"text-davinci-003\"\nsuffix = None\nmax_tokens = 30\ntemperature = 0.3\ntop_p = 1.0\nn = 1\nstream = False\nlogprobs = None\necho = False\nstop = [\"\\n\"]\npresence_penalty = 0.0\nfrequency_penalty = 0.0\nbest_of = 1\nlogit_bias = {}\nuser = \"summarizer_bot\"\n\n<testcase_5>\nprompt = \"Generate a list of 10 creative business ideas.\"\ndeployment_name = \"text-davinci-003\"\nsuffix = None\nmax_tokens = 100\ntemperature = 0.8\ntop_p = 1.0\nn = 1\nstream = False\nlogprobs = None\necho = False\nstop = [\"\\n\"]\npresence_penalty = 0.0\nfrequency_penalty = 0.0\nbest_of = 1\nlogit_bias = {}\nuser = \"business_idea_generator\""
  ],
  "gpt_output": "<needimport>\nimport openai\nfrom unittest.mock import patch\nfrom collections import namedtuple\n\n<testcase_1>\nprompt = \"What is the capital of France?\"\ndeployment_name = \"text-davinci-003\"\nsuffix = None\nmax_tokens = 50\ntemperature = 0.7\ntop_p = 1.0\nn = 1\nstream = False\nlogprobs = None\necho = False\nstop = [\"\\n\"]\npresence_penalty = 0.0\nfrequency_penalty = 0.0\nbest_of = 1\nlogit_bias = {}\nuser = \"test_user\"\n\n<testcase_2>\nprompt = \"Translate the following English text to French: 'Hello, how are you?'\"\ndeployment_name = \"text-davinci-003\"\nsuffix = None\nmax_tokens = 60\ntemperature = 0.5\ntop_p = 0.9\nn = 1\nstream = False\nlogprobs = None\necho = False\nstop = [\"\\n\"]\npresence_penalty = 0.0\nfrequency_penalty = 0.0\nbest_of = 1\nlogit_bias = {}\nuser = \"translator_bot\"\n\n<testcase_3>\nprompt = \"Write a short story about a dragon and a knight.\"\ndeployment_name = \"text-davinci-003\"\nsuffix = None\nmax_tokens = 150\ntemperature = 0.9\ntop_p = 1.0\nn = 1\nstream = False\nlogprobs = None\necho = False\nstop = [\"\\n\"]\npresence_penalty = 0.0\nfrequency_penalty = 0.0\nbest_of = 1\nlogit_bias = {}\nuser = \"story_writer\"\n\n<testcase_4>\nprompt = \"Summarize the following article: 'The quick brown fox jumps over the lazy dog.'\"\ndeployment_name = \"text-davinci-003\"\nsuffix = None\nmax_tokens = 30\ntemperature = 0.3\ntop_p = 1.0\nn = 1\nstream = False\nlogprobs = None\necho = False\nstop = [\"\\n\"]\npresence_penalty = 0.0\nfrequency_penalty = 0.0\nbest_of = 1\nlogit_bias = {}\nuser = \"summarizer_bot\"\n\n<testcase_5>\nprompt = \"Generate a list of 10 creative business ideas.\"\ndeployment_name = \"text-davinci-003\"\nsuffix = None\nmax_tokens = 100\ntemperature = 0.8\ntop_p = 1.0\nn = 1\nstream = False\nlogprobs = None\necho = False\nstop = [\"\\n\"]\npresence_penalty = 0.0\nfrequency_penalty = 0.0\nbest_of = 1\nlogit_bias = {}\nuser = \"business_idea_generator\"",
  "needimport": "import openai\nfrom unittest.mock import patch\nfrom collections import namedtuple\n\n",
  "test_results": {
    "ans1": "Echo: What is the capital of France?",
    "ans2": "Echo: Translate the following English text to French: 'Hello, how are you?'",
    "ans3": "Echo: Write a short story about a dragon and a knight.",
    "ans4": "Echo: Summarize the following article: 'The quick brown fox jumps over the lazy dog.'",
    "ans5": "Echo: Generate a list of 10 creative business ideas."
  }
}